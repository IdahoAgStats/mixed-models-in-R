[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Field Guide to the R Mixed Model Wilderness",
    "section": "",
    "text": "Preface\nRunning mixed models in R is no easy task. There are dozens of packages supporting these aims, each with varying functionality, syntax, and conventions. The linear mixed model ecosystem in R consists of over 80 libraries that either construct and solve mixed model equations or helper packages the process the results from mixed model analysis. These libraries provide a patchwork of overlapping and unique functionality regarding the fundamental structure of mixed models: allowable distributions, nested and crossed random effects, heterogeneous error structures and other facets. No single R library has all possible functionality enabled for fitting mixed models.\nThis patchwork of packages makes it very challenging for statisticians to conduct mixed model analysis and to teach others how to run mixed models in R. We have written this guide to provide recipes for handling common analytical scenarios encountered in agricultural and life sciences that require mixed models. As a field guide, it is intended to be succinct, and to help researchers meet their analytic goals.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-this-does-not-cover",
    "href": "index.html#what-this-does-not-cover",
    "title": "Field Guide to the R Mixed Model Wilderness",
    "section": "What This Does Not Cover",
    "text": "What This Does Not Cover\n\nGeneralized linear models (where the dependent variable follows a non-Gaussian distribution, and a link function is used to model how the expected value of dependent variable responds to a linear predictor). We do address cases of unequal variance in Chapter 14, but if another distribution and/or a link function is required for the model, that is not addressed in this guide.\nBasic principles of experimental design. We assume you know this, but if you do not, please check out the Grammar of Experimental Design for guidance on these topics.\nInstructions in using R. We assume familiarity with R. If you need help in learning R, there are numerous guides, including our introductory R course.\nNon-linear models. As a reminder, the ‚Äúlinear‚Äù in linear models refer to the parameters being estimated, not the structure of the independent variables.\n\n\n\n\n\n\n\nNotice!\n\n\n\nThis is a work-in-progress and will be updated over time.\n\n\nIn general, the content from this website may not be copied or reproduced without attribution. However, the example code and required data sets to run the code are MIT licensed. These can be accessed on GitHub.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/intro.html",
    "href": "chapters/intro.html",
    "title": "1¬† Introduction",
    "section": "",
    "text": "1.1 Data\nThis guide is focused on frequentist implementations of mixed models in R, covering different scenarios common in the agricultural and life sciences.\nThis is not intended to be a guide to the theory of mixed models, it is focused on R syntactical issues for correct implementations of mixed models only. We do provide a short introduction to mixed models. UCLA provides a longer more comprehensive introduction.\nReaders of this materials are welcome to test any of the code in this guide. All data used in this tutorial are from previously conducted studies and are available in the GitHub repository for this resource and explained in the chapters they are used. You can also download all data sets as a zipped file (51 kb).",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/intro.html#data",
    "href": "chapters/intro.html#data",
    "title": "1¬† Introduction",
    "section": "",
    "text": "Download Data (.zip)",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/intro.html#packages",
    "href": "chapters/intro.html#packages",
    "title": "1¬† Introduction",
    "section": "1.2 Packages",
    "text": "1.2 Packages\n\n1.2.1 Table of required packages for modelling\n\n\n\nTable¬†1.1: Table of required packages\n\n\n\n\n\nPackage\nPurpose\n\n\n\n\nlme4 (Bates et al. 2015)\nmain package for linear mixed models\n\n\nlmerTest (Kuznetsova, Brockhoff, and Christensen 2017)\nfor computing p-values when using lme4\n\n\nnlme (J. Pinheiro, Bates, and R Core Team 2023; J. C. Pinheiro and Bates 2000)\nmain package for linear mixed models and part of ‚Äòbase R‚Äô\n\n\nemmeans (Lenth 2022)\nfor estimating fixed effects and their confidence intervals, and conducting contrasts\n\n\nbroom.mixed (Bolker and Robinson 2024)\npackage for presenting the model summary output into a ‚Äútidy‚Äù workflow.\n\n\nDHARMa (Hartig 2022)\nfor evaluating residuals (error terms) in generalized linear models\n\n\nperformance (L√ºdecke et al. 2021)\nFor creating diagnostic plots or to compute fit measures\n\n\n\n\n\n\n\n\n1.2.2 Optional packages\n\n\n\nTable¬†1.2: Table of optional packages\n\n\n\n\n\nPackage Name\nFunction\n\n\nhere\nFor setting the working directory\n\n\nggplot\nplotting\n\n\ndesplot\nplotting\n\n\nagridat\nto download example dataset\n\n\nagricolae\nto download example dataset\n\n\n\n\n\n\nThis entire guide will use the here package for loading data. If you can load your data fine without this package, please carry on; here is not required for running mixed models.\n\n\n\n\nBates, Douglas, Martin M√§chler, Ben Bolker, and Steve Walker. 2015. ‚ÄúFitting Linear Mixed-Effects Models Using lme4.‚Äù Journal of Statistical Software 67 (1): 1‚Äì48. https://doi.org/10.18637/jss.v067.i01.\n\n\nBolker, Ben, and David Robinson. 2024. Broom.mixed: Tidying Methods for Mixed Models. https://CRAN.R-project.org/package=broom.mixed.\n\n\nHartig, Florian. 2022. DHARMa: Residual Diagnostics for Hierarchical (Multi-Level / Mixed) Regression Models. https://CRAN.R-project.org/package=DHARMa.\n\n\nKuznetsova, Alexandra, Per B. Brockhoff, and Rune H. B. Christensen. 2017. ‚ÄúlmerTest Package: Tests in Linear Mixed Effects Models.‚Äù Journal of Statistical Software 82 (13): 1‚Äì26. https://doi.org/10.18637/jss.v082.i13.\n\n\nLenth, Russell V. 2022. Emmeans: Estimated Marginal Means, Aka Least-Squares Means. https://CRAN.R-project.org/package=emmeans.\n\n\nL√ºdecke, Daniel, Mattan S. Ben-Shachar, Indrajeet Patil, Philip Waggoner, and Dominique Makowski. 2021. ‚Äúperformance: An R Package for Assessment, Comparison and Testing of Statistical Models.‚Äù Journal of Open Source Software 6 (60): 3139. https://doi.org/10.21105/joss.03139.\n\n\nPinheiro, Jos√© C., and Douglas M. Bates. 2000. Mixed-Effects Models in s and s-PLUS. New York: Springer. https://doi.org/10.1007/b98882.\n\n\nPinheiro, Jos√©, Douglas Bates, and R Core Team. 2023. Nlme: Linear and Nonlinear Mixed Effects Models. https://CRAN.R-project.org/package=nlme.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/analysis-tips.html",
    "href": "chapters/analysis-tips.html",
    "title": "2¬† Zen and the Art of Statistical Analysis",
    "section": "",
    "text": "Statistical analysis is hard. Despite the common view that science produces unambiguous truths, science and statistics involve making decisions and judgement calls that affect the outcomes and conclusions of a study. These decisions are informed by our scientific thinking and (hopefully) justified in our manuscripts so they can be interrogated by other experts. The (now defunct) website 538 published a well-researched article describing some of these difficulties, the temptation to ‚Äúp-hack‚Äù, and the consequences for scientific research.\nThis does not mean we should resign ourselves to get it wrong, but try our best to approach analysis remembering that the goal is to obtain truth and answer a specific research question, not obtain a low p-value.\n\nThe scientific method is the most rigorous path to knowledge, but it‚Äôs also messy and tough. Science deserves respect exactly because it is difficult ‚Äî not because it gets everything correct on the first try. The uncertainty inherent in science doesn‚Äôt mean that we can‚Äôt use it to make important policies or decisions. It just means that we should remain cautious and adopt a mindset that‚Äôs open to changing course if new data arises. We should make the best decisions we can with the current evidence and take care not to lose sight of its strength and degree of certainty. It‚Äôs no accident that every good paper includes the phrase ‚Äúmore study is needed‚Äù ‚Äî there is always more to learn.\n‚Äì Christie Aschwanden/538, Science Isn‚Äôt Broken\n\nBelow are some things our office frequently says to researchers.\n\n2.0.1 Think About Your Analytical Goals\nThroughout this guide, we have tried to explicitly state the goals of each analysis. This helps informs how to approach the analysis of an experiment. It can be difficult, especially for new scientists-in-training (i.e.¬†graduate students), to understand what it is they want to estimate. You may have been handed a data set you had no role in generating and told to ‚Äúanalyze this‚Äù with no additional context. Or perhaps you may have conducted a large study that has some overall goals that are lofty, yet vague. And now you must translate the vague aims into clear statistical questions.\nIt can helpful to think about the exact results you are hoping to get. What does this look like exactly? Do you want to estimate the changes in plant diversity as the result of a herbicide spraying program? Do you want to find out if a fertilizer treatment changed protein content in a crop and by how much? Do you want to know about changes in human diet due to an intervention? What are quantifiable difference that you and/or experts in your domain would find meaningful?\nConsider what the results would look like for (1) the best case scenario where your wildest research dreams come true, and (2) null results, when you find out that your treatment or invention had no effect. It‚Äôs very helpful to understand and recognize exactly what both situations look like.\nBy ‚Äúconsider‚Äù, we mean: imagine the final plot or table, or summary sentence you want to present, either in a peer-reviewed manuscript, or some output for stakeholders. From this, you can work backwards to determine the analytical approach needed to arrive at that desired final output. Or you may determine that your data are unsuitable to generate the desired output, in which case, it‚Äôs best to ascertain that as soon as possible.\nBy ‚Äúconsider‚Äù, we also mean: imagine exactly what the spreadsheet of results would contain after a successful trial. What columns are present and what data are in those cells? If you are planning an experiment, this can help ensure you plan it properly to actually test whatever it is you want to evaluate. If the experiment is done, this enables you to evaluate if you have the information present to test your hypothesis.\nBy taking the time to reflect on what it is you exactly want to analyze, this can save time and prevent you from doing unneeded analyzes that don‚Äôt serve this final goal. There is rarely (never?) one way to analyze an experiment or a data set, so use your limited time wisely and focus on what matters to you most.\n\n\n2.0.2 Know That Data Cleaning is Time Consuming\n\n\n\n\n\n\n\n\n\nFigure¬†2.1: How you will spend your time\n\n\n\n\nThis has and will continue to occupy the majority of researcher‚Äôs time when conducting an analysis. Truly, we are sorry for this. But, please know it is not you, it is the nature of data. Plan for and prepare yourself mentally to spend time cleaning and preparing your data for analysis. 1. The American Statistician, 72(1), 2‚Äì10.] This will likely take way longer than the actual analysis! It is needed to ensure you can actually get correct results in an analysis, and hence data cleaning is worth the time it requires.\n1¬†For an excellent set of basic instructions on data preparation, please see Broman and Woo (2018), Data Organization in Spreadsheets\n\n2.0.3 Interpret ANOVA and P-values with Caution\n\nInformally, a p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value.\n‚Äì American Statistical Association/Wasserstein and Lazar (2016)\n\nThe great majority of researchers are deeply interested in p-values. This is not a bad thing per se, but sometimes the focus is so strong it comes at the expense of other valuable pieces of information, like treatment estimates! Russ Leanth, author of the emmeans package, refers to this particular practice as ‚Äústar gazing‚Äù.\nIt is important to evaluate why you want to do ANOVA, what extra information it will bring and what you plan to do with those results. Sometimes, researchers want to conduct an ANOVA even though the original goals of analysis were reached without it. Running an ANOVA may increase or decrease confidence in your other results. That is not at all what ANOVA is intended to do, nor is this what p-values can tell us. ANOVA compares across group variation to within group variation. It cannot tell us if anything is the ‚Äòsame‚Äô (there is a separate branch of analysis, ‚Äòequivalence testing‚Äô, for that), and it cannot tell us specifically what is different, unless you are fortunate enough to only have two levels in your treatment structure. P-values provide no guarantee that something is truly different or not; it only quantifies the probability you could have observed these results by chance.\nThe American Statistics Association recommends that ‚ÄúScientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold‚Äù (Wasserstein and Lazar 2016). That article also explains what p-values are telling us and how to avoid committing analytical errors and/or misinterpreting p-values. If you have time to read the full article, it will benefit your research!\nThe main problematic behavior I see is researchers using p-values as the sole criteria on whether to present results: ‚ÄúWe wanted to test if x, y and z had an effect. We ran some model and found that that only x had a significant effect, and those results indicate‚Ä¶‚Äù (while results with a p-value less than 0.05 are ignored).\nA better option would be to discuss the the results of the analysis and how they addressed the research questions: how did the dependent variable change (or not change) as a result of the treatments/interventions/independent variables? What are the parameters or treatment predictions and what do they tell us with regard to the research goals? And to bolster those estimates, what are the confidence intervals on those estimates? What are the p-values for the statistical tests? P-values can support the results and conclusions, but the main results desired by a researcher are usually the estimates themselves - so lead with that!\nTo learn more about common pitfalls in interpreting p-values, check out our blog post on the subject and/or Greenland et al. (2016) on the subject.\n\n\n\n\n\n\nThe Great Debate Over Hypothesis Testing\n\n\n\n\n\nThere is some discussion among statisticians regarding if ‚Äúnull hypothesis significance testing‚Äù (hereafter called ‚Äòhypothesis testing‚Äô) no longer serves science well. In 2006, Doug Bates (the original author of lme4) first brought up the underlying foundational challenges of determining the denominator degrees of freedom in a complex mixed model in this infamous listserve post (truly worth a read). For many years, there was no way to obtain p-values from lme4 model objects unless you were willing to manually calculate them in R. Nowadays, we have lmerTest to solve this issue.\nNorm Matloff has recently contributed to the discussion by pointing out that all null hypotheses are false, raising the question of why do we continue to pursue this ‚Äúabsurdity‚Äù (his word choice). Both of these points of view, while valid, are highly discomforting to most scientists, whose entire discipline may revolve on demonstrating ‚Äústatistical significance.‚Äù\nDushoff et al (2019) also argues the the null hypothesis is nonsensical and that hypothesis testing is frequently misinterpreted. However, they also recognize the centrality of hypothesis testing to scientific publishing, and hence recommend that scientists better acquaint themselves with the foundation concepts of statistical hypothesis testing and shift their language away from the binary ‚Äú[statistically] significant/not significant‚Äù paradigm.\n\n\n\n\n\n2.0.4 Comments on Hypothesis Testing and Usage of Treatment Letters\n\nIf you‚Äôre going to commit statistical crimes, make sure they are misdemeanors, not felonies.\n‚Äì Neil Paton\n\nOften, I see researchers use compact letter display (e.g.¬†‚ÄúA‚Äù, ‚ÄúB‚Äù, ‚ÄúC‚Äù, ‚Ä¶.) for indicating differences among treatments. This makes for concise presentation of results in tables and figures, but it can both kill statistical power and misses nuance in the results.\n\n\n\nImage from a paper published in 2024. Although this was a fully crossed factorial experiment, compact letter display was implemented across all treatment combinations, resulting in some nonsensical comparisons among some more informative contrasts.\nImplementing compact letter display can kill statistical power (the probability of detecting true differences) because it requires that all pairwise comparison being made. Doing this, especially when there are many treatment levels, has its perils. The biggest problem is that this creates a multiple testing problem. The RCBD example in this guide has 42 treatments, resulting in a total of 861 comparisons (\\(=42*(42-1)/2\\)), that are then adjusted for multiple tests. With that many tests, a severe adjustment is likely and hence things that are different are not detected. With so many tests, it could be that there is an overall effect due to treatment, but they all share the same letter!\nThe second problem is one of interpretation. Just because two treatments or varieties share a letter does not mean they are equivalent. It only means that they were not found to be different. A funny distinction, but alas. There is an entire branch of statistics, ‚Äòequivalence testing‚Äô devoted to just this topic - how to test if two things are actually the same. This involves the user declaring a maximum allowable numeric difference for a variable in order to determine if two items are statistically different or equivalent - something that these pairwise comparisons are not doing.]\nAnother problem is that doing all pairwise comparison may not align with experimental goals. In many circumstances, not every pairwise combination is of any interest or relevance to the study. Additionally, complex treatment structure may necessitate custom contrasts that highlight differences between the marginal estimate of multiple treatments versus another. For example, there may be two levels of ‚Äòhigh‚Äô nitrogen fertilizer treatment with two different sources (i.e.¬†types of fertilizer). A researcher may want to contrast those two levels together against ‚Äòlow‚Äô nitrogen treatment levels.\nOften, researchers have embedded additional structure in the treatments that is not fully reflected in the statistical model. For example, perhaps a study is looking at five different intercropping mixtures, two that incorporate a legume and three that do not. Conducting all pairwise comparisons with miss estimating the difference due to including a legume in an intercropping mix and not incorporating one. Soil fertility and other agronomic studies often have complex treatment structure. When it is not practical or financially feasible to have a full factorial experiment, embedding different treatment combinations in the main factor of analysis can accomplish this. This is a good study design approach, but compact letter display is an inefficient way to report results. In such cases, custom contrasts are a better choice for hypothesis testing.The emmeans chapter covers how to do this.\n\n\n2.0.5 Final Thoughts\nGood statistical analysis requires a thoughtful, intentional approach. If you have gone to the trouble to conduct a well designed experiment or assemble a useful data set, take the time and effort to analyze it properly.\n\n\n\n\nBroman, Karl W., and Kara H. Woo. 2018. ‚ÄúData Organization in Spreadsheets.‚Äù The American Statistician 72 (1): 2‚Äì10. https://doi.org/10.1080/00031305.2017.1375989.\n\n\nDushoff, Jonathan, Morgan P. Kain, and Benjamin M. Bolker. 2019. ‚ÄúI Can See Clearly Now: Reinterpreting Statistical Significance.‚Äù Methods in Ecology and Evolution 10 (6): 756‚Äì59. https://doi.org/https://doi.org/10.1111/2041-210X.13159.\n\n\nGreenland, Sander, Stephen J. Senn, Kenneth J. Rothman, John B. Carlin, Charles Poole, Steven N. Goodman, and Douglas G. Altman. 2016. ‚ÄúStatistical Tests, P Values, Confidence Intervals, and Power: A Guide to Misinterpretations.‚Äù European Journal of Epidemiology 31 (4): 337‚Äì50. https://doi.org/10.1007/s10654-016-0149-3.\n\n\nWasserstein, Ronald L., and Nicole A. Lazar. 2016. ‚ÄúThe ASA Statement on p-Values: Context, Process, and Purpose.‚Äù The American Statistician 70 (2): 129‚Äì33. https://doi.org/10.1080/00031305.2016.1154108.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Zen and the Art of Statistical Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/background.html",
    "href": "chapters/background.html",
    "title": "3¬† Mixed Model Background",
    "section": "",
    "text": "3.1 Mixed model terms\nMixed-effects models are called ‚Äúmixed‚Äù because they simultaneously model fixed and random effects. These are also called ‚Äúmultilevel or‚Äùhierarchical‚Äù models in reference to groups or clusters with a hierarchical structure where we expect the groups to have correlations between their groups members.\nThis tutorial concerns (general) linear mixed models (sometimes abbreviated as ‚ÄúLMM‚Äù) where the dependent variable, when conditioned on the independent variable(s), follows a normal distribution. These are a special case of generalized linear mixed models (sometimes abbreviated ‚ÄúGLMM‚Äù), which allow the dependent variable to follow non-normal distributions. We will only be discussing the former in this tutorial.\nFixed effects represent population-level average effects that should persist across experiments. We often view our treatments or experimental interventions as fixed effects. Fixed effects are parameters drawn the entire population and/or associated with particular levels of a treatment (e.g.¬†levels of nitrogen fertilizer). Fixed effects are similar to the parameters found in traditional regression techniques like ordinary least squares.\nRandom effects are discrete units sampled from some population (e.g.¬†plots, participants), and thus they are categorical. Random effects are associated with experimental units drawn at random from a population with a probability distribution. Random effects are especially useful when we have (1) many levels (e.g., many species or blocks), (2) relatively little data on each level (although we need multiple samples from most of the levels), and/or (3) uneven sampling across levels. The general thinking is that random effects represent a sample of a population you want to make inference on.\nPlease read this section and refer back to if when you forget what these terms mean.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Mixed Model Background</span>"
    ]
  },
  {
    "objectID": "chapters/background.html#mixed-model-terms",
    "href": "chapters/background.html#mixed-model-terms",
    "title": "3¬† Mixed Model Background",
    "section": "",
    "text": "Table¬†3.1: Terms definitions\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nRandom effect\nAn independent variable where the levels being estimated compose a random sample from a population whose variance will be estimated\n\n\nFixed effect\nAn independent variable with specific, predefined levels to estimate\n\n\nExperimental unit\nThe smallest unit being used for analysis. This could be an animal, a field plot, a person, a meat or muscle sample. The unit may be assessed multiple times or through multiple point in time. When the analysis is all said and done, the predictions occur at this level.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Mixed Model Background</span>"
    ]
  },
  {
    "objectID": "chapters/background.html#models",
    "href": "chapters/background.html#models",
    "title": "3¬† Mixed Model Background",
    "section": "3.2 Models",
    "text": "3.2 Models\nRecall simple linear regression with intercept (\\(\\beta_0\\)) and slope (\\(\\beta_1\\)):\n\\[  Y_{i} = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\]\n\\(Y_i\\) is the dependent variable, and \\(x_i\\) is the independent variable. The value for \\(\\beta_0\\) is the overall average for \\(Y_i\\) and \\(\\beta_1\\) is the change in \\(Y\\) as \\(X\\) changes. The slope for \\(\\beta_1\\) could represent the effects of increasing quantities of nitrogen fertilizer on crop growth.\nThe errors,1 \\(\\epsilon_i\\), or residual are independently and identically distributed (sometimes called ‚Äúiid‚Äù) following a normal distribution for general linear mixed models.\n1¬†For the rest of this tutorial we will be using the term ‚Äúresidual‚Äù and not ‚Äúerror term‚Äù to reflect current practices. The residual is calculated as \\(Y - XB\\), the gap between the observed value for \\(Y_i\\) and the predicted value, \\(\\hat{Y_i}\\)\\[e_i \\sim N(0, \\sigma I_n)\\] These are important model assumptions that we will revisit frequently in this tutorial (and later on we will explore relaxing these assumptions).\nThere is also an expected conditional distribution for \\(Y_i\\), but we will not be discussing this again.\n\\[ Y_i|x_i \\simùëÅ(\\mathbf{X \\beta}, \\sigma^2/r_i) \\]\nIn least squares estimation, the slope and intercept are chosen in a way so that the residual sum of squares is minimized. If we consider this model in a mixed model framework, \\(\\beta_0\\) and \\(\\beta_1\\) would be fixed effects (also known as the population-averaged values).\nExtending this example to a mixed model, we can consider adding another term, \\(r_{j}\\) that reflects levels sampled from a population that represent a random subset of a larger population:\n\\[  Y_{ij} = \\beta_0 + \\beta_1 x_i + r_j + \\epsilon_{ij} \\] In an agronomic field trial, \\(r_{j}\\) could be a random effect for block (i.e.¬†the spatial positioning of a group of treatments). The random effect can be thought of as each block‚Äôs deviation from the fixed intercept parameter (that is, \\(\\beta_0\\)). Like the residuals term, \\(r_i\\) is independently and identically distributed (sometimes referred to as ‚Äúiid‚Äù):\n\\[r_i \\sim N(0, \\sigma_b)\\] While random effects do not have to be normally distributed, that is the most common and most easily estimated. These are considered ‚Äúrandom intercepts‚Äù where they all share a common \\(B_0\\) and \\(B_1\\).\n\n\n\n\n\nExample mixed model with random intercepts but identical slopes\n\n\n\n3.2.1 Random slopes models\nOther alternatives to the random intercept model include modelling random slopes where the slope between X and Y changes according to the random effect:\n\\[  Y_{ij} = \\beta_0 + (\\beta_1 + r_j)X_{ij} + \\epsilon_{ij}\\]\nIn this model, the \\(\\beta\\) is the overage effect of X on Y, and \\(r_i\\) is a random effect for block \\(j\\), and all observations share a common intercept, \\(\\beta_0\\).\n\n\n\n\n\nMixed model with random slopes but identical intercepts.\n\n\nThe other alternative is including both a random slope and a random intercept:\n\\[  Y_{ij} = (\\beta_0 + r1_j) + (\\beta_1 + r2_j)X_{ij} + \\epsilon_{ij}\\]\nIn this model, \\(a_i\\) and \\(b_i\\) are random effects for subject \\(i\\) applied to the intercept and slope, respectively. Predictions would vary depending on each subject‚Äôs slope and intercept terms:\n\n\n\n\n\nMixed Model with random intercept and slope\n\n\nAnecdotally, these models appear to be uncommon in agronomic trials.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Mixed Model Background</span>"
    ]
  },
  {
    "objectID": "chapters/background.html#formula-notation",
    "href": "chapters/background.html#formula-notation",
    "title": "3¬† Mixed Model Background",
    "section": "3.3 R Formula Syntax for Random and Fixed Effects",
    "text": "3.3 R Formula Syntax for Random and Fixed Effects\nFormula notation is often used in the R syntax for linear models. It follows this convention: \\(Y ~ X\\), where \\(Y\\) is the dependent variable (the response) and \\(X\\) is/are the independent variable(s) that is, the experimental treatments or interventions.\n\nmy_formula &lt;- formula(Y ~ treatment1 + treatment2)\nclass(my_formula)\n\n[1] \"formula\"\n\n\nThe package ‚Äòlme4‚Äô has some additional conventions regarding the formula. Random effects are put in parentheses and a 1| is used to denote random intercepts (rather than random slopes). The table below provides several examples of random effects in mixed models. The names of grouping factors are denoted g, g1, and g2, and covariates as x.\n\n\n\n\n\n\n\n\nFormula\nAlternative\nMeaning\n\n\n\n\n(1|g)\n1 + (1|g)\nRandom intercept with a fixed mean\n\n\n(1|g1/g2)\n(1| 1) + (1|g1:g2)\nIntercept varying among g1 and g2 within g1\n\n\n(1|g1) + (1|g2)\n1 + (1|g1) + (1|g2)\nIntercept varying among g1 and g2\n\n\nx + (x|g)\n1 + x + (1 + x|g)\nCorrelated random intercept and slope\n\n\nx + (x||g)\n1 + x + (1|g) + (0 + x|g)\nUncorrelated random intercept and slope\n\n\n\nThe first example, (1|g) suffices for most mixed models and is the only structure used in this guide.\nThe biggest advantage of mixed models is their incredible flexibility. They handle clustered individuals as well as repeated measures. They handle crossed as well as nested random factors. The biggest disadvantage of mixed models, at least for someone new to them, is their incredible flexibility. It‚Äôs easy to mis-specify a mixed model, and this is a place where a little knowledge can be dangerous.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Mixed Model Background</span>"
    ]
  },
  {
    "objectID": "chapters/model-flow.html",
    "href": "chapters/model-flow.html",
    "title": "4¬† Model Preparation and Flow",
    "section": "",
    "text": "4.1 Define the Research Question(s)\nThis chapter provides a brief introduction to the steps involved in data analysis using linear mixed models and some thoughts on data quality and data interpretation.\nAnalyzing data using linear mixed models involves several key steps, from data preparation to model interpretation. Here‚Äôs a structured approach:\nIt is important to define what is your question that you want to answer with your research data because it directly influences the model specification, interpretation, and validity. This was determined prior to design of the experiment and data acquisition. However, things change as an experiment unfolds, and sometimes experimental aims become lost in translation between the original grant proposal for a prioject and when a graduate student eventually implements the experiment. Before embarking on an analysis, write down the goals of analysis in the most precise terms possible. Examples:\nThe first two examples are asking for specific inference on the estimated effects of a particular intervention. The third example is a hypothesis test, and the fourth example is also inferential, but focused on variance instead of point estimates.\nFor example, We conducted a randomized field trial to study wheat yield response to different nitrogen fertilizer treatments. In this case, we want to analyze how wheat yield responded to the fertilizer treatments. The next step is to identify the dependent variable (yield), and the fixed effects (treatment) and random effects (replications) in the experiment design.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Model Prep & Workflow</span>"
    ]
  },
  {
    "objectID": "chapters/model-flow.html#define-the-research-questions",
    "href": "chapters/model-flow.html#define-the-research-questions",
    "title": "4¬† Model Preparation and Flow",
    "section": "",
    "text": "We want to know how much barley yield change as the result of this new fertilizer source compared to another source.\n\n\nWe want to estimate the change in milk yield with each unit change in cow parity.\n\n\nWe want to know if the drug has a stronger effect on reducing cholesterol rates where a difference of 5 units or more is considered a meaningful result.\n\n\nWe want to know the relative contributions of location, year and cultivar influencing barley yield across southern Idaho.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Model Prep & Workflow</span>"
    ]
  },
  {
    "objectID": "chapters/model-flow.html#data-integrity-checks",
    "href": "chapters/model-flow.html#data-integrity-checks",
    "title": "4¬† Model Preparation and Flow",
    "section": "4.2 Data integrity checks",
    "text": "4.2 Data integrity checks\nThe first thing is to make sure the data is what we expect. Here are the steps to verify our data:\n\n4.2.1 1. Check structure of the data\nIn this step, we need to make sure that class of variables in data is as expected e.g.¬†replication or block and Year are often in the numeric format after import into R. But, for analysis the replication unit and other variables that are not truly numerical1 needs to be in a ‚Äòcharacter‚Äô or ‚Äòfactor‚Äô format. The dependent variable must also be in the correct format for its intended data type. For this tutorial, the dependent variable is expected to be numeric in all cases2, although categorical outcomes are certainly plausible.\n1¬†no one views 2025 as 2,205.2¬†Since this tutorial is exclusively focused on general linear models and is not addressing generalized scenarios at all.\n\n\n\n\n\nWarning\n\n\n\nMost R import functions (e.g.¬†read.csv(), read_csv(), read_excel()) interprets continuously varying numbers (correctly) as numerical variables, but if they do not, that is often a signal that there is an unexpected character in a particular column of data that is incorrectly read as non-numerical. Perhaps a comma is present when a period is expected, or ‚ÄúN/A‚Äù instead of ‚ÄúNA‚Äù. This is an indicator to double check your data to ensure there are no errors in it.\n\n\nWe can use the base code str() in R to look at the class of each variable in the data set.\n\nstr(dataset)\n\nThis code output the data class of each variable present in the data set.\nThe example code below shows the conversion of rep from numeric to factor class and conversion of yield from character to numeric class.\n\ndataset$block &lt;- as.factor(dataset$block)\ndataset$yield &lt;- as.numeric(dataset$yield)\n\n\n\n\n\n\n\nfactor versus character class\n\n\n\nOften, factor and character object classes are used interchangeable. But we need to keep in mind the difference in these two classes. A character variable represents data stored in a text or ‚Äústring‚Äù format. A factor variable is categorical variable type with values stored as set levels. Most linear modeling packages in R expect categorical independent variables to be formatted as factors, but most will also automatically convert character variables to factors.\n\n\n\n\n4.2.2 2. Inspect the independent variables\nRunning a cross tabulation across treatments and replications is generally enough to make sure the expected levels are present in the data.\n\ntable(dataset$trt, dataset$block)\n\nThe output from this code will give us the number of observations in each replication for a given treatment. It‚Äôs useful to check (1) all the expected levels of categorical data are present and no additional levels are present (e.g.¬†‚Äúhigh‚Äù and ‚ÄúHigh‚Äù); (2) the counts are as expected; and (3) what the balance of treatments is. Are they perfectly balanced? Slightly unbalanced? Very unbalanced? Is one treatment combination missing altogether? Depending on the context, these are resolvable issues. The main goal is to check what the data look like after import compared to your personal understanding of the data.\n\n\n4.2.3 3. Check the extent of missing data\n\ncolSums(is.na(dataset))\n\nThis will give you a number of missing values in each variable. Missing data are a normal part of experiments and in most cases not a problem. However, if there is more missingness than expected, it is good to check that the data are correctly entered and nothing wrong happened during import.\n\n\n\n\n\n\nMissingness versus Zeros\n\n\n\nWe occasionally see users substitute missing data with zeros. This is not recommended at all unless there is a clear reason. If I planted an experiment and one plot failed to yield at all due to a field conditions, it is reasonable to assign a zero to that plot and variable. However, if I failed to plant anything in a plot due to user error, assigning a zero is not an accurate description of what occurred. Likewise, it not appropriate to treat true zeros as missing.\n\n\n\n\n4.2.4 4. Inspect the dependent variable and all over continuous variables\nCheck the dependent variable and all continuous variables (i.e.¬†covariates) to ensure their distributions are following expectations. A histogram is often quite sufficient to accomplish this. This is designed to be a quick check, so there is no need to spend time prettifying the plot.\nThe goal of this step is to verify the distribution of the variable and to make sure there are no anomalies in the data such as zero-inflation, right or left skewness, or any extreme observations (high or low).\n\nhist(data$yield)\n\nData are not expected to be normally distributed at this point, so don‚Äôt bother running any normality tests like the Shapiro-Wilk test. This histogram is a check to ensure that the data are entered correctly and appear valid. It requires a mixture of domain knowledge and statistical training to know this, but over time, if you look at these plots regularly, you will gain a feel for what your data should look like at this stage.\n\n\n4.2.5 Next Steps\nThe purpose of these checks is to help us find any data errors that ought to be fixed prior to model fitting. They are designed to be done quickly and should be conducted for every analysis if you have not previously inspected your data as thus. We do this before every analysis and often discover surprising things! It is best to discover these things early, since they are likely to impact the final analysis.\nIf do you identify issues, check your data and correct as necessary. Once a data set passes these checks, we can move to next step: model fitting.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Model Prep & Workflow</span>"
    ]
  },
  {
    "objectID": "chapters/model-flow.html#model-fitting---formula-syntax-and-expectations",
    "href": "chapters/model-flow.html#model-fitting---formula-syntax-and-expectations",
    "title": "4¬† Model Preparation and Flow",
    "section": "4.3 Model fitting - Formula syntax and expectations",
    "text": "4.3 Model fitting - Formula syntax and expectations\nIn this guide, we use lme4 and nlme packages to fit linear mixed models. These are similar packages that follow a similar framework and syntax. The general framework for lmer() and lme() models is to specify the response variable, fixed factors, and random factors and follow the R generic function for formula(). For this demonstration, let‚Äôs assume the dependent variable is ‚Äúyield‚Äù, ‚Äútrt‚Äù is a fixed effect and ‚Äúblock‚Äù is a random effect.\nThe code below shows the R syntax for a mixed model with one fixed and one random effect:\n\nlme4nlme\n\n\n\nmodel_lmer &lt;- lmer(yield ~ trt + (1|block),\n                   data = dataset, \n                   na.action = na.exclude)\n\n\n\n\nmodel_lme &lt;- lme(yield ~ trt ,\n                  random = ~ 1|block,\n                  data = data1, \n                  na.action = na.exclude)\n\n\n\n\nThe parentheses are used to indicate a random effect, and this particular notation (1|block) indicates that a ‚Äòrandom intercept‚Äô model is being fit 3. This is the most common approach. It means there is one overall effect fit for each block. Here, note that random effects are specified differently in the lmer() and nlme() models.\n3¬†Please refer to Chapter 3\n\n\n\n\n\nna.action = na.exclude\n\n\n\nYou may have noticed the final argument for na.action in the model statement.\nThe argument na.action = na.exclude provides instructions for how to handle missing data. na.exclude removes the missing data points before proceeding with the analysis. When any obervation-levels model outputs is generated (e.g.¬†predictions, residuals), they are padded in the appropriate place to account for missing data. This is handy because it makes it easier to add those results to the original data set if so desired.\nWe use the argument na.action = na.exclude as instruction for how to handle missing data: conduct the analysis, adjusting as needed for the missing data, and when prediction or residuals are output, padding them in the appropriate places for missing data so they can be easily merged into the main data set if need be.\nEven when there are no missing data and this step is not necessary, it is a good habit to be in.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Model Prep & Workflow</span>"
    ]
  },
  {
    "objectID": "chapters/model-flow.html#check-model-assumptions",
    "href": "chapters/model-flow.html#check-model-assumptions",
    "title": "4¬† Model Preparation and Flow",
    "section": "4.4 Check model assumptions",
    "text": "4.4 Check model assumptions\nLinear mixed models rely on linearity, independence, normality, homoscedasticity of residuals. Violating these assumptions can lead to biased estimates, inefficient inference, and convergence issues. Always check model assumptions, and if they are unmet, consider alternative model specifications.\n\n\n\n\n\n\n‚Äòiid‚Äô assumption for residuals\n\n\n\nIn these model, the error terms, \\(\\epsilon\\) are assumed to be ‚Äúiid‚Äù, that is, independently and identically distributed. This means they are expected to be normally distributed with a mean of 0 standard deviation of \\(\\sigma\\), or more specifically, \\(\\sigma \\mathbf{I}\\), which refers to constant variance and a covariance of zero between residuals.\n\n\nIn this guide, we well be testing these assumptions by using graphical methods. This can be done in two ways:\n\n4.4.1 Original method\nWe can use base plot() function in R for lme4 and nlme objects to check the homoscedasticity (residuals vs.¬†fitted values plot) of residuals.\n\nlme4nlme\n\n\n\nplot(model_lmer, resid(., scaled=TRUE) ~ fitted(.), \n     xlab = \"fitted values\", ylab = \"studentized residuals\")\n\n\n\n\nplot(model_lme, resid(., scaled=TRUE) ~ fitted(.), \n     xlab = \"fitted values\", ylab = \"studentized residuals\")\n\n\n\n\nIn this output, we expect to see a plot with random and uniform distribution of points. If we notice any specific pattern in the distribution of points, we need to look into the model structure and response variable distribution closely.\nTo check the normality of the residuals, we need to extract the residuals first using the resid() function and then generating a qq-plot:\n\nlme4nlme\n\n\n\nqqnorm(resid(model_lmer), main = NULL); qqline(resid(model_lmer))\n\n\n\n\nqqnorm(resid(model_lme), main = NULL); qqline(resid(model_lme))\n\n\n\n\nThe interpretation of the qq-plots generated from this code chunk is: if residuals falls closely along the 45-degree qq-line, it suggests normality of the residuals. However, if there is strong deviation of residual points from the qq-line, consider a different model that fits the data better. How to do this is way beyond the scope of this guide.\n\n\n4.4.2 New Method\nNowadays, we can take advantage of the performance package package, which provides a comprehensive suite of diagnostic plots.\nThe diagnostic plots we created above can be created using one function check_model() from the performance package.\n\n\n\n\n\n\n\n\nNote\n\n\n\nRead the documentation for check_model() to find what other checks this function can do. If you would like to check all assumptions you can use the argument check = \"all\".\n\n\n\nlme4nlme\n\n\n\ncheck_model(model_lmer, check = c('normality', 'linearity'))\n\n\n\n\ncheck_model(model_lme, check = c('normality', 'linearity'))",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Model Prep & Workflow</span>"
    ]
  },
  {
    "objectID": "chapters/model-flow.html#inference",
    "href": "chapters/model-flow.html#inference",
    "title": "4¬† Model Preparation and Flow",
    "section": "4.5 Inference",
    "text": "4.5 Inference\nAfter verifying the assumptions of model, we can move to the inference of model. Based on analysis goals, we can either conduct analysis of variance using anova() function or we can estimate marginal means using emmeans() function from the emmeans package. We can also run a post-hoc comparison to evaluate the pairwise comparison or contrasts using estimated means.\n\n\n\n\n\n\nModel inference\n\n\n\nPlease remember that conclusions should not be drawn from the model that doesn‚Äôt meet linear mixed model assumptions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Model Prep & Workflow</span>"
    ]
  },
  {
    "objectID": "chapters/rcbd.html",
    "href": "chapters/rcbd.html",
    "title": "5¬† Randomized Complete Block Design",
    "section": "",
    "text": "5.1 Background\nRandomized complete block design (RCBD) is very common design where experimental treatments are applied at random to experimental units within each block. The block can represent a spatial or temporal unit or even different technicians taking data. The blocks are intended to control for a nuisance source of variation, such as over time, spatial variance, changes in equipment or operators, or myriad other causes. They are a random effect where the actual blocks used in the study are a random sample of a distribution of other blocks.\nThe statistical model:\n\\[y_{ij} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}\\] Where:\n\\(\\mu\\) = overall experimental mean \\(\\alpha\\) = treatment effects (fixed) \\(\\beta\\) = block effects (random) \\(\\epsilon\\) = error terms\n\\[ \\epsilon \\sim N(0, \\sigma)\\]\n\\[ \\beta \\sim N(0, \\sigma_b)\\]\nBoth the overall error and the block effects are assumed to be normally distributed with a mean of zero and standard deviations of \\(\\sigma\\) and \\(\\sigma_B\\), respectively.",
    "crumbs": [
      "Experiment designs",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Randomized Complete Block Design</span>"
    ]
  },
  {
    "objectID": "chapters/rcbd.html#example-analysis",
    "href": "chapters/rcbd.html#example-analysis",
    "title": "5¬† Randomized Complete Block Design",
    "section": "5.2 Example Analysis",
    "text": "5.2 Example Analysis\nFirst, load the libraries for analysis and estimation:\n\nlme4nlme\n\n\n\nlibrary(lme4); library(lmerTest); library(emmeans)\nlibrary(dplyr); library(performance)\n\n\n\n\nlibrary(nlme); library(performance); library(emmeans)\nlibrary(dplyr)\n\n\n\n\nThis data set is for a single wheat variety trial conducted in Aberdeen, Idaho in 2015. The trial includes 4 blocks and 42 different treatments (wheat varieties in this case). This experiment consists of a series of plots (the experimental unit) laid out in a rectangular grid in a farm field. The goal of this analysis is the estimate the yield of each variety and the determine the relative rankings of the varieties.\n\nvar_trial &lt;- read.csv(here::here(\"data\", \"aberdeen2015.csv\"))\n\n\nTable of variables in the data set\n\n\n\n\n\n\nblock\nblocking unit\n\n\nrange\ncolumn position for each plot\n\n\nrow\nrow position for each plot\n\n\nvariety\ncrop variety (the treatment) being evaluated\n\n\nyield_bu_a\nyield (bushels per acre)\n\n\n\n\n5.2.1 Data integrity checks\nThe first thing is to make sure the data is what we expect.We will the steps explained in Chapter 4. Here are the steps to verify our data:\n\nCheck structure of the data\n\n\nstr(var_trial)\n\n'data.frame':   168 obs. of  5 variables:\n $ block     : int  4 4 4 4 4 4 4 4 4 4 ...\n $ range     : int  1 1 1 1 1 1 1 1 1 1 ...\n $ row       : int  1 2 3 4 5 6 7 8 9 10 ...\n $ variety   : chr  \"DAS004\" \"Kaseberg\" \"Bruneau\" \"OR2090473\" ...\n $ yield_bu_a: num  128 130 119 115 141 ...\n\n\nThese look okay except for block, which is currently coded as integer (numeric). We don‚Äôt want run a regression of block, where block 2 has twice the effect of block 1, and so on. So, converting it to a character will fix that. It can also be converted to a factor, but character variables are a bit easier to work with, and ultimately, equivalent to factor conversion\n\nvar_trial$block &lt;- as.character(var_trial$block)\n\n\nInspect the independent variables\n\nNext, check the independent variables. Running a cross tabulations is often sufficient to ascertain this.\n\ntable(var_trial$variety, var_trial$block)\n\n                        \n                         1 2 3 4\n  06-03303B              1 1 1 1\n  Bobtail                1 1 1 1\n  Brundage               1 1 1 1\n  Bruneau                1 1 1 1\n  DAS003                 1 1 1 1\n  DAS004                 1 1 1 1\n  Eltan                  1 1 1 1\n  IDN-01-10704A          1 1 1 1\n  IDN-02-29001A          1 1 1 1\n  IDO1004                1 1 1 1\n  IDO1005                1 1 1 1\n  Jasper                 1 1 1 1\n  Kaseberg               1 1 1 1\n  LCS Artdeco            1 1 1 1\n  LCS Biancor            1 1 1 1\n  LCS Drive              1 1 1 1\n  LOR-833                1 1 1 1\n  LOR-913                1 1 1 1\n  LOR-978                1 1 1 1\n  Madsen                 1 1 1 1\n  Madsen / Eltan (50/50) 1 1 1 1\n  Mary                   1 1 1 1\n  Norwest Duet           1 1 1 1\n  Norwest Tandem         1 1 1 1\n  OR2080637              1 1 1 1\n  OR2080641              1 1 1 1\n  OR2090473              1 1 1 1\n  OR2100940              1 1 1 1\n  Rosalyn                1 1 1 1\n  Stephens               1 1 1 1\n  SY  Ovation            1 1 1 1\n  SY 107                 1 1 1 1\n  SY Assure              1 1 1 1\n  UI Castle CLP          1 1 1 1\n  UI Magic CLP           1 1 1 1\n  UI Palouse             1 1 1 1\n  UI Sparrow             1 1 1 1\n  UI-WSU Huffman         1 1 1 1\n  WB 456                 1 1 1 1\n  WB 528                 1 1 1 1\n  WB1376 CLP             1 1 1 1\n  WB1529                 1 1 1 1\n\n\nThere are 42 varieties and there appears to be no misspellings among them that might confuse R into thinking varieties are different when they are actually the same. R is sensitive to case and white space, which can make it easy to create near duplicate treatments, such as ‚Äúeltan‚Äù and ‚ÄúEltan‚Äù. There is no evidence of that in this data set. Additionally, it is perfectly balanced, with exactly one observation per treatment per rep. Please note that this does not tell us anything about the extent of missing data.\n\nCheck the extent of missing data\n\nHere is a quick check to count the number of missing data in each column.\n\ncolSums(is.na(var_trial))\n\n     block      range        row    variety yield_bu_a \n         0          0          0          0          0 \n\n\nAlas, no missing data.\n\nInspect the dependent variable\n\nLast, check the dependent variable by creating a histogram.\n\n\n\n\n\n\n\n\n\nFigure¬†5.1: Histogram of the dependent variable.\n\n\n\n\n\nhist(var_trial$yield_bu_a, main = NA, xlab = \"yield\")\n\nThe range is roughly falling into the range we expect. We (the authors) know this from talking with the person who generated the data, not through our own intuition. There are no large spikes of points at a single value (indicating something odd), nor are there any extreme values (low or high) that might indicate problems.\nThis data set is ready for analysis!\n\n\n5.2.2 Model Building\n\n\nRecall the model:\n\\[y_{ij} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}\\]\nFor this model, \\(\\alpha_i\\) is the variety effect (fixed) and \\(\\beta_j\\) is the block effect (random).\nHere is the R syntax for the RCBD statistical model:\n\nlme4nlme\n\n\n\nmodel_rcbd_lmer &lt;- lmer(yield_bu_a ~ variety + (1|block),\n                   data = var_trial, \n                   na.action = na.exclude)\n\n\n\n\nmodel_rcbd_lme &lt;- lme(yield_bu_a ~ variety,\n                  random = ~ 1|block,\n                  data = var_trial, \n                  na.action = na.exclude)\n\n\n\n\n\n\n5.2.3 Check Model Assumptions\n\n\n\n\n\n\n\n\nNote\n\n\n\nR syntax for checking model assumptions is the same for lme4 and nlme.\n\n\nRemember those iid assumptions? Let‚Äôs make sure we actually met them.\n\n5.2.3.1 Original Method\nWe will start inspecting the model assumptions by for checking the homoscedasticity (constant variance) first using a plot() function in base R.\n\n\n\n\n\n\n\n\n\nFigure¬†5.2: Plot of residuals versus fitted values\n\n\n\n\n\nplot(model_rcbd_lmer, resid(., scaled=TRUE) ~ fitted(.), \n     xlab = \"fitted values\", ylab = \"studentized residuals\")\n\nWe have observed a random and uniform distribution of points. This looks good!\nAs explained in Chapter 4 we will first extract residuals usingresid() and then generate a qq-plot and line.\n\n\n\n\n\n\n\n\n\nFigure¬†5.3: QQ-plot of residuals\n\n\n\n\n\nqqnorm(resid(model_rcbd_lmer), main = NULL); qqline(resid(model_rcbd_lmer))\n\nThis is reasonably good. Things do tend to fall apart at the tails a little, so this is not concerning.\n\n\n5.2.3.2 New Method\nHere, we will use the check_model() function from the performance package to look at the normality and linearity of the residuals.\n\ncheck_model(model_rcbd_lmer,  check = c('qq', 'linearity', 'reqq'), detrend=FALSE, alpha = 0)\n\n\n\n\n\n\n\n\n\n\n\n5.2.4 Inference\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\nR syntax for estimating model marginal means is the same for lme4 and nlme.\nEstimates for each treatment level can be obtained with the emmeans package.\n\nrcbd_emm &lt;- emmeans(model_rcbd_lmer, ~ variety)\nas.data.frame(rcbd_emm) %&gt;% arrange(desc(emmean))\n\n variety                  emmean       SE    df  lower.CL upper.CL\n Rosalyn                155.2703 7.212203 77.85 140.91149 169.6292\n IDO1005                153.5919 7.212203 77.85 139.23310 167.9508\n OR2080641              152.6942 7.212203 77.85 138.33536 167.0530\n Bobtail                151.6403 7.212203 77.85 137.28149 165.9992\n UI Sparrow             151.6013 7.212203 77.85 137.24245 165.9601\n Kaseberg               150.9768 7.212203 77.85 136.61794 165.3356\n IDN-01-10704A          148.9861 7.212203 77.85 134.62729 163.3450\n 06-03303B              148.8300 7.212203 77.85 134.47116 163.1888\n WB1529                 148.2445 7.212203 77.85 133.88568 162.6034\n DAS003                 145.2000 7.212203 77.85 130.84116 159.5588\n IDN-02-29001A          144.5755 7.212203 77.85 130.21665 158.9343\n Bruneau                143.9900 7.212203 77.85 129.63116 158.3488\n SY 107                 143.6387 7.212203 77.85 129.27987 157.9975\n WB 528                 142.9752 7.212203 77.85 128.61633 157.3340\n OR2080637              141.7652 7.212203 77.85 127.40633 156.1240\n Jasper                 141.2968 7.212203 77.85 126.93794 155.6556\n UI Magic CLP           139.5403 7.212203 77.85 125.18149 153.8992\n Madsen                 139.2671 7.212203 77.85 124.90826 153.6259\n LCS Biancor            139.1110 7.212203 77.85 124.75213 153.4698\n SY  Ovation            138.6426 7.212203 77.85 124.28375 153.0014\n OR2090473              137.8229 7.212203 77.85 123.46407 152.1817\n Madsen / Eltan (50/50) 136.9642 7.212203 77.85 122.60536 151.3230\n UI-WSU Huffman         135.4810 7.212203 77.85 121.12213 149.8398\n Mary                   134.8564 7.212203 77.85 120.49762 149.2153\n Norwest Tandem         134.3490 7.212203 77.85 119.99020 148.7079\n Brundage               134.0758 7.212203 77.85 119.71697 148.4346\n IDO1004                132.5145 7.212203 77.85 118.15568 146.8733\n DAS004                 132.2413 7.212203 77.85 117.88245 146.6001\n Norwest Duet           132.0852 7.212203 77.85 117.72633 146.4440\n Eltan                  131.4606 7.212203 77.85 117.10181 145.8195\n LCS Artdeco            130.8361 7.212203 77.85 116.47729 145.1950\n UI Palouse             130.4848 7.212203 77.85 116.12600 144.8437\n LOR-978                130.4458 7.212203 77.85 116.08697 144.8046\n LCS Drive              128.7674 7.212203 77.85 114.40858 143.1262\n Stephens               127.1671 7.212203 77.85 112.80826 141.5259\n OR2100940              126.1523 7.212203 77.85 111.79342 140.5111\n UI Castle CLP          125.5277 7.212203 77.85 111.16891 139.8866\n WB1376 CLP             123.6932 7.212203 77.85 109.33439 138.0521\n LOR-833                122.7565 7.212203 77.85 108.39762 137.1153\n LOR-913                118.7752 7.212203 77.85 104.41633 133.1340\n WB 456                 118.4629 7.212203 77.85 104.10407 132.8217\n SY Assure              111.0468 7.212203 77.85  96.68794 125.4056\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\nThis table indicates the estimated marginal means (‚Äúemmeans‚Äù, sometimes called ‚Äúleast squares means‚Äù), the standard error (‚ÄúSE‚Äù) of those means, the degrees of freedom and the upper and lower bounds of the 95% confidence interval. As an additional step, the emmeans were sorted from largest to smallest.\nAt this point, the analysis goals have been met: we know the estimated means for each treatment and their rankings.\nIf you want to run ANOVA, it can be done quite easily. By default, sequential (type I) sums of squares is used by nlme, but partial (type 3) sums of squares is also possible. Containment is the only degrees of freedom calculation method enabled in nlme. The lmer-extender package **lmerTest* implements type 3 tests and the Kenward-Rogers method of degrees of freedom approximation by default.1\n1¬†The Type I method is called ‚Äúsequential‚Äù sum of squares because it adds terms to the model one at a time; while type 3 (‚Äúpartial‚Äù) drops model terms one at a time. This can effect the results of an F-test and p-value, but only when a data set is unbalanced across treatments, either due to design or missing data points.In this example, there is only one fixed effect (and the experiment is perfectly balanced), so the sums of squares choice is immaterial.\n\nlme4nlme\n\n\n\nanova(model_rcbd_lmer, type = \"1\")\n\nType I Analysis of Variance Table with Satterthwaite's method\n        Sum Sq Mean Sq NumDF DenDF F value    Pr(&gt;F)    \nvariety  18354  447.65    41   123  2.4528 8.017e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nanova(model_rcbd_lme, type = \"sequential\")\n\n            numDF denDF   F-value p-value\n(Intercept)     1   123 2514.1283  &lt;.0001\nvariety        41   123    2.4528   1e-04",
    "crumbs": [
      "Experiment designs",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Randomized Complete Block Design</span>"
    ]
  },
  {
    "objectID": "chapters/factorial-design.html",
    "href": "chapters/factorial-design.html",
    "title": "6¬† RCBD Design with Several Crossed Factors",
    "section": "",
    "text": "6.1 Background\nFactorial design involves studying the impact of multiple factors simultaneously. Each factor can have multiple levels, and combinations of these levels form the experimental conditions. This design allows us to understand the main effects of individual factors and their interactions on the response variable. The statistical model for factorial design is: \\[y_{ijk} = \\mu +  \\tau_i+ \\beta_j + (\\tau\\beta)_{ij} + \\delta_k +  \\epsilon_{ijk}\\] Where:\n\\(\\mu\\) = experiment mean\n\\(\\tau\\) = effect of factor A\n\\(\\beta\\) = effect of factor B\n\\(\\tau\\beta\\) = interaction effect of factor A and B.\n\\(\\delta\\) = block effect (random)\nAssumptions of this model includes: independent and identically distributed error terms with a constant variance \\(\\sigma^2\\).",
    "crumbs": [
      "Experiment designs",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Factorial RCBD Design</span>"
    ]
  },
  {
    "objectID": "chapters/factorial-design.html#example-analysis",
    "href": "chapters/factorial-design.html#example-analysis",
    "title": "6¬† RCBD Design with Several Crossed Factors",
    "section": "6.2 Example Analysis",
    "text": "6.2 Example Analysis\nFirst step is to load the libraries required for the analysis:\n\nlme4nlme\n\n\n\nlibrary(lme4); library(lmerTest); library(emmeans)\nlibrary(dplyr); library(broom.mixed); library(performance)\n\n\n\n\nlibrary(nlme); library(broom.mixed); library(emmeans)\nlibrary(dplyr); library(performance)\n\n\n\n\nThe data used in this example analysis is from the UI Department of Animal, Veterinary and Food Sciences. This trial was conducted to study the impact of two fabrication methods (trt) on beef top round quality. It includes 11 replications (animal in this case), 2 locations and 2 different treatment factors.\nThere were two fabrication methods (trt factor) applied to the beef top round which includes alternative and traditional methods. The measurements were taken from superficial and deep locations (location factor) of the muscle. The response variable is a measure of yellowness (b_star) in the meat sample.\nThe objective of this example is to evaluate the individual and interactive effect of location and treatment factors on the yellowness of the meat sample.\n\nlibrary(readxl)\nfactorial &lt;- read_excel(here::here(\"data/factorial.xlsx\"))\ndata1&lt;- factorial \n\n\nTable of variables in the data set\n\n\nanimal\nrandom variable\n\n\nlocation\nlocation factor, 2 levels\n\n\ntrt\ntreatment, 2 levels\n\n\nb_star\nresponse variable\n\n\n\n\n6.2.1 Data Integrity Checks\n\nCheck structure of the data\n\nFirst step is to verify the class of variables, where animal, location, and treatment are supposed to be a factor/character and b1 should be numeric.\n\nstr(data1)\n\ntibble [44 √ó 4] (S3: tbl_df/tbl/data.frame)\n $ animal  : num [1:44] 1 1 1 1 2 2 2 2 3 3 ...\n $ location: chr [1:44] \"superficial\" \"deep\" \"superficial\" \"deep\" ...\n $ trt     : chr [1:44] \"alt\" \"alt\" \"traditional\" \"traditional\" ...\n $ b_star  : num [1:44] 21.5 20.1 17.9 20 16.8 15.2 18.2 18.5 20.3 24.5 ...\n\n\nIn this data, treatment factors, ‚Äòn‚Äô, ‚Äòp‚Äô, & ‚Äòk‚Äô are integers, we need to convert these variables to factor.\n\ndat2 &lt;- data1 |&gt; mutate(animal = as.factor(animal))\n\n\nInspect the independent variables\n\nWe are inspecting levels of independent variables to make sure the expected levels are present in the data.\n\ntable(dat2$animal)\n\n\n 1  2  3  4  5  6  7  8  9 10 11 \n 4  4  4  4  4  4  4  4  4  4  4 \n\ntable(dat2$location, dat2$trt)\n\n             \n              alt traditional\n  deep         11          11\n  superficial  11          11\n\n\nThe design looks well balanced.\n\nCheck the extent of missing data\n\n\ncolSums(is.na(dat2))\n\n  animal location      trt   b_star \n       0        0        0        0 \n\n\nThere are no missing values in this data set.\n\nInspect the dependent variable\n\nThis is the last step is to inspect the dependent variable to ensure it looks as expected.\n\n\n\n\n\n\n\n\n\nFigure¬†6.1: Histogram of the dependent variable.\n\n\n\n\n\nhist(dat2$b_star, main = NA, xlab = \"b1\")\n\nNo extreme values are observed in the dependent variable, and the distribution looks as expected.\n\n\n6.2.2 Model fitting\nModel fitting with R is exactly the same as shown in previous chapters: we need to include all fixed effects, as well as the interaction, which is represented by using the colon indicator ‚Äò:‚Äô.\nThe model syntax is:\nb_star ~  trt + location + trt:location\nwhich can be abbreviated as:\nb_star ~  location*trt\nIn this analysis, location and trt are fixed factors and animal is a random effect.\n\nlme4nlme\n\n\n\nmodel1_lmer &lt;- lmer(b_star ~ location*trt + (1|animal),\n                   data = dat2, \n                   na.action = na.exclude)\ntidy(model1_lmer)\n\n# A tibble: 6 √ó 8\n  effect   group    term            estimate std.error statistic    df   p.value\n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed    &lt;NA&gt;     (Intercept)        19.5      0.868     22.5   24.1  1.20e-17\n2 fixed    &lt;NA&gt;     locationsuperf‚Ä¶    -2.87     0.893     -3.22  30.0  3.11e- 3\n3 fixed    &lt;NA&gt;     trttraditional     -1.18     0.893     -1.32  30.0  1.96e- 1\n4 fixed    &lt;NA&gt;     locationsuperf‚Ä¶     1.81     1.26       1.43  30.0  1.62e- 1\n5 ran_pars animal   sd__(Intercept)     1.97    NA         NA     NA   NA       \n6 ran_pars Residual sd__Observation     2.09    NA         NA     NA   NA       \n\n\n\n\n\nmodel2_lme &lt;- lme(b_star ~ location*trt,\n              random = ~ 1|animal,\n              data = dat2, \n              na.action = na.exclude)\ntidy(model2_lme)\n\n# A tibble: 6 √ó 8\n  effect   group    term            estimate std.error    df statistic   p.value\n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed    &lt;NA&gt;     (Intercept)        19.5      0.868    30     22.5   2.58e-20\n2 fixed    &lt;NA&gt;     locationsuperf‚Ä¶    -2.87     0.893    30     -3.22  3.11e- 3\n3 fixed    &lt;NA&gt;     trttraditional     -1.18     0.893    30     -1.32  1.96e- 1\n4 fixed    &lt;NA&gt;     locationsuperf‚Ä¶     1.81     1.26     30      1.43  1.62e- 1\n5 ran_pars animal   sd_(Intercept)      1.97    NA        NA     NA    NA       \n6 ran_pars Residual sd_Observation      2.09    NA        NA     NA    NA       \n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe tidy() function from the broom.mixed package provides a short summary output of the model.\n\n\n\n\n6.2.3 Check Model Assumptions\n\nlme4nlme\n\n\n\ncheck_model(model1_lmer,  check = c('qq', 'linearity', 'reqq'), detrend=FALSE, alpha=0)\n\n\n\n\n\n\n\n\n\n\n\ncheck_model(model2_lme,  check = c('qq', 'linearity'), detrend=FALSE, alpha=0)\n\n\n\n\n\n\n\n\n\n\n\nThe linearity and homogeneity of variance plots show no trend. There are modest departures in the normality of residuals as indicated by the heavy tails.\n\n\n6.2.4 Inference\nWe can obtain an ANOVA table for the linear mixed model using the function anova(), which works for both lmer() and lme() models.\n\nlme4nlme\n\n\n\nanova(model1_lmer, type = \"3\")\n\nType III Analysis of Variance Table with Satterthwaite's method\n             Sum Sq Mean Sq NumDF DenDF F value   Pr(&gt;F)   \nlocation     42.611  42.611     1    30  9.7104 0.004016 **\ntrt           0.846   0.846     1    30  0.1927 0.663810   \nlocation:trt  9.000   9.000     1    30  2.0510 0.162442   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nanova(model2_lme, type = \"marginal\")\n\n             numDF denDF  F-value p-value\n(Intercept)      1    30 504.2656  &lt;.0001\nlocation         1    30  10.3435  0.0031\ntrt              1    30   1.7506  0.1958\nlocation:trt     1    30   2.0510  0.1624\n\n\n\n\n\nHere we did not observe any difference in group variance of interaction effects. Among all treatment factors, only sampling location had a significant effect on the yellowness in the meat samples.\nLet‚Äôs find estimates for some of the main effects and intercation effects of fixed factors on yellowness in the meat samples.\n\nlme4nlme\n\n\n\nemmeans(model1_lmer, specs = ~ location)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n location    emmean    SE   df lower.CL upper.CL\n deep          18.9 0.744 14.7     17.3     20.5\n superficial   16.9 0.744 14.7     15.3     18.5\n\nResults are averaged over the levels of: trt \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nemmeans(model1_lmer, specs = ~ location)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n location    emmean    SE   df lower.CL upper.CL\n deep          18.9 0.744 14.7     17.3     20.5\n superficial   16.9 0.744 14.7     15.3     18.5\n\nResults are averaged over the levels of: trt \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nemmeans(model1_lmer, specs = ~ location:trt)\n\n location    trt         emmean    SE   df lower.CL upper.CL\n deep        alt           19.5 0.868 24.1     17.7     21.3\n superficial alt           16.6 0.868 24.1     14.8     18.4\n deep        traditional   18.3 0.868 24.1     16.5     20.1\n superficial traditional   17.2 0.868 24.1     15.4     19.0\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\n\n\nemmeans(model2_lme, specs = ~ location)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n location    emmean    SE df lower.CL upper.CL\n deep          18.9 0.744 10     17.2     20.5\n superficial   16.9 0.744 10     15.3     18.6\n\nResults are averaged over the levels of: trt \nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\nemmeans(model2_lme, specs = ~ trt)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n trt         emmean    SE df lower.CL upper.CL\n alt           18.0 0.744 10     16.4     19.7\n traditional   17.8 0.744 10     16.1     19.4\n\nResults are averaged over the levels of: location \nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\nemmeans(model2_lme, specs = ~ location:trt)\n\n location    trt         emmean    SE df lower.CL upper.CL\n deep        alt           19.5 0.868 10     17.5     21.4\n superficial alt           16.6 0.868 10     14.7     18.5\n deep        traditional   18.3 0.868 10     16.4     20.2\n superficial traditional   17.2 0.868 10     15.3     19.2\n\nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\n\n\n\nThe code above is calculating the estimated marginal means for main effects of ‚Äúlocation‚Äù and ‚Äútrt‚Äù and their interaction (location:trt) effect on yellowness in meat sample. Make sure to pay attention to the warning message that means are averaged over the levels. It‚Äôs an important detail to take into account when conducting inference and making conclusions. When working with factorial designs, make sure to interpret ANOVA and estimated marginal means for main and interaction effects with care.",
    "crumbs": [
      "Experiment designs",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Factorial RCBD Design</span>"
    ]
  },
  {
    "objectID": "chapters/split-plot-design.html",
    "href": "chapters/split-plot-design.html",
    "title": "7¬† Split Plot Design",
    "section": "",
    "text": "7.1 Background\nSplit plot designs are needed when we cannot randomly assign multiple levels of treatments in a completely randomized experiment. This often results in a generalization of the factorial design called a split-plot design. In split-plot design, the experimental units are called split-plots , and are nested within whole plots. The main principle is that there are whole plots, to which the levels of one or more factors of split-plots are assigned randomly.",
    "crumbs": [
      "Experiment designs",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Split Plot Design</span>"
    ]
  },
  {
    "objectID": "chapters/split-plot-design.html#statistical-details",
    "href": "chapters/split-plot-design.html#statistical-details",
    "title": "7¬† Split Plot Design",
    "section": "7.2 Statistical Details",
    "text": "7.2 Statistical Details\n\\[y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + r_k + \\epsilon_{ik} + \\delta_{ijk}\\]\nWhere:\n\\(\\mu\\) = overall experimental mean\n\\(\\alpha\\) = main effect of whole plot (fixed)\n\\(\\beta\\) = main effect of split plot (fixed)\n\\(\\alpha\\beta\\) = interaction between factors A and B\n\\(r\\) = block effect (random)\n\\(\\epsilon_{ij}\\) = whole plot error\n\\(\\delta_{ijk}\\) = split plot error\nBoth the overall error and the rep effects are assumed to be normally distributed with a mean of zero and standard deviations of \\(\\sigma\\) (whole-plot) and \\(\\sigma_{sp}\\) (split-plot), respectively.\n\\[ \\epsilon \\sim N(0, \\sigma)\\]\n\\[\\ \\delta  \\sim N(0, \\sigma_{sp})\\]",
    "crumbs": [
      "Experiment designs",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Split Plot Design</span>"
    ]
  },
  {
    "objectID": "chapters/split-plot-design.html#analysis-examples",
    "href": "chapters/split-plot-design.html#analysis-examples",
    "title": "7¬† Split Plot Design",
    "section": "7.3 Analysis Examples",
    "text": "7.3 Analysis Examples\nLet‚Äôs start analyzing by loading required libraries.\n\nlme4nlme\n\n\n\nlibrary(lme4); library(lmerTest); library(emmeans)\nlibrary(dplyr); library(performance); library(ggplot2)\nlibrary(broom.mixed)\n\n\n\n\nlibrary(nlme); library(performance); library(emmeans)\nlibrary(dplyr); library(ggplot2); library(broom.mixed)\n\n\n\n\n\n7.3.1 Example Model for RCBD Split Plot Designs\nThe oats data used in this example is from the MASS package. The design is RCBD split plot with 6 blocks, 3 levels for the main plots and 4 levels for the split plot. The primary outcome variable is yield.\n\nTable of variables in the oat data set\n\n\nblock\nblocking unit\n\n\nVariety (V)\nMain plot with 3 levels\n\n\nNitrogen (N)\nSplit-plot with 4 levels\n\n\nyield (Y)\nyield (lbs per acre)\n\n\n\nThe objective of this analysis is to study the impact of different varieties and nitrogen application rates on oat yields.\nTo fully examine the response of oat yield with different varieties and nutrient levels in a split plots, we will need to statistically analyse and compare the effects of varieties (main plot), nutrient levels (split plot), their interaction.\nLet‚Äôs start this example analysis by first loading the ‚Äòoat‚Äô data from the MASS package.\n\ndata(\"oats\", package = \"MASS\")\nhead(oats,5)\n\n  B           V      N   Y\n1 I     Victory 0.0cwt 111\n2 I     Victory 0.2cwt 130\n3 I     Victory 0.4cwt 157\n4 I     Victory 0.6cwt 174\n5 I Golden.rain 0.0cwt 117\n\n\n\n7.3.1.1 Data integrity checks\n\nCheck structure of the data\n\nWe will first examine the structure of the data. The ‚ÄúB‚Äù, ‚ÄúV‚Äù, and ‚ÄúN‚Äù needs to be a factor and ‚ÄúY‚Äù should be numeric.\n\nstr(oats)\n\n'data.frame':   72 obs. of  4 variables:\n $ B: Factor w/ 6 levels \"I\",\"II\",\"III\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ V: Factor w/ 3 levels \"Golden.rain\",..: 3 3 3 3 1 1 1 1 2 2 ...\n $ N: Factor w/ 4 levels \"0.0cwt\",\"0.2cwt\",..: 1 2 3 4 1 2 3 4 1 2 ...\n $ Y: int  111 130 157 174 117 114 161 141 105 140 ...\n\n\n\nInspect the independent variables\n\nNext, run the table() command to verify the levels of the main plot and the split plot.\n\ntable(oats$V, oats$N)\n\n             \n              0.0cwt 0.2cwt 0.4cwt 0.6cwt\n  Golden.rain      6      6      6      6\n  Marvellous       6      6      6      6\n  Victory          6      6      6      6\n\n\n\nCheck the extent of missing data\n\n\ncolSums(is.na(oats))\n\nB V N Y \n0 0 0 0 \n\n\n\nInspect the dependent variable\n\nLast, check the distribution of the dependent variable by plotting a histogram.\n\n\n\n\n\n\n\n\n\nFigure¬†7.1: Histogram of the dependent variable.\n\n\n\n\n\nhist(oats$Y, main = NA, xlab = \"yield\")\n\n\n\n7.3.1.2 Model Building\nWe are evaluating the effect of V, N and their interaction on yield. The 1|B/V means that random intercepts vary with block and V nested within each block.\n\n\nRecall the model:\n\\[y_{ijk} = \\mu + \\rho_j +  \\alpha_i + \\beta_k + (\\alpha_i\\beta_k) + \\epsilon_{ij} + \\delta_{ijk}\\] Where:\n\\(\\mu\\) = overall experimental mean, \\(\\rho\\) = block effect (random), \\(\\alpha\\) = main effect of whole plot (fixed), \\(\\beta\\) = main effect of split plot (fixed), \\(\\alpha\\)\\(\\beta\\) = interaction between factors A and B, \\(\\epsilon_{ij}\\) = whole plot error, \\(\\delta_{ijk}\\) = split plot error.\n\nlme4nlme\n\n\n\noats_lmer &lt;- lmer(Y ~  V + N + V:N + (1|B/V),\n                  data = oats, \n                  na.action = na.exclude)\ntidy(oats_lmer)\n\n# A tibble: 15 √ó 8\n   effect   group    term            estimate std.error statistic    df  p.value\n   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 fixed    &lt;NA&gt;     (Intercept)       80.0        9.11    8.78    16.1  1.55e-7\n 2 fixed    &lt;NA&gt;     VMarvellous        6.67       9.72    0.686   30.2  4.98e-1\n 3 fixed    &lt;NA&gt;     VVictory          -8.50       9.72   -0.875   30.2  3.89e-1\n 4 fixed    &lt;NA&gt;     N0.2cwt           18.5        7.68    2.41    45.0  2.02e-2\n 5 fixed    &lt;NA&gt;     N0.4cwt           34.7        7.68    4.51    45.0  4.58e-5\n 6 fixed    &lt;NA&gt;     N0.6cwt           44.8        7.68    5.84    45.0  5.48e-7\n 7 fixed    &lt;NA&gt;     VMarvellous:N0‚Ä¶    3.33      10.9     0.307   45.0  7.60e-1\n 8 fixed    &lt;NA&gt;     VVictory:N0.2c‚Ä¶   -0.333     10.9    -0.0307  45.0  9.76e-1\n 9 fixed    &lt;NA&gt;     VMarvellous:N0‚Ä¶   -4.17      10.9    -0.383   45.0  7.03e-1\n10 fixed    &lt;NA&gt;     VVictory:N0.4c‚Ä¶    4.67      10.9     0.430   45.0  6.70e-1\n11 fixed    &lt;NA&gt;     VMarvellous:N0‚Ä¶   -4.67      10.9    -0.430   45.0  6.70e-1\n12 fixed    &lt;NA&gt;     VVictory:N0.6c‚Ä¶    2.17      10.9     0.199   45.0  8.43e-1\n13 ran_pars V:B      sd__(Intercept)   10.3       NA      NA       NA   NA      \n14 ran_pars B        sd__(Intercept)   14.6       NA      NA       NA   NA      \n15 ran_pars Residual sd__Observation   13.3       NA      NA       NA   NA      \n\n\n\n\n\noats_lme &lt;- lme(Y ~  V + N + V:N,\n                random = ~1|B/V,\n                data = oats,\n                na.action = na.exclude)\ntidy(oats_lme)\n\nWarning in tidy.lme(oats_lme): ran_pars not yet implemented for multiple levels\nof nesting\n\n\n# A tibble: 12 √ó 7\n   effect term                estimate std.error    df statistic  p.value\n   &lt;chr&gt;  &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 fixed  (Intercept)           80          9.11    45    8.78   2.56e-11\n 2 fixed  VMarvellous            6.67       9.72    10    0.686  5.08e- 1\n 3 fixed  VVictory              -8.5        9.72    10   -0.875  4.02e- 1\n 4 fixed  N0.2cwt               18.5        7.68    45    2.41   2.02e- 2\n 5 fixed  N0.4cwt               34.7        7.68    45    4.51   4.58e- 5\n 6 fixed  N0.6cwt               44.8        7.68    45    5.84   5.48e- 7\n 7 fixed  VMarvellous:N0.2cwt    3.33      10.9     45    0.307  7.60e- 1\n 8 fixed  VVictory:N0.2cwt      -0.333     10.9     45   -0.0307 9.76e- 1\n 9 fixed  VMarvellous:N0.4cwt   -4.17      10.9     45   -0.383  7.03e- 1\n10 fixed  VVictory:N0.4cwt       4.67      10.9     45    0.430  6.70e- 1\n11 fixed  VMarvellous:N0.6cwt   -4.67      10.9     45   -0.430  6.70e- 1\n12 fixed  VVictory:N0.6cwt       2.17      10.9     45    0.199  8.43e- 1\n\n\n\n\n\n\n\n7.3.1.3 Check Model Assumptions\nAs shown in example 1, We need to verify the normality of residuals and homogeneous variance. Here we are using the check_model() function from the performance package.\n\nlme4nlme\n\n\n\ncheck_model(oats_lmer, check = c('qq', 'linearity', 'reqq'), detrend=FALSE, alpha =0)\n\n\n\n\n\n\n\n\n\n\n\ncheck_model(oats_lme,  check = c('qq', 'linearity'), detrend=FALSE, alpha=0)\n\n\n\n\n\n\n\n\n\n\n\nResiduals from the model follows normal distribution and no evidence of Homoscedasticity.\n\n\n7.3.1.4 Inference\nLet‚Äôs have a look at the analysis of variance, for V, N and their interaction effect.\n\nlme4nlme\n\n\n\nanova(oats_lmer, type = \"III\") \n\nType III Analysis of Variance Table with Satterthwaite's method\n     Sum Sq Mean Sq NumDF DenDF F value    Pr(&gt;F)    \nV     526.1   263.0     2    10  1.4853    0.2724    \nN   20020.5  6673.5     3    45 37.6857 2.458e-12 ***\nV:N   321.7    53.6     6    45  0.3028    0.9322    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncar::Anova(oats_lmer, type = \"3\", test.statistics = \"F\")\n\nAnalysis of Deviance Table (Type III Wald chisquare tests)\n\nResponse: Y\n              Chisq Df Pr(&gt;Chisq)    \n(Intercept) 77.1664  1  &lt; 2.2e-16 ***\nV            2.4491  2     0.2939    \nN           39.0683  3  1.679e-08 ***\nV:N          1.8169  6     0.9357    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nanova(oats_lme, type = \"marginal\")\n\n            numDF denDF  F-value p-value\n(Intercept)     1    45 77.16732  &lt;.0001\nV               2    10  1.22454  0.3344\nN               3    45 13.02273  &lt;.0001\nV:N             6    45  0.30282  0.9322\n\n\n\n\n\nNext, we can estimate marginal means for V, N, or their interaction (V*N) effect.\n\nlme4nlme\n\n\n\nemm1 &lt;- emmeans(oats_lmer, ~ V|N) \nemm1\n\nN = 0.0cwt:\n V           emmean   SE   df lower.CL upper.CL\n Golden.rain   80.0 9.11 16.1     60.7     99.3\n Marvellous    86.7 9.11 16.1     67.4    106.0\n Victory       71.5 9.11 16.1     52.2     90.8\n\nN = 0.2cwt:\n V           emmean   SE   df lower.CL upper.CL\n Golden.rain   98.5 9.11 16.1     79.2    117.8\n Marvellous   108.5 9.11 16.1     89.2    127.8\n Victory       89.7 9.11 16.1     70.4    109.0\n\nN = 0.4cwt:\n V           emmean   SE   df lower.CL upper.CL\n Golden.rain  114.7 9.11 16.1     95.4    134.0\n Marvellous   117.2 9.11 16.1     97.9    136.5\n Victory      110.8 9.11 16.1     91.5    130.1\n\nN = 0.6cwt:\n V           emmean   SE   df lower.CL upper.CL\n Golden.rain  124.8 9.11 16.1    105.5    144.1\n Marvellous   126.8 9.11 16.1    107.5    146.1\n Victory      118.5 9.11 16.1     99.2    137.8\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\n\n\nemm1 &lt;- emmeans(oats_lme, ~ V|N) \nemm1\n\nN = 0.0cwt:\n V           emmean   SE df lower.CL upper.CL\n Golden.rain   80.0 9.11  5     56.6    103.4\n Marvellous    86.7 9.11  5     63.3    110.1\n Victory       71.5 9.11  5     48.1     94.9\n\nN = 0.2cwt:\n V           emmean   SE df lower.CL upper.CL\n Golden.rain   98.5 9.11  5     75.1    121.9\n Marvellous   108.5 9.11  5     85.1    131.9\n Victory       89.7 9.11  5     66.3    113.1\n\nN = 0.4cwt:\n V           emmean   SE df lower.CL upper.CL\n Golden.rain  114.7 9.11  5     91.3    138.1\n Marvellous   117.2 9.11  5     93.8    140.6\n Victory      110.8 9.11  5     87.4    134.2\n\nN = 0.6cwt:\n V           emmean   SE df lower.CL upper.CL\n Golden.rain  124.8 9.11  5    101.4    148.2\n Marvellous   126.8 9.11  5    103.4    150.2\n Victory      118.5 9.11  5     95.1    141.9\n\nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\n\n\n\nThe estimated means for the variety ‚ÄòMarvellous‚Äô were higher compared to other varieties across all N treatments.",
    "crumbs": [
      "Experiment designs",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Split Plot Design</span>"
    ]
  },
  {
    "objectID": "chapters/split-split-plot.html",
    "href": "chapters/split-split-plot.html",
    "title": "8¬† Split-Split Plot Design",
    "section": "",
    "text": "8.1 Details for split-split plot designs\nThe concept of split-plot design can be extended to situations in which randomization restrictions may occur at any number of levels within the experiment. If there are two levels of randomization restriction, the layout is called a split-split plot design. The split-split plot design is an extension of the split-plot design to accommodate a third factor: one factor in the main plot, another in the split plot and a third factor in the split-split plot. The example shown below illustrates this experiment layout.\nThe statistical model structure this design:\n\\[y_{ijk} = \\mu + r_j +  \\alpha_i + \\beta_k + (\\alpha\\beta)_{ik} + \\gamma_n + (\\alpha\\gamma)_{in} + (\\gamma\\beta)_{nk} + (\\alpha\\beta\\gamma)_{ikn} + \\epsilon_{ijk} + \\delta_{ijkn}\\]\nWhere:\n\\(\\mu\\) = overall experimental mean\n\\(r\\) = replication effect (random)\n\\(\\alpha\\) = main effect of whole plot (fixed)\n\\(\\beta\\) = main effect of split plot (fixed)\n\\(\\gamma\\) = main effect of split-split plot (fixed)\n\\(\\epsilon_{ij}\\) = whole plot error\n\\(\\delta_{ijk}\\) = split plot error\n\\(\\tau_{xxx}\\) = split-split plot error\nThe assumptions of the model includes normal distribution of both the error and the rep effects with a mean of zero and standard deviations of \\(\\sigma\\) and \\(\\sigma_{sp}\\), respectively.\n\\[\\epsilon \\sim N(0, \\sigma_\\epsilon)\\]\n\\[\\delta  \\sim N(0, \\sigma_{sp})\\]\n\\[\\tau  \\sim N(0, \\sigma_{ssp})\\]",
    "crumbs": [
      "Experiment designs",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Split-Split Plot Design</span>"
    ]
  },
  {
    "objectID": "chapters/split-split-plot.html#example-analysis",
    "href": "chapters/split-split-plot.html#example-analysis",
    "title": "8¬† Split-Split Plot Design",
    "section": "8.2 Example Analysis",
    "text": "8.2 Example Analysis\n\nlme4nlme\n\n\n\nlibrary(dplyr)\nlibrary(lme4); library(lmerTest); library(broom.mixed)\nlibrary(emmeans); library(performance)\n\n\n\n\nlibrary(dplyr)\nlibrary(nlme); library(emmeans)\nlibrary(broom.mixed); library(performance)\n\n\n\n\nIn this example, we have a rice yield data from the agricolae package (data set ‚Äússp‚Äù). The experiment consists of 3 different rice varieties grown under 3 management practices and 5 nitrogen levels in the split-split plot design.\n\nrice &lt;- read.csv(here::here(\"data\", \"rice_ssp.csv\"))\n\n\nTable of variables in the rice data set\n\n\n\n\n\n\nblock\nblocking unit\n\n\nnitrogen\ndifferent nitrogen fertilizer rates as main plot with 5 levels\n\n\nmanagement\nmanagement practices as split plot with 3 levels\n\n\nvariety\ncrop variety being a split-split plot with 3 levels\n\n\nyield\nyield (bushels per acre)\n\n\n\nIn this example, note that there are two randomization restrictions within each block: nitrogen & management. The order is which nitrogen treatments are applied is randomly selected. The 3 management practices are randomly assigned to the split plots. Finally, within a particular management, the 3 varieties are tested in a random order, forming 3 split-split plots.\n\n8.2.1 Data integrity checks\nBefore analyzing the data let‚Äôs do some preliminary data quality checks.\n\nCheck structure of the data\n\nWe will start with evaluation of the structure of the data where class of block, nitrogen, management and variety should be a character/factor and yield should be numeric.\n\nstr(rice)\n\n'data.frame':   135 obs. of  5 variables:\n $ block     : int  1 1 1 1 1 1 1 1 1 1 ...\n $ nitrogen  : int  0 0 0 50 50 50 80 80 80 110 ...\n $ management: chr  \"m1\" \"m2\" \"m3\" \"m1\" ...\n $ variety   : int  1 1 1 1 1 1 1 1 1 1 ...\n $ yield     : num  3.32 3.77 4.66 3.19 3.62 ...\n\n\nHere we need to convert block, nitrogen, variety, and management to data type character.\n\nrice$block &lt;- as.character(rice$block)\nrice$nitrogen &lt;- as.character(rice$nitrogen)\nrice$management &lt;- as.character(rice$management)\nrice$variety &lt;- as.character(rice$variety)\n\n\nInspect the independent variables\n\nNext, run a cross tabulations to check balance of observations across independent variables:\n\ntable(rice$variety, rice$nitrogen, rice$management)\n\n, ,  = m1\n\n   \n    0 110 140 50 80\n  1 3   3   3  3  3\n  2 3   3   3  3  3\n  3 3   3   3  3  3\n\n, ,  = m2\n\n   \n    0 110 140 50 80\n  1 3   3   3  3  3\n  2 3   3   3  3  3\n  3 3   3   3  3  3\n\n, ,  = m3\n\n   \n    0 110 140 50 80\n  1 3   3   3  3  3\n  2 3   3   3  3  3\n  3 3   3   3  3  3\n\n\nIt looks perfectly balanced, with exactly 3 observation per treatment group.\n\nCheck the extent of missing data\n\n\ncolSums(is.na(rice))\n\n     block   nitrogen management    variety      yield \n         0          0          0          0          0 \n\n\nGreat! no missing values here.\n\nInspect the dependent variable\n\nLast, check the distribution of the dependent variable by plotting a histogram of yield values using hist() in R.\n\nhist(rice$yield, main = NA, xlab = \"yield\")\n\n\n\n\n\n\n\n\n\n\nFigure¬†8.1: Histogram of the dependent variable.\n\n\n\n\n\n\n8.2.2 Model Building\nThe model and analysis of variance of a split-split plot design is divided into three parts: the main plot, split plot and split-split plot analysis. We can use the nesting notation in the random part because nitrogen and management are nested in blocks. We can do blocks as fixed or random.\n\nlme4nlme\n\n\n\nmodel_lmer &lt;- lmer(yield ~ nitrogen * management * variety + (1|block/nitrogen/management),\n                   data = rice,\n                   na.action = na.exclude)\n\nboundary (singular) fit: see help('isSingular')\n\ntidy(model_lmer)\n\n# A tibble: 49 √ó 8\n   effect group term                 estimate std.error statistic    df  p.value\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 fixed  &lt;NA&gt;  (Intercept)             3.90      0.386    10.1    89.7 1.79e-16\n 2 fixed  &lt;NA&gt;  nitrogen110             0.753     0.545     1.38   89.7 1.71e- 1\n 3 fixed  &lt;NA&gt;  nitrogen140             0.165     0.545     0.302  89.7 7.63e- 1\n 4 fixed  &lt;NA&gt;  nitrogen50              0.335     0.545     0.614  89.7 5.41e- 1\n 5 fixed  &lt;NA&gt;  nitrogen80              1.33      0.545     2.44   89.7 1.68e- 2\n 6 fixed  &lt;NA&gt;  managementm2            0.420     0.540     0.779  80.0 4.38e- 1\n 7 fixed  &lt;NA&gt;  managementm3            1.43      0.540     2.65   80.0 9.82e- 3\n 8 fixed  &lt;NA&gt;  variety2                1.45      0.540     2.68   80.0 8.83e- 3\n 9 fixed  &lt;NA&gt;  variety3                1.48      0.540     2.74   80.0 7.49e- 3\n10 fixed  &lt;NA&gt;  nitrogen110:managem‚Ä¶    0.377     0.763     0.493  80.0 6.23e- 1\n# ‚Ñπ 39 more rows\n\n\n\n\n\nmodel_lme &lt;- lme(yield ~ nitrogen*management*variety, \n                 random = ~ 1|block/nitrogen/management,\n                 data = rice,\n                 na.action = na.exclude)\ntidy(model_lme)\n\nWarning in tidy.lme(model_lme): ran_pars not yet implemented for multiple\nlevels of nesting\n\n\n# A tibble: 45 √ó 7\n   effect term                     estimate std.error    df statistic  p.value\n   &lt;chr&gt;  &lt;chr&gt;                       &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 fixed  (Intercept)                 3.90      0.386    60    10.1   1.43e-14\n 2 fixed  nitrogen110                 0.753     0.545     8     1.38  2.05e- 1\n 3 fixed  nitrogen140                 0.165     0.545     8     0.302 7.70e- 1\n 4 fixed  nitrogen50                  0.335     0.545     8     0.614 5.56e- 1\n 5 fixed  nitrogen80                  1.33      0.545     8     2.44  4.08e- 2\n 6 fixed  managementm2                0.420     0.540    20     0.779 4.45e- 1\n 7 fixed  managementm3                1.43      0.540    20     2.65  1.55e- 2\n 8 fixed  variety2                    1.45      0.540    60     2.68  9.38e- 3\n 9 fixed  variety3                    1.48      0.540    60     2.74  7.99e- 3\n10 fixed  nitrogen110:managementm2    0.377     0.763    20     0.493 6.27e- 1\n# ‚Ñπ 35 more rows\n\n\n\n\n\n\n\nFor information on what boundary (singular) fit means, please refer to Chapter 14. For now, this warning message will be ignored.\n\n\n8.2.3 Check Model Assumptions\n\nlme4nlme\n\n\n\ncheck_model(model_lmer,  check = c('qq', 'linearity', 'reqq'), detrend=FALSE, alpha = 0)\n\n\n\n\n\n\n\n\n\n\n\ncheck_model(model_lme,  check = c('qq', 'linearity'), detrend=FALSE)\n\n\n\n\n\n\n\n\n\n\n\nThe assumptions of homescedascity and normality of residuals appear met.\n\n\n8.2.4 Inference\nLet‚Äôs look at the analysis of variance for fixed effects and their interaction effect on rice yield.\n\nlme4nlme\n\n\n\ncar::Anova(model_lmer, type = \"III\")\n\nAnalysis of Deviance Table (Type III Wald chisquare tests)\n\nResponse: yield\n                               Chisq Df Pr(&gt;Chisq)    \n(Intercept)                 102.1211  1  &lt; 2.2e-16 ***\nnitrogen                      7.6641  4   0.104685    \nmanagement                    7.3923  2   0.024819 *  \nvariety                       9.8259  2   0.007351 ** \nnitrogen:management           1.6941  8   0.988997    \nnitrogen:variety             21.3448  8   0.006286 ** \nmanagement:variety            8.8771  4   0.064245 .  \nnitrogen:management:variety   8.4629 16   0.933876    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nanova(model_lme, type = \"marginal\")\n\n                            numDF denDF   F-value p-value\n(Intercept)                     1    60 102.12108  &lt;.0001\nnitrogen                        4     8   1.91603  0.2012\nmanagement                      2    20   3.69617  0.0431\nvariety                         2    60   4.91295  0.0106\nnitrogen:management             8    20   0.21177  0.9850\nnitrogen:variety                8    60   2.66810  0.0141\nmanagement:variety              4    60   2.21929  0.0775\nnitrogen:management:variety    16    60   0.52893  0.9210\n\n\n\n\n\nwe observed a significant impact of management, variety, and nitrogen x variety interaction effect on rice yield.\nNext, we can estimate the marginal means for each treatment factor (variety, nitrogen, management) which will be averaged across other factors and their interaction.\n\nlme4nlme\n\n\n\nemmeans(model_lmer, ~ management)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n management emmean    SE   df lower.CL upper.CL\n m1           5.90 0.102 11.2     5.68     6.12\n m2           6.49 0.102 11.2     6.26     6.71\n m3           7.28 0.102 11.2     7.05     7.50\n\nResults are averaged over the levels of: nitrogen, variety \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nemmeans(model_lmer, ~ nitrogen*variety)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n nitrogen variety emmean    SE df lower.CL upper.CL\n 0        1         4.51 0.227 49     4.06     4.97\n 110      1         5.44 0.227 49     4.99     5.90\n 140      1         5.08 0.227 49     4.62     5.53\n 50       1         4.76 0.227 49     4.31     5.22\n 80       1         5.83 0.227 49     5.38     6.29\n 0        2         5.16 0.227 49     4.71     5.62\n 110      2         6.92 0.227 49     6.47     7.38\n 140      2         7.29 0.227 49     6.83     7.74\n 50       2         6.02 0.227 49     5.56     6.47\n 80       2         6.59 0.227 49     6.13     7.04\n 0        3         6.48 0.227 49     6.02     6.93\n 110      3         8.44 0.227 49     7.99     8.90\n 140      3         9.34 0.227 49     8.88     9.79\n 50       3         7.88 0.227 49     7.42     8.34\n 80       3         8.56 0.227 49     8.11     9.02\n\nResults are averaged over the levels of: management \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\n\n\nemmeans(model_lme, ~ management)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n management emmean    SE df lower.CL upper.CL\n m1           5.90 0.102  2     5.46     6.34\n m2           6.49 0.102  2     6.05     6.92\n m3           7.28 0.102  2     6.84     7.71\n\nResults are averaged over the levels of: nitrogen, variety \nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\nemmeans(model_lme, ~ nitrogen*variety)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n nitrogen variety emmean    SE df lower.CL upper.CL\n 0        1         4.51 0.227  2     3.54     5.49\n 110      1         5.44 0.227  2     4.47     6.42\n 140      1         5.08 0.227  2     4.10     6.05\n 50       1         4.76 0.227  2     3.79     5.74\n 80       1         5.83 0.227  2     4.86     6.81\n 0        2         5.16 0.227  2     4.19     6.14\n 110      2         6.92 0.227  2     5.95     7.90\n 140      2         7.29 0.227  2     6.31     8.27\n 50       2         6.02 0.227  2     5.04     6.99\n 80       2         6.59 0.227  2     5.61     7.57\n 0        3         6.48 0.227  2     5.50     7.46\n 110      3         8.44 0.227  2     7.47     9.42\n 140      3         9.34 0.227  2     8.36    10.31\n 50       3         7.88 0.227  2     6.90     8.86\n 80       3         8.56 0.227  2     7.59     9.54\n\nResults are averaged over the levels of: management \nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\n\n\n\nNotice we get a message that the estimated means for ‚Äònitrogen x variety‚Äô are averaged over the levels of ‚Äòmanagement‚Äô. So we need to be careful about this while making conslusions/recommendations.\n\n\n\n\n\n\nNested random effects\n\n\n\nYou may have noticed the order of random effects in model statement:\nmodel_lme &lt;- lme(yield ~ nitrogen*management*variety,\n                  random = ~ 1|block/nitrogen/management,\n                  data = rice, \n                  na.action = na.exclude)\nThe random effects follow the order of ~1|block/main plot/split-plot. While fitting the model for split-split plot design please make sure to have a clear understanding of the main plot, split-plot and split-split plot factors to avoid having an erroneous model.",
    "crumbs": [
      "Experiment designs",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Split-Split Plot Design</span>"
    ]
  },
  {
    "objectID": "chapters/strip-plot.html",
    "href": "chapters/strip-plot.html",
    "title": "9¬† Strip Plot Design",
    "section": "",
    "text": "9.1 Background\nAlso referred by ‚Äòstrip-plot design‚Äô or ‚Äòstrip-split-plot design‚Äô. This design had an extensive application in the agricultural sciences. In the simplest case, we have two factors A and B. Factor A is applied to whole plots same as in the standard split-plot design. Then factor B is applied to strips (which are just another set of whole plots) that are orthogonal to the original whole plots used for factor A.\nThe statistical model for this design is:\n\\[y_{ijk} = \\mu + \\tau_i + \\alpha_j + (\\tau\\alpha)_{ij} + \\beta_k + (\\tau\\beta)_{ik} + (\\alpha\\beta)_{jk} +  \\epsilon_{ijk}\\] Where:\n\\(\\mu\\)= overall experimental mean, \\((\\tau\\alpha)_{ij}\\), \\((\\tau\\beta)_{ik}\\), and \\(\\epsilon_{ijk}\\) are the errors used to test factor A, B, and their interaction AB, respectively. \\(\\alpha\\) and \\(\\beta\\) are the main effects applied A and B, and \\(\\alpha_j\\beta_k\\) represents the interaction between main factors.\n\\[ \\epsilon_{ijk} \\sim N(0, \\sigma^2)\\]",
    "crumbs": [
      "Experiment designs",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Strip Plot Design</span>"
    ]
  },
  {
    "objectID": "chapters/strip-plot.html#example-analysis",
    "href": "chapters/strip-plot.html#example-analysis",
    "title": "9¬† Strip Plot Design",
    "section": "9.2 Example Analysis",
    "text": "9.2 Example Analysis\nWe will start the analysis first by loading the required libraries for this analysis for lmer() and lme() models, respectively.\n\nlme4nlme\n\n\n\nlibrary(lme4); library(lmerTest); library(emmeans)\nlibrary(dplyr); library(performance); library(desplot)\nlibrary(broom.mixed)\n\n\n\n\nlibrary(nlme); library(performance); library(emmeans)\nlibrary(dplyr); library(desplot); library(broom.mixed)\n\n\n\n\nFor this example, we will use Rice strip-plot experiment data from the agridat package. This data contains a strip-plot experiment with three reps, variety as the whole plot and nitrogen fertilizer as the whole plot applied in the strips.\n\nrice &lt;- read.csv(here::here(\"data\", \"rice_stripplot.csv\"))\n\n\nTable of variables in the data set\n\n\nrep\nreplication unit\n\n\nnitro\nnitrogen fertilizer in kg/ha\n\n\ngen\nrice variety\n\n\nrow\nrow (represents gen)\n\n\ncol\ncolumn (represents nitro)\n\n\nyield\ngrain yield in kg/ha\n\n\n\nFor the sake of analysis, ‚Äòrow‚Äô and ‚Äòcol‚Äô variables are used to represent ‚Äònitrogen‚Äô and ‚ÄòGen‚Äô factors. The plot below shows the application of treatments in horizontal and vertical direction in a strip plot design.\n\n\n\n\n\n\n\n\n\n\n9.2.1 Data integrity checks\n\nCheck structure of the data\n\nThe ‚Äòrep‚Äô, ‚Äònitro‚Äô, and ‚Äògen‚Äô variables in the data needs to be a factor/character variables and ‚Äòyield‚Äô should be numeric.\n\nstr(rice)\n\n'data.frame':   54 obs. of  6 variables:\n $ yield: int  2373 4076 7254 4007 5630 7053 2620 4676 7666 2726 ...\n $ rep  : chr  \"R1\" \"R1\" \"R1\" \"R1\" ...\n $ nitro: int  0 60 120 0 60 120 0 60 120 0 ...\n $ gen  : chr  \"G1\" \"G1\" \"G1\" \"G2\" ...\n $ col  : int  1 3 2 1 3 2 1 3 2 1 ...\n $ row  : int  1 1 1 3 3 3 4 4 4 2 ...\n\n\nLet‚Äôs convert ‚Äònitro‚Äô from numeric to factor.\n\nrice$nitro &lt;- as.factor(rice$nitro)\n\n\nInspect the independent variables\n\nLet‚Äôs running a a cross tabulation of independent variables to look at the balance of treatment factors.\n\ntable(rice$gen, rice$nitro)\n\n    \n     0 60 120\n  G1 3  3   3\n  G2 3  3   3\n  G3 3  3   3\n  G4 3  3   3\n  G5 3  3   3\n  G6 3  3   3\n\n\nIt looks balanced with 3 number of observations for each variety and nitrogen level.\n\nCheck the extent of missing data\n\nNext step is to identify if there are any missing observations in the data set.\n\ncolSums(is.na(rice))\n\nyield   rep nitro   gen   col   row \n    0     0     0     0     0     0 \n\n\nWe don‚Äôt have any missing values in this data set.\n\nInspect the dependent variable\n\nLet‚Äôs check the distribution of dependent variable by plotting a histogram.\n\nhist(rice$yield, main = NA, xlab = \"yield\")\n\n\n\n\n\n\n\n\n\n\nFigure¬†9.1: Histogram of the dependent variable.\n\n\n\n\nNo extreme values or skewness is present in the yield values.\n\n\n9.2.2 Model Building\nThe goal of this analysis is to evaluate the impact of nitrogen, genotype, and their interaction on rice yield. The variables ‚Äúrep‚Äù, ‚Äúgen‚Äù (nested in rep), and ‚Äúnitro‚Äù (nested in rep) were random effects in the model.\n\nlme4nlme\n\n\n\nmodel_lmer &lt;- lmer(yield ~  nitro*gen +  (1|rep) + \n                   (1|rep:gen) + (1|rep:nitro), \n                   data = rice)\ntidy(model_lmer)\n\n# A tibble: 22 √ó 8\n   effect group term           estimate std.error statistic    df     p.value\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n 1 fixed  &lt;NA&gt;  (Intercept)       3572.      572.     6.24   17.8 0.00000732 \n 2 fixed  &lt;NA&gt;  nitro60           1560.      558.     2.80   22.4 0.0104     \n 3 fixed  &lt;NA&gt;  nitro120          3976.      558.     7.13   22.4 0.000000341\n 4 fixed  &lt;NA&gt;  genG2             1363.      717.     1.90   20.9 0.0714     \n 5 fixed  &lt;NA&gt;  genG3              678.      717.     0.945  20.9 0.355      \n 6 fixed  &lt;NA&gt;  genG4              487.      717.     0.679  20.9 0.504      \n 7 fixed  &lt;NA&gt;  genG5              530.      717.     0.739  20.9 0.468      \n 8 fixed  &lt;NA&gt;  genG6             -364.      717.    -0.508  20.9 0.617      \n 9 fixed  &lt;NA&gt;  nitro60:genG2      219.      741.     0.296  20.0 0.771      \n10 fixed  &lt;NA&gt;  nitro120:genG2   -1699.      741.    -2.29   20.0 0.0328     \n# ‚Ñπ 12 more rows\n\n\n\n\n\nmodel_lme &lt;-lme(yield ~  nitro*gen,\n                random = list(one = pdBlocked(list(\n        pdIdent(~ 0 + rep), \n         pdIdent(~ 0 + rep:gen), \n        pdIdent(~ 0 + rep:nitro)))),\n        data = rice %&gt;% mutate(one = factor(1)))\n\nsummary(model_lme)\n\nLinear mixed-effects model fit by REML\n  Data: rice %&gt;% mutate(one = factor(1)) \n       AIC      BIC    logLik\n  651.4204 686.2578 -303.7102\n\nRandom effects:\n Composite Structure: Blocked\n\n Block 1: repR1, repR2, repR3\n Formula: ~0 + rep | one\n Structure: Multiple of an Identity\n           repR1    repR2    repR3\nStdDev: 393.4278 393.4278 393.4278\n\n Block 2: repR1:genG1, repR2:genG1, repR3:genG1, repR1:genG2, repR2:genG2, repR3:genG2, repR1:genG3, repR2:genG3, repR3:genG3, repR1:genG4, repR2:genG4, repR3:genG4, repR1:genG5, repR2:genG5, repR3:genG5, repR1:genG6, repR2:genG6, repR3:genG6\n Formula: ~0 + rep:gen | one\n Structure: Multiple of an Identity\n        repR1:genG1 repR2:genG1 repR3:genG1 repR1:genG2 repR2:genG2 repR3:genG2\nStdDev:    600.1711    600.1711    600.1711    600.1711    600.1711    600.1711\n        repR1:genG3 repR2:genG3 repR3:genG3 repR1:genG4 repR2:genG4 repR3:genG4\nStdDev:    600.1711    600.1711    600.1711    600.1711    600.1711    600.1711\n        repR1:genG5 repR2:genG5 repR3:genG5 repR1:genG6 repR2:genG6 repR3:genG6\nStdDev:    600.1711    600.1711    600.1711    600.1711    600.1711    600.1711\n\n Block 3: repR1:nitro0, repR2:nitro0, repR3:nitro0, repR1:nitro60, repR2:nitro60, repR3:nitro60, repR1:nitro120, repR2:nitro120, repR3:nitro120\n Formula: ~0 + rep:nitro | one\n Structure: Multiple of an Identity\n        repR1:nitro0 repR2:nitro0 repR3:nitro0 repR1:nitro60 repR2:nitro60\nStdDev:     235.2591     235.2591     235.2591      235.2591      235.2591\n        repR3:nitro60 repR1:nitro120 repR2:nitro120 repR3:nitro120 Residual\nStdDev:      235.2591       235.2591       235.2591       235.2591 641.5963\n\nFixed effects:  yield ~ nitro * gen \n                   Value Std.Error DF   t-value p-value\n(Intercept)     3571.667  572.1257 36  6.242800  0.0000\nnitro60         1560.333  557.9682 36  2.796456  0.0082\nnitro120        3976.333  557.9682 36  7.126452  0.0000\ngenG2           1362.667  717.3336 36  1.899628  0.0655\ngenG3            678.000  717.3336 36  0.945167  0.3509\ngenG4            487.333  717.3336 36  0.679368  0.5012\ngenG5            530.000  717.3336 36  0.738847  0.4648\ngenG6           -364.333  717.3336 36 -0.507899  0.6146\nnitro60:genG2    219.000  740.8516 36  0.295606  0.7692\nnitro120:genG2 -1699.333  740.8516 36 -2.293757  0.0277\nnitro60:genG3    312.333  740.8516 36  0.421587  0.6758\nnitro120:genG3  -357.667  740.8516 36 -0.482778  0.6322\nnitro60:genG4    -65.667  740.8516 36 -0.088637  0.9299\nnitro120:genG4  -941.000  740.8516 36 -1.270160  0.2122\nnitro60:genG5    -28.667  740.8516 36 -0.038694  0.9693\nnitro120:genG5 -2066.000  740.8516 36 -2.788682  0.0084\nnitro60:genG6  -1053.333  740.8516 36 -1.421787  0.1637\nnitro120:genG6 -4691.667  740.8516 36 -6.332802  0.0000\n Correlation: \n               (Intr) nitr60 ntr120 genG2  genG3  genG4  genG5  genG6  n60:G2\nnitro60        -0.488                                                        \nnitro120       -0.488  0.500                                                 \ngenG2          -0.627  0.343  0.343                                          \ngenG3          -0.627  0.343  0.343  0.500                                   \ngenG4          -0.627  0.343  0.343  0.500  0.500                            \ngenG5          -0.627  0.343  0.343  0.500  0.500  0.500                     \ngenG6          -0.627  0.343  0.343  0.500  0.500  0.500  0.500              \nnitro60:genG2   0.324 -0.664 -0.332 -0.516 -0.258 -0.258 -0.258 -0.258       \nnitro120:genG2  0.324 -0.332 -0.664 -0.516 -0.258 -0.258 -0.258 -0.258  0.500\nnitro60:genG3   0.324 -0.664 -0.332 -0.258 -0.516 -0.258 -0.258 -0.258  0.500\nnitro120:genG3  0.324 -0.332 -0.664 -0.258 -0.516 -0.258 -0.258 -0.258  0.250\nnitro60:genG4   0.324 -0.664 -0.332 -0.258 -0.258 -0.516 -0.258 -0.258  0.500\nnitro120:genG4  0.324 -0.332 -0.664 -0.258 -0.258 -0.516 -0.258 -0.258  0.250\nnitro60:genG5   0.324 -0.664 -0.332 -0.258 -0.258 -0.258 -0.516 -0.258  0.500\nnitro120:genG5  0.324 -0.332 -0.664 -0.258 -0.258 -0.258 -0.516 -0.258  0.250\nnitro60:genG6   0.324 -0.664 -0.332 -0.258 -0.258 -0.258 -0.258 -0.516  0.500\nnitro120:genG6  0.324 -0.332 -0.664 -0.258 -0.258 -0.258 -0.258 -0.516  0.250\n               n120:G2 n60:G3 n120:G3 n60:G4 n120:G4 n60:G5 n120:G5 n60:G6\nnitro60                                                                   \nnitro120                                                                  \ngenG2                                                                     \ngenG3                                                                     \ngenG4                                                                     \ngenG5                                                                     \ngenG6                                                                     \nnitro60:genG2                                                             \nnitro120:genG2                                                            \nnitro60:genG3   0.250                                                     \nnitro120:genG3  0.500   0.500                                             \nnitro60:genG4   0.250   0.500  0.250                                      \nnitro120:genG4  0.500   0.250  0.500   0.500                              \nnitro60:genG5   0.250   0.500  0.250   0.500  0.250                       \nnitro120:genG5  0.500   0.250  0.500   0.250  0.500   0.500               \nnitro60:genG6   0.250   0.500  0.250   0.500  0.250   0.500  0.250        \nnitro120:genG6  0.500   0.250  0.500   0.250  0.500   0.250  0.500   0.500\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-1.52993309 -0.52842524  0.05394367  0.51465584  1.46902934 \n\nNumber of Observations: 54\nNumber of Groups: 1 \n\n\n\n\n\n\n\n\n\n\n\n\n\nCrossed random effects\n\n\n\nThis type of variance-covariance structure in nlme::lme() is represented by a pdBlocked object with pdIdent elements.\n\n\n\n\n9.2.3 Check Model Assumptions\nLet‚Äôs evaluate the assumptions of linear mixed models by looking at the residuals and normality of error terms.\n\nlme4nlme\n\n\n\ncheck_model(model_lmer,  check = c('qq', 'linearity', 'reqq'), detrend=FALSE, alpha =0)\n\n\n\n\n\n\n\n\n\n\nplot(model_lme, resid(., scaled=TRUE) ~ fitted(.), \n     xlab = \"fitted values\", ylab = \"studentized residuals\")\nqqnorm(residuals(model_lme))\nqqline(residuals(model_lme))\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe residuals fit the assumptions of the model well.\n\n\n9.2.4 Inference\nWe can evaluate the model for the analysis of variance, for main and interaction effects.\n\nlme4nlme\n\n\n\ncar::Anova(model_lmer, type = \"III\")\n\nAnalysis of Deviance Table (Type III Wald chisquare tests)\n\nResponse: yield\n              Chisq Df Pr(&gt;Chisq)    \n(Intercept) 38.9728  1  4.298e-10 ***\nnitro       51.5701  2  6.334e-12 ***\ngen          6.8343  5     0.2333    \nnitro:gen   58.0064 10  8.621e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nanova(model_lme, type = \"marginal\")\n\n            numDF denDF  F-value p-value\n(Intercept)     1    36 38.97256  &lt;.0001\nnitro           2    36 25.78512  &lt;.0001\ngen             5    36  1.36687  0.2597\nnitro:gen      10    36  5.80061  &lt;.0001\n\n\n\n\n\nAnalysis of variance showed a significant interaction impact of gen and nitro on rice grain yield.\nNext, We can estimate marginal means for nitro and gen interaction effects using the emmeans package.\n\nlme4nlme\n\n\n\nemm1 &lt;- emmeans(model_lmer, ~ nitro*gen) \nemm1\n\n nitro gen emmean  SE   df lower.CL upper.CL\n 0     G1    3572 572 17.8     2368     4775\n 60    G1    5132 572 17.8     3929     6335\n 120   G1    7548 572 17.8     6345     8751\n 0     G2    4934 572 17.8     3731     6138\n 60    G2    6714 572 17.8     5510     7917\n 120   G2    7211 572 17.8     6008     8415\n 0     G3    4250 572 17.8     3046     5453\n 60    G3    6122 572 17.8     4919     7326\n 120   G3    7868 572 17.8     6665     9072\n 0     G4    4059 572 17.8     2856     5262\n 60    G4    5554 572 17.8     4350     6757\n 120   G4    7094 572 17.8     5891     8298\n 0     G5    4102 572 17.8     2898     5305\n 60    G5    5633 572 17.8     4430     6837\n 120   G5    6012 572 17.8     4809     7215\n 0     G6    3207 572 17.8     2004     4411\n 60    G6    3714 572 17.8     2511     4918\n 120   G6    2492 572 17.8     1289     3695\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\n\n\nemm1 &lt;- emmeans(model_lme, ~ nitro*gen)\nemm1\n\nWarning in qt((1 - level)/adiv, df): NaNs produced\n\n\n nitro gen emmean  SE df lower.CL upper.CL\n 0     G1    3572 572  0      NaN      NaN\n 60    G1    5132 572  0      NaN      NaN\n 120   G1    7548 572  0      NaN      NaN\n 0     G2    4934 572  0      NaN      NaN\n 60    G2    6714 572  0      NaN      NaN\n 120   G2    7211 572  0      NaN      NaN\n 0     G3    4250 572  0      NaN      NaN\n 60    G3    6122 572  0      NaN      NaN\n 120   G3    7868 572  0      NaN      NaN\n 0     G4    4059 572  0      NaN      NaN\n 60    G4    5554 572  0      NaN      NaN\n 120   G4    7094 572  0      NaN      NaN\n 0     G5    4102 572  0      NaN      NaN\n 60    G5    5633 572  0      NaN      NaN\n 120   G5    6012 572  0      NaN      NaN\n 0     G6    3207 572  0      NaN      NaN\n 60    G6    3714 572  0      NaN      NaN\n 120   G6    2492 572  0      NaN      NaN\n\nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\n\n\n\nNote that, confidence intervals were not estimated through emmeans() from lme() model. This was due to complex nested and crossed random effects in the strip-plot design. If the random-effects structure is too complex, emmeans might struggle to recover correct error terms.\n\n\n\n\n\n\nlme vs lmer\n\n\n\nFor strip plot experiment design, fitting nested and crossed random effects is more complicated through nlme. Therefore, it‚Äôs more convenient to use lme4 in this case as both models yielded same results in the example shown above.",
    "crumbs": [
      "Experiment designs",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Strip Plot Design</span>"
    ]
  },
  {
    "objectID": "chapters/incomplete-block-design.html",
    "href": "chapters/incomplete-block-design.html",
    "title": "10¬† Incomplete Block Design",
    "section": "",
    "text": "10.1 Background\nThe block design described in Chapter 4 was complete, meaning that each block contained each treatment level at least once. In practice, it may not be possible or advisable to include all treatments in each block, either due to limitations in treatment availability (e.g.¬†limited seed stocks) or the block size becomes too large to serve its original goals of controlling for spatial variation.\nIn such cases, randomized incomplete block designs (IBD) can be used. Incomplete block designs break the experiment into many smaller incomplete blocks that are nested within standard RCBD-style blocks and assigns a subset of the treatment levels to each incomplete block. There are several different approaches (Yates 1936) for how to assign treatment levels to incomplete blocks and these designs impact the final statistical analysis (and if all treatments included in the experimental design are estimable). An excellent description of incomplete block design is provided in ANOVA and Mixed Models by Lukas Meier.\nIncomplete block designs are grouped into two groups: (1) balanced lattice designs; and (2) partially balanced (also commonly called alpha-lattice) designs. Balanced IBD designs have been previously called ‚Äúlattice designs‚Äù (Nair 1952), but we are not using that term to avoid confusion with alpha-lattice designs, a term that is commonly used.\nIn alpha-lattice design, the blocks are grouped into complete replicates. These designs are also termed as ‚Äúresolvable incomplete block designs‚Äù or ‚Äúpartially balanced incomplete block designs‚Äù (Patterson and Williams 1976). This design has been more commonly used instead of balanced IBD because of it‚Äôs practicability, flexibility, and versatility. In this design, we have t treatment groups, each occuring r times, with bk experimentals units groups iinto b blocks of size k&lt;v in such a manner that units within a block are same and units in different blocks are substantially different.",
    "crumbs": [
      "Experiment designs",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Incomplete Block Design</span>"
    ]
  },
  {
    "objectID": "chapters/incomplete-block-design.html#background",
    "href": "chapters/incomplete-block-design.html#background",
    "title": "10¬† Incomplete Block Design",
    "section": "",
    "text": "10.1.1 Statistical Model\nThe statistical model for a balanced incomplete block design is:\n\\[y_{ij} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}\\]\nWhere:\n\\(\\mu\\) = overall experimental mean\n\\(\\alpha\\) = treatment effects (fixed)\n\\(\\beta\\) = block effects (random)\n\\(\\epsilon\\) = error terms\n\\[ \\epsilon \\sim N(0, \\sigma)\\]\n\\[ \\beta \\sim N(0, \\sigma_b)\\]\nThere are few key points that we need to keep in mind while designing incomplete block experiments:\n\nA drawback of this design is that block effect and treatment effects are confounded.\nTo remove the block effects, it is better compare treatments within a block.\nNo treatment should appear twice in any block as it contributes nothing to within block comparisons.\n\nThe balanced incomplete block designs are guided by strict principles and guidelines including: the number of treatments must be a perfect square (e.g.¬†25, 36, and so on), and number of replicates must be equal to number of blocks + 1.\n\n\n\n\n\n\nNote on Sums of Squares\n\n\n\nBecause the blocks are incomplete, the type I and type III sums of squares will be different even when there is no missing data from a trail. That is because the missing treatments in each block represent missing observations (even though they are not missing ‚Äòat random‚Äô).",
    "crumbs": [
      "Experiment designs",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Incomplete Block Design</span>"
    ]
  },
  {
    "objectID": "chapters/incomplete-block-design.html#examples-analyses",
    "href": "chapters/incomplete-block-design.html#examples-analyses",
    "title": "10¬† Incomplete Block Design",
    "section": "10.2 Examples Analyses",
    "text": "10.2 Examples Analyses\n\n10.2.1 Balanced Incomplete Block Design\nWe will demonstrate an example data set designed in a balanced incomplete block design. First, load the libraries required for analysis.\n\nlme4nlme\n\n\n\nlibrary(lme4); library(lmerTest); library(emmeans)\nlibrary(dplyr); library(broom.mixed); library(performance)\n\n\n\n\nlibrary(nlme); library(broom.mixed); library(emmeans)\nlibrary(dplyr); library(performance)\n\n\n\n\nThe data used for this example analysis is from the agridat package (data set ‚Äúweiss.incblock‚Äù). This example is comprised of soybean balanced incomplete block experiment.\n\nweiss &lt;- read.csv(here::here(\"data\", \"weiss_ICB.csv\"))\n\n\nTable of variables in the data set\n\n\nblock\nblocking unit\n\n\ngen\ngenotype (variety) factor\n\n\nrow\nrow position for each plot\n\n\ncol\ncolumn position for each plot\n\n\nyield\ngrain yield in bu/ac\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.2.1.1 Data integrity checks\n\nCheck structure of the data\n\nWe will start inspecting the data set first by looking at the class of each variable:\n\nstr(weiss)\n\n'data.frame':   186 obs. of  5 variables:\n $ block: chr  \"B01\" \"B02\" \"B03\" \"B04\" ...\n $ gen  : chr  \"G24\" \"G15\" \"G20\" \"G18\" ...\n $ yield: num  29.8 24.2 30.5 20 35.2 25 23.6 23.6 29.3 25.5 ...\n $ row  : int  42 36 30 24 18 12 6 42 36 30 ...\n $ col  : int  1 1 1 1 1 1 1 2 2 2 ...\n\n\nThe variables we need for the model are block, gen, and yield. The block and gen are classified as factor variables and yield is numeric. Therefore, we do not need to change class of any of the required variables.\n\nInspect the independent variables\n\nNext, let‚Äôs check the independent variables. We can look at this by running a cross tabulations among block and gen factors.\n\nagg_tbl &lt;- weiss %&gt;% group_by(gen) %&gt;% \n  summarise(total_count=n(),\n            .groups = 'drop')\nagg_tbl\n\n# A tibble: 31 √ó 2\n   gen   total_count\n   &lt;chr&gt;       &lt;int&gt;\n 1 G01             6\n 2 G02             6\n 3 G03             6\n 4 G04             6\n 5 G05             6\n 6 G06             6\n 7 G07             6\n 8 G08             6\n 9 G09             6\n10 G10             6\n# ‚Ñπ 21 more rows\n\n\n\nagg_df &lt;- aggregate(weiss$gen, by=list(weiss$block), FUN=length)\nagg_df\n\n   Group.1 x\n1      B01 6\n2      B02 6\n3      B03 6\n4      B04 6\n5      B05 6\n6      B06 6\n7      B07 6\n8      B08 6\n9      B09 6\n10     B10 6\n11     B11 6\n12     B12 6\n13     B13 6\n14     B14 6\n15     B15 6\n16     B16 6\n17     B17 6\n18     B18 6\n19     B19 6\n20     B20 6\n21     B21 6\n22     B22 6\n23     B23 6\n24     B24 6\n25     B25 6\n26     B26 6\n27     B27 6\n28     B28 6\n29     B29 6\n30     B30 6\n31     B31 6\n\n\nThere are 31 varieties (levels of gen) and it is perfectly balanced, with exactly one observation per treatment per block.\n\nCheck the extent of missing data\n\nWe can calculate the sum of missing values in variables in this data set to evaluate the extent of missing values in different variables:\n\ncolSums(is.na(weiss))\n\nblock   gen yield   row   col \n    0     0     0     0     0 \n\n\nNo missing data!\n\nInspect the dependent variable\n\nLast, let‚Äôs plot a histogram of the dependent variable. This is a quick check before analysis to see if there is any strong deviation in values.\n\n\n\n\n\n\n\n\n\nFigure¬†10.1: Histogram of the dependent variable.\n\n\n\n\n\nhist(weiss$yield, main = NA, xlab = \"yield\")\n\nResponse variable values fall within expected range, with few extreme values on right tail. This data set is ready for analysis!\n\n\n10.2.1.2 Model Building\nWe will be evaluating the response of yield as affected by gen (fixed effect) and block (random effect). 1\n1¬† Please note that incomplete block effect can be analyzed as a fixed (intra-block analysis) or a random (inter-block analysis) effect. When we consider block as a random effect, the mean values of a block also contain information about the treatment effects.\nlme4nlme\n\n\n\nmodel_icbd &lt;- lmer(yield ~ gen + (1|block),\n                   data = weiss, \n                   na.action = na.exclude)\ntidy(model_icbd)\n\n# A tibble: 33 √ó 8\n   effect group term        estimate std.error statistic    df  p.value\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 fixed  &lt;NA&gt;  (Intercept)  24.6        0.922   26.7     153. 2.30e-59\n 2 fixed  &lt;NA&gt;  genG02        2.40       1.17     2.06    129. 4.17e- 2\n 3 fixed  &lt;NA&gt;  genG03        8.04       1.17     6.88    129. 2.31e-10\n 4 fixed  &lt;NA&gt;  genG04        2.37       1.17     2.03    129. 4.42e- 2\n 5 fixed  &lt;NA&gt;  genG05        1.60       1.17     1.37    129. 1.73e- 1\n 6 fixed  &lt;NA&gt;  genG06        7.39       1.17     6.32    129. 3.82e- 9\n 7 fixed  &lt;NA&gt;  genG07       -0.419      1.17    -0.359   129. 7.20e- 1\n 8 fixed  &lt;NA&gt;  genG08        3.04       1.17     2.60    129. 1.04e- 2\n 9 fixed  &lt;NA&gt;  genG09        4.84       1.17     4.14    129. 6.22e- 5\n10 fixed  &lt;NA&gt;  genG10       -0.0429     1.17    -0.0367  129. 9.71e- 1\n# ‚Ñπ 23 more rows\n\n\n\n\n\nmodel_icbd1 &lt;- lme(yield ~ gen,\n                  random = ~ 1|block,\n                  data = weiss, \n                  na.action = na.exclude)\ntidy(model_icbd1)\n\n# A tibble: 33 √ó 8\n   effect group term        estimate std.error    df statistic  p.value\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 fixed  &lt;NA&gt;  (Intercept)  24.6        0.922   125   26.7    2.10e-53\n 2 fixed  &lt;NA&gt;  genG02        2.40       1.17    125    2.06   4.18e- 2\n 3 fixed  &lt;NA&gt;  genG03        8.04       1.17    125    6.88   2.54e-10\n 4 fixed  &lt;NA&gt;  genG04        2.37       1.17    125    2.03   4.43e- 2\n 5 fixed  &lt;NA&gt;  genG05        1.60       1.17    125    1.37   1.73e- 1\n 6 fixed  &lt;NA&gt;  genG06        7.39       1.17    125    6.32   4.11e- 9\n 7 fixed  &lt;NA&gt;  genG07       -0.419      1.17    125   -0.359  7.20e- 1\n 8 fixed  &lt;NA&gt;  genG08        3.04       1.17    125    2.60   1.04e- 2\n 9 fixed  &lt;NA&gt;  genG09        4.84       1.17    125    4.14   6.33e- 5\n10 fixed  &lt;NA&gt;  genG10       -0.0429     1.17    125   -0.0367 9.71e- 1\n# ‚Ñπ 23 more rows\n\n\n\n\n\n\n\n10.2.1.3 Check Model Assumptions\nLet‚Äôs verify the assumption of linear mixed models including normal distribution and constant variance of residuals.\n\nlme4nlme\n\n\n\ncheck_model(model_icbd, check = c('qq', 'linearity', 'reqq'), detrend=FALSE, alpha =0)\n\n\n\n\n\n\n\n\n\n\n\ncheck_model(model_icbd1, check = c('qq', 'linearity'), detrend=FALSE, alpha = 0)\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.2.1.4 Inference\nWe can extract information about ANOVA using anova().\n\nlme4nlme\n\n\n\nanova(model_icbd, type = \"1\")\n\nType I Analysis of Variance Table with Satterthwaite's method\n    Sum Sq Mean Sq NumDF  DenDF F value    Pr(&gt;F)    \ngen 1901.1  63.369    30 129.06  17.675 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nanova(model_icbd1, type = \"sequential\")\n\n            numDF denDF  F-value p-value\n(Intercept)     1   125 4042.016  &lt;.0001\ngen            30   125   17.675  &lt;.0001\n\n\n\n\n\nLet‚Äôs look at the estimated marginal means of yield for each variety (gen).\n\nlme4nlme\n\n\n\nemmeans(model_icbd, ~ gen)\n\n gen emmean    SE  df lower.CL upper.CL\n G01   24.6 0.923 153     22.7     26.4\n G02   27.0 0.923 153     25.2     28.8\n G03   32.6 0.923 153     30.8     34.4\n G04   26.9 0.923 153     25.1     28.8\n G05   26.2 0.923 153     24.4     28.0\n G06   32.0 0.923 153     30.1     33.8\n G07   24.2 0.923 153     22.3     26.0\n G08   27.6 0.923 153     25.8     29.4\n G09   29.4 0.923 153     27.6     31.2\n G10   24.5 0.923 153     22.7     26.4\n G11   27.1 0.923 153     25.2     28.9\n G12   29.3 0.923 153     27.4     31.1\n G13   29.9 0.923 153     28.1     31.8\n G14   24.2 0.923 153     22.4     26.1\n G15   26.1 0.923 153     24.3     27.9\n G16   25.9 0.923 153     24.1     27.8\n G17   19.7 0.923 153     17.9     21.5\n G18   25.7 0.923 153     23.9     27.5\n G19   29.0 0.923 153     27.2     30.9\n G20   33.2 0.923 153     31.3     35.0\n G21   31.1 0.923 153     29.3     32.9\n G22   25.2 0.923 153     23.3     27.0\n G23   29.8 0.923 153     28.0     31.6\n G24   33.6 0.923 153     31.8     35.5\n G25   27.0 0.923 153     25.2     28.8\n G26   27.1 0.923 153     25.3     29.0\n G27   23.8 0.923 153     22.0     25.6\n G28   26.5 0.923 153     24.6     28.3\n G29   24.8 0.923 153     22.9     26.6\n G30   36.2 0.923 153     34.4     38.0\n G31   27.1 0.923 153     25.3     28.9\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\n\n\nemmeans(model_icbd1, ~ gen)\n\n gen emmean    SE df lower.CL upper.CL\n G01   24.6 0.922 30     22.7     26.5\n G02   27.0 0.922 30     25.1     28.9\n G03   32.6 0.922 30     30.7     34.5\n G04   26.9 0.922 30     25.1     28.8\n G05   26.2 0.922 30     24.3     28.1\n G06   32.0 0.922 30     30.1     33.8\n G07   24.2 0.922 30     22.3     26.0\n G08   27.6 0.922 30     25.7     29.5\n G09   29.4 0.922 30     27.5     31.3\n G10   24.5 0.922 30     22.6     26.4\n G11   27.1 0.922 30     25.2     28.9\n G12   29.3 0.922 30     27.4     31.1\n G13   29.9 0.922 30     28.1     31.8\n G14   24.2 0.922 30     22.4     26.1\n G15   26.1 0.922 30     24.2     28.0\n G16   25.9 0.922 30     24.0     27.8\n G17   19.7 0.922 30     17.8     21.6\n G18   25.7 0.922 30     23.8     27.6\n G19   29.0 0.922 30     27.2     30.9\n G20   33.2 0.922 30     31.3     35.0\n G21   31.1 0.922 30     29.2     33.0\n G22   25.2 0.922 30     23.3     27.1\n G23   29.8 0.922 30     27.9     31.7\n G24   33.6 0.922 30     31.8     35.5\n G25   27.0 0.922 30     25.1     28.9\n G26   27.1 0.922 30     25.3     29.0\n G27   23.8 0.922 30     21.9     25.7\n G28   26.5 0.922 30     24.6     28.4\n G29   24.8 0.922 30     22.9     26.6\n G30   36.2 0.922 30     34.3     38.1\n G31   27.1 0.922 30     25.2     29.0\n\nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\n\n\n\n\n\n\n10.2.2 Partially Balanced IBD (Alpha Lattice Design)\nThe statistical model for partially balanced design includes:\n\\[y_{ij(l)} = \\mu + \\alpha_i + \\beta_{i(l)} + \\tau_j + \\epsilon_{ij(l)}\\]\nWhere:\n\\(\\mu\\) = overall experimental mean\n\\(\\alpha\\) = replicate effect (random)\n\\(\\beta\\) = incomplete block effect (random)\n\\(\\tau\\) = treatment effect (fixed)\n\\(\\epsilon_{ij(l)}\\) = intra-block residual\nThe data used in this example is published in Cyclic and Computer Generated Designs (John and Williams 1995). The trial was laid out in an alpha lattice design. This trial data had 24 genotypes (‚Äúgen‚Äù), 6 incomplete blocks, each replicated 3 times.\nLet‚Äôs start analyzing this example first by loading the required libraries for linear mixed models:\n\nlme4nlme\n\n\n\nlibrary(lme4); library(lmerTest); library(emmeans)\nlibrary(dplyr); library(broom.mixed); library(performance)\n\n\n\n\nlibrary(nlme); library(broom.mixed); library(emmeans)\nlibrary(dplyr); library(performance)\n\n\n\n\nLet‚Äôs import a data with partial balanced icomplete design. This data was obtained from the agridat package.\n\np_icb &lt;- read.csv(here::here(\"data\", \"partial_incblock.csv\"))\n\n\nTable of variables in the data set\n\n\nblock\nincomplete blocking unit\n\n\ngen\ngenotype (variety) factor\n\n\nrow\nrow position for each plot\n\n\ncol\ncolumn position for each plot\n\n\nyield\ngrain yield in tonnes/ha\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.2.2.1 Data integrity checks\n\nCheck structure of the data\n\nLet‚Äôs look into the structure of the data first to verify the class of the variables.\n\nstr(p_icb)\n\n'data.frame':   72 obs. of  7 variables:\n $ plot : int  1 2 3 4 5 6 7 8 9 10 ...\n $ rep  : chr  \"R1\" \"R1\" \"R1\" \"R1\" ...\n $ block: chr  \"B1\" \"B1\" \"B1\" \"B1\" ...\n $ gen  : chr  \"G11\" \"G04\" \"G05\" \"G22\" ...\n $ yield: num  4.12 4.45 5.88 4.58 4.65 ...\n $ row  : int  1 2 3 4 5 6 7 8 9 10 ...\n $ col  : int  1 1 1 1 1 1 1 1 1 1 ...\n\n\nHere, rep, block and gen are character and yield as a integer. We can continue with this.\n\nInspect the independent variables\n\nNext step is to evaluate the independent variables. First, check the number of treatments per replication (each treatment should be replicated 3 times).\n\nagg_tbl &lt;- p_icb %&gt;% group_by(gen) %&gt;% \n  summarise(total_count=n(),\n            .groups = 'drop')\nagg_tbl\n\n# A tibble: 24 √ó 2\n   gen   total_count\n   &lt;chr&gt;       &lt;int&gt;\n 1 G01             3\n 2 G02             3\n 3 G03             3\n 4 G04             3\n 5 G05             3\n 6 G06             3\n 7 G07             3\n 8 G08             3\n 9 G09             3\n10 G10             3\n# ‚Ñπ 14 more rows\n\n\nThis looks balanced, as expected. Also, let‚Äôs have a look at the number of times each treatment appear per block.\n\n(agg_blk &lt;- aggregate(p_icb$gen, by=list(p_icb$block), FUN=length))\n\n  Group.1  x\n1      B1 12\n2      B2 12\n3      B3 12\n4      B4 12\n5      B5 12\n6      B6 12\n\n\n12 treatments randomly appear in incomplete block. Each incomplete block has same number of treatments.\n\nCheck the extent of missing data\n\n\ncolSums(is.na(p_icb))\n\n plot   rep block   gen yield   row   col \n    0     0     0     0     0     0     0 \n\n\nNo missing values in data!\n\nInspect the dependent variable\n\nBefore fitting the model, it‚Äôs a good idea to look at the distribution of dependent variable, yield.\n\n\n\n\n\n\n\n\n\nFigure¬†10.2: Histogram of the dependent variable.\n\n\n\n\n\nhist(p_icb$yield, main = NA, xlab = \"yield\")\n\nThe response variables seems to follow a normal distribution curve, with few values on extreme lower and higher ends.\n\n\n10.2.2.2 Model Building\nWe are evaluating the response of yield to Gen (fixed effect) and rep and block as a random effect.\n\nlme4nlme\n\n\n\nmod_alpha &lt;- lmer(yield ~ gen + (1|rep/block),\n                   data = p_icb, \n                   na.action = na.exclude)\ntidy(mod_alpha)\n\n# A tibble: 27 √ó 8\n   effect group term        estimate std.error statistic    df     p.value\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n 1 fixed  &lt;NA&gt;  (Intercept)   5.11       0.276    18.5    6.19 0.00000118 \n 2 fixed  &lt;NA&gt;  genG02       -0.629      0.269    -2.34  38.2  0.0248     \n 3 fixed  &lt;NA&gt;  genG03       -1.61       0.268    -6.00  37.7  0.000000590\n 4 fixed  &lt;NA&gt;  genG04       -0.618      0.268    -2.30  37.7  0.0269     \n 5 fixed  &lt;NA&gt;  genG05       -0.0705     0.258    -0.274 34.8  0.786      \n 6 fixed  &lt;NA&gt;  genG06       -0.571      0.268    -2.13  37.7  0.0398     \n 7 fixed  &lt;NA&gt;  genG07       -0.997      0.258    -3.87  34.8  0.000457   \n 8 fixed  &lt;NA&gt;  genG08       -0.580      0.268    -2.16  37.7  0.0370     \n 9 fixed  &lt;NA&gt;  genG09       -1.61       0.258    -6.21  35.3  0.000000390\n10 fixed  &lt;NA&gt;  genG10       -0.735      0.259    -2.83  35.9  0.00754    \n# ‚Ñπ 17 more rows\n\n\n\n\n\nmod_alpha1 &lt;- lme(yield ~ gen,\n                  random = ~ 1|rep/block,\n                  data = p_icb, \n                  na.action = na.exclude)\ntidy(mod_alpha1)\n\nWarning in tidy.lme(mod_alpha1): ran_pars not yet implemented for multiple\nlevels of nesting\n\n\n# A tibble: 24 √ó 7\n   effect term        estimate std.error    df statistic  p.value\n   &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 fixed  (Intercept)   5.11       0.276    31    18.5   2.63e-18\n 2 fixed  genG02       -0.629      0.269    31    -2.34  2.61e- 2\n 3 fixed  genG03       -1.61       0.268    31    -6.00  1.23e- 6\n 4 fixed  genG04       -0.618      0.268    31    -2.30  2.81e- 2\n 5 fixed  genG05       -0.0705     0.258    31    -0.274 7.86e- 1\n 6 fixed  genG06       -0.571      0.268    31    -2.13  4.12e- 2\n 7 fixed  genG07       -0.997      0.258    31    -3.87  5.23e- 4\n 8 fixed  genG08       -0.580      0.268    31    -2.16  3.84e- 2\n 9 fixed  genG09       -1.61       0.258    31    -6.21  6.71e- 7\n10 fixed  genG10       -0.735      0.259    31    -2.83  8.05e- 3\n# ‚Ñπ 14 more rows\n\n\n\n\n\n\n\n10.2.2.3 Check Model Assumptions\nLet‚Äôs verify the assumption of linear mixed models including normal distribution and constant variance of residuals.\n\nlme4nlme\n\n\n\ncheck_model(mod_alpha, check = c('qq', 'linearity', 'reqq'), detrend=FALSE, alpha = 0)\n\n\n\n\n\n\n\n\n\n\n\ncheck_model(mod_alpha1, check = c('qq', 'linearity'), detrend=FALSE, alpha = 0)\n\n\n\n\n\n\n\n\n\n\n\nHere a little skewness is present normality of residuals, but that‚Äôs not a major deviation in the model assumptions.\n\n\n10.2.2.4 Inference\nLet‚Äôs look at the ANOVA table using anova() from lmer and lme models, respectively.\n\nlme4nlme\n\n\n\nanova(mod_alpha, type = \"1\")\n\nType I Analysis of Variance Table with Satterthwaite's method\n    Sum Sq Mean Sq NumDF  DenDF F value    Pr(&gt;F)    \ngen 10.679 0.46429    23 34.902  5.4478 4.229e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nanova(mod_alpha1, type = \"sequential\")\n\n            numDF denDF  F-value p-value\n(Intercept)     1    31 470.9507  &lt;.0001\ngen            23    31   5.4478  &lt;.0001\n\n\n\n\n\nLet‚Äôs look at the estimated marginal means of yield for each variety (gen).\n\nlme4nlme\n\n\n\nemmeans(mod_alpha, ~ gen)\n\n gen emmean    SE   df lower.CL upper.CL\n G01   5.11 0.279 6.20     4.43     5.78\n G02   4.48 0.279 6.20     3.80     5.15\n G03   3.50 0.279 6.20     2.82     4.18\n G04   4.49 0.279 6.20     3.81     5.17\n G05   5.04 0.278 6.19     4.36     5.71\n G06   4.54 0.278 6.19     3.86     5.21\n G07   4.11 0.279 6.20     3.43     4.79\n G08   4.53 0.279 6.20     3.85     5.20\n G09   3.50 0.278 6.19     2.83     4.18\n G10   4.37 0.279 6.20     3.70     5.05\n G11   4.28 0.279 6.20     3.61     4.96\n G12   4.76 0.279 6.20     4.08     5.43\n G13   4.76 0.278 6.19     4.08     5.43\n G14   4.78 0.278 6.19     4.10     5.45\n G15   4.97 0.278 6.19     4.29     5.65\n G16   4.73 0.279 6.20     4.05     5.41\n G17   4.60 0.278 6.19     3.93     5.28\n G18   4.36 0.279 6.20     3.69     5.04\n G19   4.84 0.278 6.19     4.16     5.52\n G20   4.04 0.278 6.19     3.36     4.72\n G21   4.80 0.278 6.19     4.12     5.47\n G22   4.53 0.278 6.19     3.85     5.20\n G23   4.25 0.278 6.19     3.58     4.93\n G24   4.15 0.279 6.20     3.48     4.83\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\n\n\nemmeans(mod_alpha1, ~ gen)\n\n gen emmean    SE df lower.CL upper.CL\n G01   5.11 0.276  2     3.92     6.30\n G02   4.48 0.276  2     3.29     5.67\n G03   3.50 0.276  2     2.31     4.69\n G04   4.49 0.276  2     3.30     5.68\n G05   5.04 0.276  2     3.85     6.22\n G06   4.54 0.276  2     3.35     5.72\n G07   4.11 0.276  2     2.92     5.30\n G08   4.53 0.276  2     3.34     5.72\n G09   3.50 0.276  2     2.31     4.69\n G10   4.37 0.276  2     3.19     5.56\n G11   4.28 0.276  2     3.10     5.47\n G12   4.76 0.276  2     3.57     5.94\n G13   4.76 0.276  2     3.57     5.95\n G14   4.78 0.276  2     3.59     5.96\n G15   4.97 0.276  2     3.78     6.16\n G16   4.73 0.276  2     3.54     5.92\n G17   4.60 0.276  2     3.42     5.79\n G18   4.36 0.276  2     3.17     5.55\n G19   4.84 0.276  2     3.65     6.03\n G20   4.04 0.276  2     2.85     5.23\n G21   4.80 0.276  2     3.61     5.98\n G22   4.53 0.276  2     3.34     5.72\n G23   4.25 0.276  2     3.06     5.44\n G24   4.15 0.276  2     2.97     5.34\n\nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\n\n\n\n\n\n\n\nJohn, JA, and ER Williams. 1995. Cyclic and Computer Generated Designs. 2nd ed. New York: Chapman; Hall/CRC Press. https://doi.org/10.1201/b15075.\n\n\nNair, K. R. 1952. ‚ÄúAnalysis of Partially Balanced Incomplete Block Designs Illustrated on the Simple Square and Rectangular Lattices.‚Äù Biometrics 8 (2): 122‚Äì55. https://doi.org/10.2307/3001929.\n\n\nPatterson, H. D., and E. R. Williams. 1976. ‚ÄúA New Class of Resolvable Incomplete Block Designs.‚Äù Biometrika 63 (1): 83‚Äì92. https://doi.org/10.2307/2335087.\n\n\nYates, F. 1936. ‚ÄúA New Method of Arranging Variety Trials Involving a Large Number of Varieties.‚Äù J Agric Sci 26: 424‚Äì55.",
    "crumbs": [
      "Experiment designs",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Incomplete Block Design</span>"
    ]
  },
  {
    "objectID": "chapters/latin-design.html",
    "href": "chapters/latin-design.html",
    "title": "11¬† Latin Square Design",
    "section": "",
    "text": "11.1 Background\nIn the Latin Square design, two blocking factors are are assigned to rows and columns, respectively of the square. This allows blocking across rows and columns to reduce the variability in those spatial units in the trial. The requirement of Latin square design is that all ‚Äòt‚Äô treatments appears only once in each row and column and number of replications is equal to number of treatments.\nAdvantages of Latin square design are:\nDisadvantages:\nThe statistical model for the Latin square design:\n\\(Y_{ijk} = \\mu + \\alpha_i + \\beta_j +  \\gamma_k + \\epsilon_{ijk}\\)\nwhere, \\(\\mu\\) is the experiment mean, \\(\\alpha_i\\) represents treatment effect, \\(\\beta\\) and \\(\\gamma\\) are the row- and column-specific effects.\nAssumptions of this design includes normality and independent distribution of error (\\(\\epsilon_{ijk}\\)) terms. And there is no interaction between two blocking (rows & columns) factors and treatments.",
    "crumbs": [
      "Experiment designs",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Latin Square Design</span>"
    ]
  },
  {
    "objectID": "chapters/latin-design.html#background",
    "href": "chapters/latin-design.html#background",
    "title": "11¬† Latin Square Design",
    "section": "",
    "text": "The design is particularly appropriate for comparing t treatment means in the presence of two sources of extraneous variation, each measured at t levels.\nEasy to analyze.\n\n\n\nA Latin square can be constructed for any value of t, however, it is best suited for comparing t treatments when 5‚â§ t‚â§ 10.\nAdditional sources of variability reduces efficiency and tend to inflate the error term which makes it more difficult to detect differences among the treatments.\nThere must be no interaction of rows and columns with treatment effects.",
    "crumbs": [
      "Experiment designs",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Latin Square Design</span>"
    ]
  },
  {
    "objectID": "chapters/latin-design.html#example-analysis",
    "href": "chapters/latin-design.html#example-analysis",
    "title": "11¬† Latin Square Design",
    "section": "11.2 Example Analysis",
    "text": "11.2 Example Analysis\nLet‚Äôs start the analysis first by loading the required libraries:\n\nlme4nlme\n\n\n\nlibrary(lme4); library(lmerTest); library(emmeans); library(performance)\nlibrary(dplyr); library(broom.mixed)\n\n\n\n\nlibrary(nlme); library(broom.mixed); library(emmeans); library(performance)\nlibrary(dplyr)\n\n\n\n\nThe data used in this example is from the agridat package (data set ‚Äúgoulden.latin‚Äù). In this experiment, 5 treatmentswere tested to control stem rust in wheat (A = applied before rain; B = applied after rain; C = applied once each week; D = drifting once each week. E = not applied).\n\ndat &lt;- read.csv(here::here(\"data\", \"goulden_latin.csv\"))\n\n\nTable of variables in the data set\n\n\ntrt\ntreatment factor, 5 levels\n\n\nrow\nrow position for each plot\n\n\ncol\ncolumn position for each plot\n\n\nyield\nwheat yield\n\n\n\n\n11.2.1 Data integrity checks\n\nCheck structure of the data\n\nFirst, let‚Äôs verify the class of variables in the data set using str() function in base R\n\nstr(dat)\n\n'data.frame':   25 obs. of  4 variables:\n $ trt  : chr  \"B\" \"C\" \"D\" \"E\" ...\n $ yield: num  4.9 9.3 7.6 5.3 9.3 6.4 4 15.4 7.6 6.3 ...\n $ row  : int  5 4 3 2 1 5 4 3 2 1 ...\n $ col  : int  1 1 1 1 1 2 2 2 2 2 ...\n\n\nHere yield and trt are classified as numeric and factor variables, respectively, as needed. But we need to change ‚Äòrow‚Äô and ‚Äòcol‚Äô from integer to factors (characeter is also a valid choice).\n\ndat1 &lt;- dat |&gt; \n        mutate(row = as.factor(row),\n               col = as.factor(col))\n\n\nInspect the independent variables\n\nNext, to verify if the data meets the assumption of the Latin square design, let‚Äôs plot the field layout for this experiment.\n\n\n\n\n\n\n\n\n\nThis looks great! Here we can see that there are equal number (5) of treatments, rows, and columns. Treatments were randomized in such a way that one treatment doesn‚Äôt appear more than once in each row and column.\n\nCheck the extent of missing data\n\nNext step is to check if there are any missing values in response variable.\n\ncolSums(is.na(dat))\n\n  trt yield   row   col \n    0     0     0     0 \n\n\nNo missing values detected in this data set.\n\nInspect the dependent variable\n\nBefore fitting the model, let‚Äôs create a histogram of response variable to see if there are extreme values.\n\n\n\n\n\n\nHistogram of the dependent variable.\n\n\n\n\nhist(dat1$yield, main = NA, xlab = \"yield\")\n\n\n\n11.2.2 Model fitting\nHere we will fit a model to evaluate the impact of fungicide treatments on wheat yield with ‚Äútrt‚Äù as a fixed effect and ‚Äúrow‚Äù and ‚Äúcol‚Äù as a random effects.\n\nlme4nlme\n\n\n\nm1_a &lt;- lmer(yield ~ trt + (1|row) + (1|col),\n           data = dat1,\n           na.action = na.exclude)\nsummary(m1_a) \n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: yield ~ trt + (1 | row) + (1 | col)\n   Data: dat1\n\nREML criterion at convergence: 89.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.3994 -0.5383 -0.1928  0.5220  1.8429 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n row      (Intercept) 1.8660   1.3660  \n col      (Intercept) 0.2336   0.4833  \n Residual             2.3370   1.5287  \nNumber of obs: 25, groups:  row, 5; col, 5\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(&gt;|t|)    \n(Intercept)   6.8400     0.9420 11.9446   7.261 1.03e-05 ***\ntrtB         -0.3800     0.9669 12.0000  -0.393   0.7012    \ntrtC          6.2800     0.9669 12.0000   6.495 2.96e-05 ***\ntrtD          1.1200     0.9669 12.0000   1.158   0.2692    \ntrtE         -1.9200     0.9669 12.0000  -1.986   0.0704 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n     (Intr) trtB   trtC   trtD  \ntrtB -0.513                     \ntrtC -0.513  0.500              \ntrtD -0.513  0.500  0.500       \ntrtE -0.513  0.500  0.500  0.500\n\n\n\n\n\nm1_b &lt;-lme(yield ~ trt,\n                random = list(one = pdBlocked(list(\n        pdIdent(~ 0 + row), \n         pdIdent(~ 0 + col)))),\n        data = dat %&gt;% mutate(one = factor(1)))\n\nsummary(m1_b)\n\nLinear mixed-effects model fit by REML\n  Data: dat %&gt;% mutate(one = factor(1)) \n       AIC      BIC    logLik\n  105.2465 113.2124 -44.62327\n\nRandom effects:\n Composite Structure: Blocked\n\n Block 1: row\n Formula: ~0 + row | one\n              row\nStdDev: 0.7529153\n\n Block 2: col\n Formula: ~0 + col | one\n                 col Residual\nStdDev: 2.708438e-05 1.737645\n\nFixed effects:  yield ~ trt \n                Value Std.Error DF   t-value p-value\n(Intercept)  8.987259  1.046448 20  8.588351  0.0000\ntrtB        -0.380000  1.098983 20 -0.345774  0.7331\ntrtC         6.280000  1.098983 20  5.714374  0.0000\ntrtD         1.120000  1.098983 20  1.019124  0.3203\ntrtE        -1.920000  1.098983 20 -1.747070  0.0960\n Correlation: \n     (Intr) trtB   trtC   trtD  \ntrtB -0.525                     \ntrtC -0.525  0.500              \ntrtD -0.525  0.500  0.500       \ntrtE -0.525  0.500  0.500  0.500\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-1.78646821 -0.20717699 -0.07394724  0.26228203  2.35462748 \n\nNumber of Observations: 25\nNumber of Groups: 1 \n\n\n\n\n\n\n\n11.2.3 Check Model Assumptions\nThis step involves inspection of the model residuals by using check_model() function from the performance package.\n\nlme4nlme\n\n\n\ncheck_model(m1_a, check = c('qq', 'linearity', 'reqq'), detrend=FALSE, alpha = 0)\n\n\n\n\n\n\n\n\n\n\nplot(m1_b, resid(., scaled=TRUE) ~ fitted(.), \n     xlab = \"fitted values\", ylab = \"studentized residuals\")\nqqnorm(residuals(m1_b))\nqqline(residuals(m1_b))\n\n\n\n\n\n\n\n\n\n\nThese visuals support that assumptions of linear model have been met.\n\n11.2.4 Inference\nWe can now proceed to the variance partioning. In this case, we will use anova() with type = 1 or type = \"sequential\" for lmer() and lme() models, respectively.\n\n\n\n\n\nlme4nlme\n\n\n\nanova(m1_a, type = \"1\")\n\nType I Analysis of Variance Table with Satterthwaite's method\n    Sum Sq Mean Sq NumDF DenDF F value    Pr(&gt;F)    \ntrt 196.61  49.152     4    12  21.032 2.366e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nanova(m1_b, type = \"sequential\")\n\n            numDF denDF   F-value p-value\n(Intercept)     1    20 163.65015  &lt;.0001\ntrt             4    20  16.27868  &lt;.0001\n\n\n\n\n\nHere we observed a statistically significant impact on fungicide treatment on crop yield. Let‚Äôs have a look at the estimated marginal means of wheat yield with each treatment using the emmeans() function.\n\nlme4nlme\n\n\n\nemmeans(m1_a, ~ trt)\n\n trt emmean    SE   df lower.CL upper.CL\n A     6.84 0.942 11.9     4.79     8.89\n B     6.46 0.942 11.9     4.41     8.51\n C    13.12 0.942 11.9    11.07    15.17\n D     7.96 0.942 11.9     5.91    10.01\n E     4.92 0.942 11.9     2.87     6.97\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\n\n\nemmeans(m1_b, ~ trt)\n\nWarning in qt((1 - level)/adiv, df): NaNs produced\n\n\n trt emmean   SE df lower.CL upper.CL\n A     8.99 1.05  0      NaN      NaN\n B     8.61 1.05  0      NaN      NaN\n C    15.27 1.05  0      NaN      NaN\n D    10.11 1.05  0      NaN      NaN\n E     7.07 1.05  0      NaN      NaN\n\nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\n\n\n\nWe see that wheat yield was higher with ‚ÄòC‚Äô fungicide treatment compared to other fungicides applied in this study, indicating that ‚ÄòC‚Äô fungicide was more efficient in controlling the stem rust in wheat.\n\n\n\n\n\n\nCI with lme\n\n\n\nNote that, confidence intervals were not estimated through emmeans() from lme() model due to complex crossed random effects in this model.",
    "crumbs": [
      "Experiment designs",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Latin Square Design</span>"
    ]
  },
  {
    "objectID": "chapters/repeated-measures.html",
    "href": "chapters/repeated-measures.html",
    "title": "12¬† Repeated measures mixed models",
    "section": "",
    "text": "13 Example Analysis\nIn the previous chapters we have covered how to run linear mixed models for different experiment designs. All of the examples in those chapters were independent measure designs, where each subject was assigned to a different treatment. Now we will move on to experiment with repeated measures effects (also called ‚Äúlongitudinal data‚Äù).\nStudies that involve repeated observations of the exact same experimental units (or subjects) requires a repeated measures component in analysis to properly model correlations across time for each subject. This is common in studies that are evaluated across different time periods. For example, if samples are collected over the different time periods from same subject, we have to model the repeated measures effect while analyzing the main effects.\nIn these models, the ‚Äòiid‚Äô assumption (independently and identically distributed) is being violated often, so we need to introduce specialized covariance structures that can account for these correlations between error terms.\nFitting models with correlated observations requires new libraries including mmrm and nlme. The lme4 package allows random effects only. In this chapter, we will analyze the data with repeated measures from different experiment designs including randomized complete block design, split plot, and split-split plot design.\nThere are several types of covariance structures:\nThe repeated measures syntax in nlme follow this convention:\ncorr = corAR1(value = 0, form = ~ t|g, fixed = FALSE).\nThe argument for ‚Äòvalue‚Äô is the starting value for iterations (zero, unless you specify something else), and if fixed = FALSE (the current nlme default), this value will be allowed to change during the model fitting process. The argument structure for form, ~ t or ~ t|g, specifying a time covariate \\(t\\) and, optionally a grouping factor g (if a group factor is not specified, the observation order will be used). When we use ~t|g form, the correlation structure is assumed to apply only to observations within the same grouping level. The covariate for this correlation structure must be a integer value. corCompSymm() and corSymm() follow the same argument syntax.\nThere are other covariance structures (e.g.¬†corARMA(), corCAR1()), but we have found that corAR1() and corCompSymm() work for most circumstances. Type ?cor in an R console for more options and details on the syntax\nFor this chapter we will be analyzing using nlme and mmrm packages. So, let‚Äôs start with loading the required libraries for this analysis. These packages have the easiest implementation of heterogenous error structures. While it is possible to do this with lme4 is requires extensive comfort programming in lme4.\nFirst, we will start with the first example with randomized complete block design with repeated measures.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Repeated Measures</span>"
    ]
  },
  {
    "objectID": "chapters/repeated-measures.html#rcbd-repeated-measures",
    "href": "chapters/repeated-measures.html#rcbd-repeated-measures",
    "title": "12¬† Repeated measures mixed models",
    "section": "13.1 RCBD Repeated Measures",
    "text": "13.1 RCBD Repeated Measures\nThe example shown below contains data from a sorghum trial laid out as a randomized complete block design (5 blocks) with variety (4 varieties) treatment effect. The response variable ‚Äòy‚Äô is the leaf area index assessed in five consecutive weeks on each plot.\nWe need to have time as numeric and factor variable. In the model, to assess the week effect, week was used as a factor (factweek). For the correlation matrix, week needs to be numeric (week).\n\nsorghum &lt;- read.csv(here::here(\"data\", \"sorghum.csv\")) |&gt; \n  mutate(block = as.character(varblock),\n         factweek = as.character(factweek),\n         variety = as.character(variety))\n\n\nTable of variables in the data set\n\n\nblock\nblocking unit\n\n\nReplicate\nreplication unit\n\n\nWeek\nTime points when data was collected\n\n\nvariety\ntreatment factor, 4 levels\n\n\ny\nleaf area index\n\n\n\n\n13.1.1 Data Integrity Checks\nLet‚Äôs do preliminary data check including evaluating data structure, distribution of treatments, number of missing values, and distribution of response variable.\n\nCheck structure of the data\n\n\nstr(sorghum)\n\n'data.frame':   100 obs. of  9 variables:\n $ y        : num  5 4.84 4.02 3.75 3.13 4.42 4.3 3.67 3.23 2.83 ...\n $ variety  : chr  \"1\" \"1\" \"1\" \"1\" ...\n $ Replicate: int  1 1 1 1 1 2 2 2 2 2 ...\n $ factweek : chr  \"1\" \"2\" \"3\" \"4\" ...\n $ factplot : int  1 1 1 1 1 2 2 2 2 2 ...\n $ varweek  : int  1 2 3 4 5 1 2 3 4 5 ...\n $ varblock : int  1 1 1 1 1 2 2 2 2 2 ...\n $ week     : int  1 2 3 4 5 1 2 3 4 5 ...\n $ block    : chr  \"1\" \"1\" \"1\" \"1\" ...\n\n\nIn this data, we have block, factplot, factweek as factor variables and y & week as numeric.\n\nInspect the independent variables\n\n\ntable(sorghum$variety, sorghum$block)\n\n   \n    1 2 3 4 5\n  1 5 5 5 5 5\n  2 5 5 5 5 5\n  3 5 5 5 5 5\n  4 5 5 5 5 5\n\n\nThe cross tabulation shows a equal number of variety treatments in each block.\n\nCheck the extent of missing data\n\n\ncolSums(is.na(sorghum))\n\n        y   variety Replicate  factweek  factplot   varweek  varblock      week \n        0         0         0         0         0         0         0         0 \n    block \n        0 \n\n\nNo missing values\n\nInspect the dependent variable\n\n\nggplot(data = sorghum, aes(y = y, x = factweek, fill = variety)) +\n  geom_boxplot() +  \n    theme_bw()\n\n\n\n\n\n\n\n\nLooks like variety ‚Äò1‚Äô has the lowest yield and showed drastic reduction in yield over weeks compared to other varieties. One last step before we fit model is to look at the distribution of response variable.\n\nhist(sorghum$y, main = NA, xlab = \"leaf area index\")\n\n\n\n\n\n\n\n\n\n\nFigure¬†13.1: Histogram of the dependent variable.\n\n\n\n\n\n\n13.1.2 Model Building\nLet‚Äôs fit the model first using lme() from the nlme package.\n\nlm1 &lt;- lme(y ~ variety + factweek + variety:factweek,\n           random = ~1|block/factplot,\n           data = sorghum,\n           na.action = na.exclude)\n\nThe model fitted above doesn‚Äôt account for the repeated measures effect. To account for the variation caused by repeated measurements, we can model the correlation among responses for a given subject which is plot (factor variable) in this case.\nBy adding this correlation structure, we are accounting for variation caused by repeated measurements over weeks for each plot. The AR1 structure assumes that data points collected more proximate are more correlated. Whereas, the compound symmetry structure assumes that correlation is equal for all time gaps. Here, we will fit model with both correlation structures and compare models to find out the best fit model.\nIn this analysis, time variable is week and it must be numeric.\n\ncs1 &lt;- corAR1(form = ~ week|block/factplot,  value = 0.2, fixed = FALSE)\ncs2 &lt;- corCompSymm(form = ~ week|block/factplot,  value = 0.2, fixed = FALSE)\n\nIn the code chunk above, we fitted two correlation structures including AR1 and compound symmetry matrices. Next we will update the model lm1, with these two matrices.\n\nlm2 &lt;- update(lm1, corr = cs1)\nlm3 &lt;- update(lm1, corr= cs2)\n\nNow let‚Äôs compare how model fitness differs among models with no correlation structure (lm1), with AR1 correlation structure (lm2), and with compound symmetry structure (lm3). We will compare these models by using anova() or by compare_performance() function from the performance library.\n\nanova\n\n\n\nanova(lm1, lm2, lm3)\n\n    Model df       AIC      BIC   logLik   Test  L.Ratio p-value\nlm1     1 23 18.837478 73.62409 13.58126                        \nlm2     2 24 -2.347391 54.82125 25.17370 1 vs 2 23.18487  &lt;.0001\nlm3     3 24 20.837478 78.00612 13.58126                        \n\n\nLet‚Äôs compare the models performance to select a model that fits better.\n\nresult &lt;- compare_performance(lm1, lm2, lm3)\n\nSome of the nested models seem to be identical and probably only vary in\n  their random effects.\n\nprint_md(result)\n\n\nComparison of Model Performance Indices\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nAIC (weights)\nAICc (weights)\nBIC (weights)\nR2 (cond.)\nR2 (marg.)\nICC\nRMSE\nSigma\n\n\n\n\nlm1\nlme\n-50.5 (&lt;.001)\n-36.0 (&lt;.001)\n9.4 (&lt;.001)\n0.99\n0.37\n0.98\n0.10\n0.13\n\n\nlm2\nlme\n-77.5 (&gt;.999)\n-61.5 (&gt;.999)\n-15.0 (&gt;.999)\n0.97\n0.41\n0.95\n0.15\n0.18\n\n\nlm3\nlme\n-48.5 (&lt;.001)\n-32.5 (&lt;.001)\n14.0 (&lt;.001)\n0.98\n0.37\n0.98\n0.11\n0.14\n\n\n\n\n\n\n\n\nWe prefer to chose model with lower AIC and BIC values. In this scenario, we will move forward with lm2 model containing AR1 structure.\nLet‚Äôs run a tidy() on lm2 model to look at the estimates for random and fixed effects.\n\ntidy(lm2)\n\nWarning in tidy.lme(lm2): ran_pars not yet implemented for multiple levels of\nnesting\n\n\n# A tibble: 20 √ó 7\n   effect term               estimate std.error    df statistic  p.value\n   &lt;chr&gt;  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 fixed  (Intercept)          4.24      0.291     64    14.6   5.44e-22\n 2 fixed  variety2             0.906     0.114     12     7.94  4.05e- 6\n 3 fixed  variety3             0.646     0.114     12     5.66  1.05e- 4\n 4 fixed  variety4             0.912     0.114     12     8.00  3.78e- 6\n 5 fixed  factweek2           -0.196     0.0571    64    -3.44  1.04e- 3\n 6 fixed  factweek3           -0.836     0.0755    64   -11.1   1.60e-16\n 7 fixed  factweek4           -1.16      0.0867    64   -13.3   4.00e-20\n 8 fixed  factweek5           -1.54      0.0943    64   -16.3   1.57e-24\n 9 fixed  variety2:factweek2   0.0280    0.0807    64     0.347 7.30e- 1\n10 fixed  variety3:factweek2   0.382     0.0807    64     4.73  1.26e- 5\n11 fixed  variety4:factweek2  -0.0140    0.0807    64    -0.174 8.63e- 1\n12 fixed  variety2:factweek3   0.282     0.107     64     2.64  1.03e- 2\n13 fixed  variety3:factweek3   0.662     0.107     64     6.20  4.55e- 8\n14 fixed  variety4:factweek3   0.388     0.107     64     3.64  5.55e- 4\n15 fixed  variety2:factweek4   0.228     0.123     64     1.86  6.77e- 2\n16 fixed  variety3:factweek4   0.744     0.123     64     6.06  7.86e- 8\n17 fixed  variety4:factweek4   0.39      0.123     64     3.18  2.28e- 3\n18 fixed  variety2:factweek5   0.402     0.133     64     3.01  3.70e- 3\n19 fixed  variety3:factweek5   0.672     0.133     64     5.04  4.11e- 6\n20 fixed  variety4:factweek5   0.222     0.133     64     1.66  1.01e- 1\n\n\n\n\n13.1.3 Check Model Assumptions\n\ncheck_model(lm2,  check = c('linearity','qq', 'reqq'), detrend=FALSE, alpha=0)\n\n\n\n\n\n\n\n\n\n\n13.1.4 Inference\n\nanova(lm2, type = \"marginal\")\n\n                 numDF denDF   F-value p-value\n(Intercept)          1    64 212.10509  &lt;.0001\nvariety              3    12  28.28895  &lt;.0001\nfactweek             4    64  74.79758  &lt;.0001\nvariety:factweek    12    64   7.03546  &lt;.0001\n\n\nThe ANOVA table suggests a significant effect of the variety, week, and variety x week interaction effect.\nWe can estimate the marginal means for variety and week effect and their interaction using emmeans() function.\n\nmean_1 &lt;- emmeans(lm2, ~ variety)\n\nNOTE: Results may be misleading due to involvement in interactions\n\nmean_1\n\n variety emmean    SE df lower.CL upper.CL\n 1         3.50 0.288  4     2.70     4.29\n 2         4.59 0.288  4     3.79     5.39\n 3         4.63 0.288  4     3.84     5.43\n 4         4.61 0.288  4     3.81     5.40\n\nResults are averaged over the levels of: factweek \nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\nmean_2 &lt;- emmeans(lm2, ~ variety*factweek)\nmean_2\n\n variety factweek emmean    SE df lower.CL upper.CL\n 1       1          4.24 0.291  4     3.43     5.05\n 2       1          5.15 0.291  4     4.34     5.96\n 3       1          4.89 0.291  4     4.08     5.70\n 4       1          5.15 0.291  4     4.35     5.96\n 1       2          4.05 0.291  4     3.24     4.85\n 2       2          4.98 0.291  4     4.17     5.79\n 3       2          5.07 0.291  4     4.27     5.88\n 4       2          4.94 0.291  4     4.14     5.75\n 1       3          3.41 0.291  4     2.60     4.21\n 2       3          4.59 0.291  4     3.79     5.40\n 3       3          4.71 0.291  4     3.91     5.52\n 4       3          4.71 0.291  4     3.90     5.51\n 1       4          3.09 0.291  4     2.28     3.89\n 2       4          4.22 0.291  4     3.41     5.03\n 3       4          4.48 0.291  4     3.67     5.28\n 4       4          4.39 0.291  4     3.58     5.20\n 1       5          2.70 0.291  4     1.89     3.51\n 2       5          4.01 0.291  4     3.20     4.82\n 3       5          4.02 0.291  4     3.21     4.83\n 4       5          3.83 0.291  4     3.03     4.64\n\nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\n\n\n\n\n\n\nTime variable\n\n\n\nHere is a quick step to make sure your fitting model correctly: make sure to have two time variables in your data one being numeric (e.g.¬†‚Äòday‚Äô as number) and other being factor/character(e.g.¬†‚Äòday_factor‚Äô as a factor/character). Where, numeric variable is used for fitting correlation matrix and factor/character variable used in model statement to evaluate the time variable effect on response variable.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Repeated Measures</span>"
    ]
  },
  {
    "objectID": "chapters/repeated-measures.html#split-plot-repeated-measures",
    "href": "chapters/repeated-measures.html#split-plot-repeated-measures",
    "title": "12¬† Repeated measures mixed models",
    "section": "13.2 Split Plot Repeated Measures",
    "text": "13.2 Split Plot Repeated Measures\nRecall, we have evaluated split plot design Chapter 7. In this example we will use the same methodology used in Chapter 5 and update it with repeated measures component.\nNext, let‚Äôs load an alfalfa intercropping data set. This was from an irrigation and intercropping experiment was conducted in southern Idaho. Irrigation is the main plot, intercropping is the split plot, and the in-season alfalfa cutting (‚Äúcutting‚Äù) is the unit for repeated measures.\n\nalfalfa &lt;- read.csv(here::here(\"data/alfalfa_intercropping.csv\"))\n\nThis example contains yield data in a split-plot design. The yield data was collected repeatedly from the same Reps over 5 Sample_times. In this data set, we have:\n\nTable of variables in the data set\n\n\ncutting\ntime points for data collection\n\n\nirrigation\nMain plot, 2 levels\n\n\nplot\nexperimental unit\n\n\nblock\nreplication unit\n\n\nintercrop\nSplit plot, 3 levels\n\n\nyield\ncrop yield\n\n\nrow\nspatial position by row\n\n\ncol\nspatial position by column\n\n\n\n\n13.2.1 Data Integrity Checks\n\nCheck structure of the data\n\nFirst, we need to look at the class of variables in the data set.\n\nstr(alfalfa)\n\n'data.frame':   240 obs. of  8 variables:\n $ cutting   : chr  \"First\" \"Second\" \"Third\" \"First\" ...\n $ irrigation: chr  \"Full\" \"Full\" \"Full\" \"Full\" ...\n $ plot      : int  1101 1101 1101 1102 1102 1102 1103 1103 1103 1104 ...\n $ block     : int  1 1 1 1 1 1 1 1 1 1 ...\n $ intercrop : chr  \"50A+50O\" \"50A+50O\" \"50A+50O\" \"75A+25O\" ...\n $ yield     : num  221 355 365 289 606 ...\n $ row       : int  1 1 1 1 1 1 1 1 1 1 ...\n $ col       : int  1 1 1 2 2 2 3 3 3 4 ...\n\n\nWe will now convert the fertilizer and rep to factors. In addition, we need to create a new factor variable (sample_time1) to analyze the time effect.\n\n\nFor lme(), independent variables in a character/factor form works fine. But, for mmrm() independent variables must be a factor. Thus, for consistency, we will be using independent variables in factor class.\n\nalfalfa &lt;- alfalfa |&gt; \n  mutate(cut_num = as.numeric(as.factor(cutting))) |&gt; \n  mutate_at(c(\"cutting\", \"irrigation\", \"plot\", \"block\"), as.factor)\n\nTo fit the model, we first need to convert Variety, Fertilizer, and Sample_time to factors. In addition, we need to create a variable for each subject which is plot in this case and contains a unique value for each plot. The plot variable is needed to model the variation in each plot over the sampling time. The plot will be used as a subject with repeated measures. The subject variable can be factor or numeric but the time (it could be year, or sample_time) has to be a factor.\n\nInspect the independent variables\n\n\ntable(alfalfa$intercrop, alfalfa$irrigation) \n\n            \n             Deficit Full\n  100A            12   12\n  50A+50F         12   12\n  50A+50F_AR      12   12\n  50A+50M         12   12\n  50A+50M_AR      12   12\n  50A+50O         12   12\n  50A+50O_AR      12   12\n  75A+25F         12   12\n  75A+25M         12   12\n  75A+25O         12   12\n\n\nLooks like a balanced design with 2 irrigation treatments and 10 intercropping treatments.\n\nCheck the extent of missing data\n\n\ncolSums(is.na(alfalfa))\n\n   cutting irrigation       plot      block  intercrop      yield        row \n         0          0          0          0          0          3          0 \n       col    cut_num \n         0          0 \n\n\n\nInspect the dependent variable\n\nBefore fitting a model, let‚Äôs check the distribution of the response variable.\n\n\n\n\n\n\n\n\n\nFigure¬†13.2: Histogram of the dependent variable\n\n\n\n\n\nhist(Yield$Yield, xlab = \"yield\", main = NA)\n\n\n\n13.2.2 Model fit\nThis data can be analyzed either using nlme or mmrm.\nLet‚Äôs say we want to fit a model using AR1 structure as shown in the RCBD repeated measures example. Previously, we used lme() from nlme package to fit the model. In this example, along with nlme() we will also mmrm() function from the mmrm package. In addition, instead of summary() function we will use tidy() function from the broom.mixed package to look at estimates of mixed and random effects. This will generate a tidy workflow in particular by providing standardized verbs that provide information on estimates, standard errors, confidence intervals, etc.\n\ncorr_str1 = corAR1(form = ~ cut_num|block/irrigation/intercrop/plot, value = 0.2, fixed = FALSE)\n\nfit1 &lt;- lme(yield ~ irrigation*intercrop*cutting,\n                random = ~ 1|block/irrigation/intercrop/plot,\n                corr= corr_str1,\n                data = alfalfa, na.action= na.exclude)\n\n\n\n13.2.3 Check Model Assumptions\nWe will use check_model() from performance package to evaluate the model fitness of model fitted using nlme (mod1). However, the mmrm() model class doesn‚Äôt work with performance package, so we will evaluate the model diagnostics by plotting the residuals using base R functions.\n\ncheck_model(fit1, check = c('linearity', 'qq'), detrend=FALSE, alpha = 0)\n\n\n\n\n\n\n\n\nThese diagnostic plots look fine. The linearity and homogeneity of variance plots show no trend. The normal Q-Q plots for the overall residuals and for the random effects fall on a straight line so we can be satisfied with that.\n\n\n13.2.4 Inference\n\nanova(fit1, type = \"marginal\")\n\n                             numDF denDF   F-value p-value\n(Intercept)                      1   115 157.67647  &lt;.0001\nirrigation                       1     3   0.17409  0.7046\nintercrop                        9    54   3.25590  0.0031\ncutting                          2   115   4.24957  0.0166\nirrigation:intercrop             9    54   0.68879  0.7158\nirrigation:cutting               2   115   0.35554  0.7016\nintercrop:cutting               18   115   1.22854  0.2505\nirrigation:intercrop:cutting    18   115   0.77391  0.7263\n\n\nNext, we can estimate marginal means and confidence intervals at different levels of the independent variables using emmeans().\n\nemmeans(fit1,~ cutting)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n cutting emmean   SE df lower.CL upper.CL\n First      447 11.3  3      411      483\n Second     380 11.4  3      344      416\n Third      362 11.5  3      325      398\n\nResults are averaged over the levels of: irrigation, intercrop \nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\nemmeans(fit1,~ intercrop)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n intercrop  emmean   SE df lower.CL upper.CL\n 100A          540 19.0  3      480      601\n 50A+50F       432 19.5  3      370      494\n 50A+50F_AR    321 19.0  3      261      381\n 50A+50M       432 19.0  3      371      492\n 50A+50M_AR    267 19.0  3      207      328\n 50A+50O       396 19.5  3      334      458\n 50A+50O_AR    285 19.0  3      225      345\n 75A+25F       447 19.5  3      385      509\n 75A+25M       434 19.0  3      374      494\n 75A+25O       407 19.0  3      347      467\n\nResults are averaged over the levels of: irrigation, cutting \nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\n\n\nTo explore more about contrasts and emmeans please refer to Chapter 13.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Repeated Measures</span>"
    ]
  },
  {
    "objectID": "chapters/repeated-measures.html#ssp-rm",
    "href": "chapters/repeated-measures.html#ssp-rm",
    "title": "12¬† Repeated measures mixed models",
    "section": "13.3 Split-split Plot Repeated Measures",
    "text": "13.3 Split-split Plot Repeated Measures\nRecall, we have evaluated the split-split experiment design in Chapter 8, where we had a one factor in main-plot, other in subplot and the third factor in sub-subplot. In this example we will be adding a repeated measures component to the split-split plot design.\n\nphos &lt;- read.csv(here::here(\"data\", \"split_split_repeated.csv\"))\n\n\n\n\nplot\nexperimental unit\n\n\nblock\nreplication unit\n\n\nPtrt\nMain plot, 2 levels\n\n\nInoc\nSplit plot, 2 levels\n\n\nCv\nSplit-split plot, 5 levels\n\n\ntime\ntime points for data collection\n\n\nP_leaf\nleaf phosphorous content\n\n\n\n\n13.3.1 Data Integrity Checks\n\nCheck structure of the data\n\n\nstr(phos)\n\n'data.frame':   240 obs. of  7 variables:\n $ plot  : int  1 1 1 2 2 2 3 3 3 4 ...\n $ bloc  : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Ptrt  : chr  \"high\" \"high\" \"high\" \"high\" ...\n $ Inoc  : chr  \"none\" \"none\" \"none\" \"none\" ...\n $ Cv    : chr  \"Louise\" \"Louise\" \"Louise\" \"Blanca Grande\" ...\n $ time  : chr  \"PT1\" \"PT2\" \"PT3\" \"PT1\" ...\n $ P_leaf: num  3154 2331 247 3016 2160 ...\n\n\nWe need to have two variables for time one being a factor other being a numeric.\n\nphos1 &lt;- phos %&gt;%   \n  mutate(\n    time = as.factor(time), \n    time1 = as.numeric(time),\n    rep = as.character(bloc),\n    plot = as.character(plot)) \n\n\nInspect the independent variables\n\n\ntable(phos1$Cv, phos1$Ptrt, phos1$Inoc) \n\n, ,  = myco\n\n               \n                high low\n  Alpowa          12  12\n  Blanca Grande   12  12\n  Louise          12  12\n  Otis            12  12\n  Walworth        12  12\n\n, ,  = none\n\n               \n                high low\n  Alpowa          12  12\n  Blanca Grande   12  12\n  Louise          12  12\n  Otis            12  12\n  Walworth        12  12\n\n\nLooks like a well balanced design with 2 variety treatments and 3 fertilizer treatments.\n\nCheck the extent of missing data\n\n\ncolSums(is.na(phos1))\n\n  plot   bloc   Ptrt   Inoc     Cv   time P_leaf  time1    rep \n     0      0      0      0      0      0      0      0      0 \n\n\nNo missing values in data.\n\nInspect the dependent variable\n\nBefore fitting a model, let‚Äôs check the distribution of the response variable.\n\n\n\n\n\n\n\n\n\nFigure¬†13.3: Histogram of the dependent variable.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†13.4: Boxplot of P concentration by stage\n\n\n\n\n\nhist(phos1$P_leaf, main = NA, xlab = \"P leaf\")\nboxplot(P_leaf ~ time, data = phos1, main = NA)\n\n\n\n\n\n\n\ndistribution of dependent variables\n\n\n\nHere note that we observed uneven distribution of response variable with a bimodal distribution and a noticeable gap in the 500 to 1500 range. Given this odd distribution, it may be tempting to consider a transformation in order to attempt to impose normality. It‚Äôs important to remember that the assumption of normality applies to the residuals, not the raw data. Plotting the data is for checking the data looks as expected, a judgement that requires some knowledge of the experiment (this was Julia Piaskowski‚Äôs PhD research). In this case, the time points, PT1 and PT2, reflect early wheat growth stages (tillering and jointing, respectively), and the final time point is senescent leaf tissue at grain maturity. At that physiological stage, it is normal for phosphorus leaf concentration to be much lower. Since the data look as expected, we will proceed with a general linear model and evaluate the residuals from the model fitting process when deciding if a non-normal distribution is appropriate for the data.\n\n\n\n\n13.3.2 Model fit\n\ncorr_str1 = corCompSymm(form = ~ time1|rep/Ptrt/Inoc/plot, value = 0.2, fixed = FALSE)\n\npfit1 &lt;- lme(P_leaf ~ time*Ptrt*Inoc*Cv,\n                random = ~ 1|rep/Ptrt/Inoc/plot,\n                #corr = corr_str1,\n                data = phos1, na.action= na.exclude)\n\n\n\n13.3.3 Check model assumptions\n\ncheck_model(pfit1, check = c('linearity', 'qq'), detrend=FALSE, alpha =0)\n\n\n\n\n\n\n\n\nThis model fit a first glance is not ideal, but that LOESS line is trying to model a space where there are no data (between 500 and 1500 ppm P leaf concentration), so that can introduce artifacts. Performance does have an option for testing for heteroscedascity:\n\ncheck_heteroscedasticity(pfit1)\n\nWarning in deviance.lme(x, ...): deviance undefined for REML fit\n\n\nOK: Error variance appears to be homoscedastic (p &gt; .999).\n\n\nThese results do confirm our suspicions that the residuals were not as heteroscedastic as they first appeared. However, the boxplot indicated a a difference in variance for each time time point. This addressed in the chapter on variance components.\n\n\n13.3.4 Inference\n\nanova(pfit1, type = \"marginal\")\n\n                  numDF denDF   F-value p-value\n(Intercept)           1   120 1458.2383  &lt;.0001\ntime                  2   120  518.3986  &lt;.0001\nPtrt                  1     3    3.3677  0.1638\nInoc                  1     6    1.7697  0.2317\nCv                    4    48    7.6028  0.0001\ntime:Ptrt             2   120    0.6789  0.5091\ntime:Inoc             2   120    1.9809  0.1424\nPtrt:Inoc             1     6    2.4919  0.1655\ntime:Cv               8   120    2.4111  0.0189\nPtrt:Cv               4    48    0.5082  0.7299\nInoc:Cv               4    48    2.1349  0.0909\ntime:Ptrt:Inoc        2   120    0.8409  0.4339\ntime:Ptrt:Cv          8   120    0.2223  0.9863\ntime:Inoc:Cv          8   120    0.9461  0.4816\nPtrt:Inoc:Cv          4    48    0.4761  0.7530\ntime:Ptrt:Inoc:Cv     8   120    0.3886  0.9249\n\n\n\nemmeans(pfit1, ~ Inoc|Cv)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\nCv = Alpowa:\n Inoc emmean SE df lower.CL upper.CL\n myco   1832 53  3     1663     2000\n none   1904 53  3     1736     2073\n\nCv = Blanca Grande:\n Inoc emmean SE df lower.CL upper.CL\n myco   1919 53  3     1750     2088\n none   1899 53  3     1730     2068\n\nCv = Louise:\n Inoc emmean SE df lower.CL upper.CL\n myco   1876 53  3     1707     2045\n none   1898 53  3     1730     2067\n\nCv = Otis:\n Inoc emmean SE df lower.CL upper.CL\n myco   1855 53  3     1686     2023\n none   1956 53  3     1787     2124\n\nCv = Walworth:\n Inoc emmean SE df lower.CL upper.CL\n myco   1667 53  3     1498     1836\n none   1737 53  3     1568     1906\n\nResults are averaged over the levels of: time, Ptrt \nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\nemmeans(pfit1, ~ time|Cv)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\nCv = Alpowa:\n time emmean   SE df lower.CL upper.CL\n PT1    3201 56.4  3  3021.89     3381\n PT2    2225 56.4  3  2045.20     2404\n PT3     178 56.4  3    -1.14      358\n\nCv = Blanca Grande:\n time emmean   SE df lower.CL upper.CL\n PT1    3183 56.4  3  3003.83     3363\n PT2    2334 56.4  3  2154.77     2513\n PT3     210 56.4  3    30.28      389\n\nCv = Louise:\n time emmean   SE df lower.CL upper.CL\n PT1    3121 56.4  3  2941.69     3300\n PT2    2366 56.4  3  2186.88     2546\n PT3     174 56.4  3    -5.11      354\n\nCv = Otis:\n time emmean   SE df lower.CL upper.CL\n PT1    3228 56.4  3  3048.98     3408\n PT2    2253 56.4  3  2073.98     2433\n PT3     234 56.4  3    54.19      413\n\nCv = Walworth:\n time emmean   SE df lower.CL upper.CL\n PT1    2744 56.4  3  2564.63     2923\n PT2    2170 56.4  3  1990.22     2349\n PT3     193 56.4  3    13.21      372\n\nResults are averaged over the levels of: Ptrt, Inoc \nDegrees-of-freedom method: containment \nConfidence level used: 0.95",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Repeated Measures</span>"
    ]
  },
  {
    "objectID": "chapters/means-and-contrasts.html",
    "href": "chapters/means-and-contrasts.html",
    "title": "13¬† Marginal Means & Contrasts",
    "section": "",
    "text": "13.1 Background\nTo start off with, we need to define estimated marginal means. Estimated marginal means are defined as marginal means of a variable across all levels of other variables in a model, essentially giving a ‚Äúpopulation-level‚Äù average.\nThe emmeans package is one of the most commonly used package in R in determine marginal means. This package provides methods for obtaining marginal means (also known as least-squares means) for factor combinations in a variety of models. The emmeans package is one of several alternatives to facilitate post-hoc methods application and contrast analysis. It is a relatively recent replacement for the lsmeans package that some R users may be familiar with. It is intended for use with a wide variety of ANOVA models, including repeated measures and nested designs (mixed models). This is a flexible package that comes with a set of detailed vignettes and works with a lot of different model objects.\nIn this chapter, we will demonstrate the extended use of the emmeans package to calculate estimated marginal means and contrasts.\nTo demonstrate the use of the emmeans package. We will pull the model from split plot lesson (Chapter 7), where we evaluated the effect of Nitrogen and Variety on Oat yield. This data contains 6 blocks, 3 main plots (Variety) and 4 subplots (Nitrogen). The primary outcome variable was oat yield. To read more about the experiment layout details please read RCBD split-plot section in Chapter 7.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Marginal Means and Contrasts</span>"
    ]
  },
  {
    "objectID": "chapters/means-and-contrasts.html#background",
    "href": "chapters/means-and-contrasts.html#background",
    "title": "13¬† Marginal Means & Contrasts",
    "section": "",
    "text": "Marginal means using lmer and nlme\n\n\n\nFor demonstration of the emmeans package, we are fitting model with nlme package. Please note that code below calculating marginal means works for both lmer() and nlme() models.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Marginal Means and Contrasts</span>"
    ]
  },
  {
    "objectID": "chapters/means-and-contrasts.html#analysis-examples",
    "href": "chapters/means-and-contrasts.html#analysis-examples",
    "title": "13¬† Marginal Means & Contrasts",
    "section": "13.2 Analysis Examples",
    "text": "13.2 Analysis Examples\nLet‚Äôs start this example by loading the required libraries for fitting linear mixed models using nlme package.\n\nlibrary(nlme); library(performance); library(emmeans)\nlibrary(dplyr); library(broom.mixed); library(multcompView)\nlibrary(multcomp); library(ggplot2)\n\n\n13.2.1 Import data\nLet‚Äôs import oats data from the MASS package.\n\ndata1 &lt;- MASS::oats\n\n\n\nTo read more about data and model fitting explanation please refer to Chapter 7.\n\n\n13.2.2 Model fitting\n\nmodel1 &lt;- lme(Y ~  V + N + V:N ,\n                  random = ~1|B/V,\n                  data = data1, \n                  na.action = na.exclude)\ntidy(model1)\n\nWarning in tidy.lme(model1): ran_pars not yet implemented for multiple levels\nof nesting\n\n\n# A tibble: 12 √ó 7\n   effect term                estimate std.error    df statistic  p.value\n   &lt;chr&gt;  &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 fixed  (Intercept)           80          9.11    45    8.78   2.56e-11\n 2 fixed  VMarvellous            6.67       9.72    10    0.686  5.08e- 1\n 3 fixed  VVictory              -8.5        9.72    10   -0.875  4.02e- 1\n 4 fixed  N0.2cwt               18.5        7.68    45    2.41   2.02e- 2\n 5 fixed  N0.4cwt               34.7        7.68    45    4.51   4.58e- 5\n 6 fixed  N0.6cwt               44.8        7.68    45    5.84   5.48e- 7\n 7 fixed  VMarvellous:N0.2cwt    3.33      10.9     45    0.307  7.60e- 1\n 8 fixed  VVictory:N0.2cwt      -0.333     10.9     45   -0.0307 9.76e- 1\n 9 fixed  VMarvellous:N0.4cwt   -4.17      10.9     45   -0.383  7.03e- 1\n10 fixed  VVictory:N0.4cwt       4.67      10.9     45    0.430  6.70e- 1\n11 fixed  VMarvellous:N0.6cwt   -4.67      10.9     45   -0.430  6.70e- 1\n12 fixed  VVictory:N0.6cwt       2.17      10.9     45    0.199  8.43e- 1\n\n\n\n\n13.2.3 Check Model Assumptions\n\ncheck_model(model1, check = c('qq', 'linearity'), detrend=FALSE, alpha = 0)\n\n\n\n\n\n\n\n\nResiduals look good with a small hump in middle and normality curve looks reasonable.\n\n\n13.2.4 Model Inference\n\nanova(model1, type = \"marginal\")\n\n            numDF denDF  F-value p-value\n(Intercept)     1    45 77.16732  &lt;.0001\nV               2    10  1.22454  0.3344\nN               3    45 13.02273  &lt;.0001\nV:N             6    45  0.30282  0.9322\n\n\nThe analysis of variance showed a significant N effect and no effect of V and V x N interaction effect on oat yield.\n\n\n13.2.5 Estimated Marginal Means\nNow that we have fitted a linear mixed model (model1) and it meets the model assumption. Let‚Äôs use the emmeans() function to obtain estimated marginal means for main (variety and nitrogen) and interaction (variety x nitrogen) effects.\n\n13.2.5.1 Main effects\nThe main effects in model1 were V and N. we will start by looking at the estimated means of V and N main effects.\n\nm1 &lt;- emmeans(model1, ~V, level = 0.95)\n\nNOTE: Results may be misleading due to involvement in interactions\n\nm1\n\n V           emmean  SE df lower.CL upper.CL\n Golden.rain  104.5 7.8  5     84.5      125\n Marvellous   109.8 7.8  5     89.7      130\n Victory       97.6 7.8  5     77.6      118\n\nResults are averaged over the levels of: N \nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\n\nm2 &lt;- emmeans(model1, ~N)\n\nNOTE: Results may be misleading due to involvement in interactions\n\nm2\n\n N      emmean   SE df lower.CL upper.CL\n 0.0cwt   79.4 7.17  5     60.9     97.8\n 0.2cwt   98.9 7.17  5     80.4    117.3\n 0.4cwt  114.2 7.17  5     95.8    132.7\n 0.6cwt  123.4 7.17  5    104.9    141.8\n\nResults are averaged over the levels of: V \nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\nMake sure to read and interpret marginal means carefully. Here, when we calculated marginal means for main effects of V and N, these were averaged over the levels of other factor in experiment. For example, estimated means for each variety were averaged over it‚Äôs N treatments, respectively.\n\n\n13.2.5.2 Interaction effects\nNow let‚Äôs evaluate the marginal means for the interaction effect of V and N. These can be calculated either using V*N or V|N.\n\nm3 &lt;- emmeans(model1, ~V*N)\nm3\n\n V           N      emmean   SE df lower.CL upper.CL\n Golden.rain 0.0cwt   80.0 9.11  5     56.6    103.4\n Marvellous  0.0cwt   86.7 9.11  5     63.3    110.1\n Victory     0.0cwt   71.5 9.11  5     48.1     94.9\n Golden.rain 0.2cwt   98.5 9.11  5     75.1    121.9\n Marvellous  0.2cwt  108.5 9.11  5     85.1    131.9\n Victory     0.2cwt   89.7 9.11  5     66.3    113.1\n Golden.rain 0.4cwt  114.7 9.11  5     91.3    138.1\n Marvellous  0.4cwt  117.2 9.11  5     93.8    140.6\n Victory     0.4cwt  110.8 9.11  5     87.4    134.2\n Golden.rain 0.6cwt  124.8 9.11  5    101.4    148.2\n Marvellous  0.6cwt  126.8 9.11  5    103.4    150.2\n Victory     0.6cwt  118.5 9.11  5     95.1    141.9\n\nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\n\nm4 &lt;- emmeans(model1, ~V|N)\nm4\n\nN = 0.0cwt:\n V           emmean   SE df lower.CL upper.CL\n Golden.rain   80.0 9.11  5     56.6    103.4\n Marvellous    86.7 9.11  5     63.3    110.1\n Victory       71.5 9.11  5     48.1     94.9\n\nN = 0.2cwt:\n V           emmean   SE df lower.CL upper.CL\n Golden.rain   98.5 9.11  5     75.1    121.9\n Marvellous   108.5 9.11  5     85.1    131.9\n Victory       89.7 9.11  5     66.3    113.1\n\nN = 0.4cwt:\n V           emmean   SE df lower.CL upper.CL\n Golden.rain  114.7 9.11  5     91.3    138.1\n Marvellous   117.2 9.11  5     93.8    140.6\n Victory      110.8 9.11  5     87.4    134.2\n\nN = 0.6cwt:\n V           emmean   SE df lower.CL upper.CL\n Golden.rain  124.8 9.11  5    101.4    148.2\n Marvellous   126.8 9.11  5    103.4    150.2\n Victory      118.5 9.11  5     95.1    141.9\n\nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\nThe marginal means (m3 and m4) gives the results for V x N interaction effect but the calculated differently.\n\n\n\n\n\n\nNote\n\n\n\nHere, note that we estimated marginal means in two ways: V*N estimate the marginal means for each combination and when we do pairwise compariosn on these, all combinations are compared to one another simultaneously.\nV|N estimates the marginal means for each V at a given level of N. When we do pairwise comparison on this, V groups are compared among each other at a given level of N, not across all levels of N.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Marginal Means and Contrasts</span>"
    ]
  },
  {
    "objectID": "chapters/means-and-contrasts.html#contrasts-using-emmeans",
    "href": "chapters/means-and-contrasts.html#contrasts-using-emmeans",
    "title": "13¬† Marginal Means & Contrasts",
    "section": "13.3 Contrasts using emmeans",
    "text": "13.3 Contrasts using emmeans\nThe pairs() function from emmeans package can be used to evaluate the pairwise comparison among treatment objects. The emmean object (m1, m2) will be passed through pairs() function which will provide a p-value adjustment equivalent to the Tukey test.\n\npairs(m1, adjust = \"tukey\")\n\n contrast                 estimate   SE df t.ratio p.value\n Golden.rain - Marvellous    -5.29 7.08 10  -0.748  0.7419\n Golden.rain - Victory        6.88 7.08 10   0.971  0.6104\n Marvellous - Victory        12.17 7.08 10   1.719  0.2458\n\nResults are averaged over the levels of: N \nDegrees-of-freedom method: containment \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\n\npairs(m2)\n\n contrast        estimate   SE df t.ratio p.value\n 0.0cwt - 0.2cwt   -19.50 4.44 45  -4.396  0.0004\n 0.0cwt - 0.4cwt   -34.83 4.44 45  -7.853  &lt;.0001\n 0.0cwt - 0.6cwt   -44.00 4.44 45  -9.919  &lt;.0001\n 0.2cwt - 0.4cwt   -15.33 4.44 45  -3.457  0.0064\n 0.2cwt - 0.6cwt   -24.50 4.44 45  -5.523  &lt;.0001\n 0.4cwt - 0.6cwt    -9.17 4.44 45  -2.067  0.1797\n\nResults are averaged over the levels of: V \nDegrees-of-freedom method: containment \nP value adjustment: tukey method for comparing a family of 4 estimates \n\n\nHere if we look at the results from code chunk above, it‚Äôs easy to interpret results from pairs() function in case of variety comparison because there were only 3 groups. But it‚Äôs little confusing in case of Nitrogen treatments where we had 4 groups. We can further simplify it by using custom contrasts.\n\n\n\n\n\n\npairs()\n\n\n\nThis function is conducting all pairwise tests for the variable specified. The default p-value adjustment in pairs() function is ‚Äútukey‚Äù; other options include ‚Äúholm‚Äù, ‚Äúhochberg‚Äù, ‚ÄúBH‚Äù, ‚ÄúBY‚Äù, and ‚Äúnone‚Äù. If you are conducting this on a variable with many levels, this adjustment can be severe, resulting in very few statistically significant results. To avoid this, consider other tests such ‚Äòcompare to a control‚Äô or custom contrasts.\n\n\n\n13.3.1 Custom contrasts\nFirst, run emmean object ‚Äòm2‚Äô for nitrogen treatments.\n\nm2\n\n N      emmean   SE df lower.CL upper.CL\n 0.0cwt   79.4 7.17  5     60.9     97.8\n 0.2cwt   98.9 7.17  5     80.4    117.3\n 0.4cwt  114.2 7.17  5     95.8    132.7\n 0.6cwt  123.4 7.17  5    104.9    141.8\n\nResults are averaged over the levels of: V \nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\nNow, let‚Äôs create a vector for each nitrogen treatment in the same order as presented in output from m2.\n\nA1 = c(1, 0, 0, 0)\nA2 = c(0, 1, 0, 0)\nA3 = c(0, 0, 1, 0)\nA4 = c(0, 0, 0, 1)\n\nThese vectors (A1, A2, A3, A4) represent each Nitrogen treatment in an order as presented in m2 emmeans object. A1, A2, and A3, A4 vectors represents 0.0 cwt, 0.2 cwt, 0.4 cwt, and 0.6 cwt treatments, respectively.\nNext step is to create a custom contrasts for comparing ‚Äò0.0cwt‚Äô (A1) treatment to ‚Äò0.2cwt‚Äô (A2), ‚Äò0.4cwt‚Äô (A3), and ‚Äò0.6cwt‚Äô (A4) treatments. This can be evaluated as shown below:\n\ncontrast(m2, method = list(A1 - A2) )\n\n contrast       estimate   SE df t.ratio p.value\n c(1, -1, 0, 0)    -19.5 4.44 45  -4.396  0.0001\n\nResults are averaged over the levels of: V \nDegrees-of-freedom method: containment \n\ncontrast(m2, method = list(A1 - A3) )\n\n contrast       estimate   SE df t.ratio p.value\n c(1, 0, -1, 0)    -34.8 4.44 45  -7.853  &lt;.0001\n\nResults are averaged over the levels of: V \nDegrees-of-freedom method: containment \n\ncontrast(m2, method = list(A1 - A4) )\n\n contrast       estimate   SE df t.ratio p.value\n c(1, 0, 0, -1)      -44 4.44 45  -9.919  &lt;.0001\n\nResults are averaged over the levels of: V \nDegrees-of-freedom method: containment \n\n\nHere the output shows the difference in mean yield between control and 3 N treatments. The results shows that yield was significantly higher N treatments compared to the control (0.0 cwt) irrespective of the oat variety.\n\n\n\n\n\n\ncontrast() vs pairs()\n\n\n\nUsing custom contrast() is strongly recommended instead of pairs() when you are comparing multiple treatment groups (&gt;5).\n\n\nIn addition to these conventional custom contrast options, emmeans package allows to do some easy contrasts such as comparing all the treatments to control or estimating custom contrast to compare one treatment level to all other groups.\nWe will start with running ‚Äòm1‚Äô emmeans object\n\nm1\n\n V           emmean  SE df lower.CL upper.CL\n Golden.rain  104.5 7.8  5     84.5      125\n Marvellous   109.8 7.8  5     89.7      130\n Victory       97.6 7.8  5     77.6      118\n\nResults are averaged over the levels of: N \nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\nBased on m1 object, let‚Äôs assume we want to compare all varieties with Marvellous. The code shown below estimates custom contrast for treatments vs control and here we are referring Marvellous variety as a control.\n\ncontrast(m1, \"trt.vs.ctrl\", ref = \"Marvellous\")\n\n contrast                 estimate   SE df t.ratio p.value\n Golden.rain - Marvellous    -5.29 7.08 10  -0.748  0.6833\n Victory - Marvellous       -12.17 7.08 10  -1.719  0.2045\n\nResults are averaged over the levels of: N \nDegrees-of-freedom method: containment \nP value adjustment: dunnettx method for 2 tests \n\n\nOr we can refer control group by the appearance order of varieties in m1. In m1, order of Golden rain, Marvellous, and Victory are ordered as 1, 2, and 3 respenctively.\nLet‚Äôs suppose we want to compare Golden rain to other varieties, we can do this by either trt.vs.ctrl1' code, ortrt.vs.ctrlk` and referring to group 1. Both code options will generate same results.\n\ncontrast(m1, \"trt.vs.ctrl1\")\n\n contrast                 estimate   SE df t.ratio p.value\n Marvellous - Golden.rain     5.29 7.08 10   0.748  0.6833\n Victory - Golden.rain       -6.88 7.08 10  -0.971  0.5476\n\nResults are averaged over the levels of: N \nDegrees-of-freedom method: containment \nP value adjustment: dunnettx method for 2 tests \n\ncontrast(m1, \"trt.vs.ctrlk\", ref = 1)\n\n contrast                 estimate   SE df t.ratio p.value\n Marvellous - Golden.rain     5.29 7.08 10   0.748  0.6833\n Victory - Golden.rain       -6.88 7.08 10  -0.971  0.5476\n\nResults are averaged over the levels of: N \nDegrees-of-freedom method: containment \nP value adjustment: dunnettx method for 2 tests \n\n\nWe can further customize the contrasts by excluding specific treatment groups from the comparison.\nwe can demonstrate this by running m2 object first.\n\nm2\n\n N      emmean   SE df lower.CL upper.CL\n 0.0cwt   79.4 7.17  5     60.9     97.8\n 0.2cwt   98.9 7.17  5     80.4    117.3\n 0.4cwt  114.2 7.17  5     95.8    132.7\n 0.6cwt  123.4 7.17  5    104.9    141.8\n\nResults are averaged over the levels of: V \nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\nWe can conduct a pairwise comparison by excluding 0.2 cwt.\n\npairs(m2, exclude = 2)\n\n contrast        estimate   SE df t.ratio p.value\n 0.0cwt - 0.4cwt   -34.83 4.44 45  -7.853  &lt;.0001\n 0.0cwt - 0.6cwt   -44.00 4.44 45  -9.919  &lt;.0001\n 0.4cwt - 0.6cwt    -9.17 4.44 45  -2.067  0.1084\n\nResults are averaged over the levels of: V \nDegrees-of-freedom method: containment \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nOr we can also evaluate pairwise comparison\n\ncontrast(m2, \"consec\")\n\n contrast        estimate   SE df t.ratio p.value\n 0.2cwt - 0.0cwt    19.50 4.44 45   4.396  0.0002\n 0.4cwt - 0.2cwt    15.33 4.44 45   3.457  0.0036\n 0.6cwt - 0.4cwt     9.17 4.44 45   2.067  0.1153\n\nResults are averaged over the levels of: V \nDegrees-of-freedom method: containment \nP value adjustment: mvt method for 3 tests",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Marginal Means and Contrasts</span>"
    ]
  },
  {
    "objectID": "chapters/means-and-contrasts.html#compact-letter-displays",
    "href": "chapters/means-and-contrasts.html#compact-letter-displays",
    "title": "13¬† Marginal Means & Contrasts",
    "section": "13.4 Compact letter displays",
    "text": "13.4 Compact letter displays\nCompact letter displays (CLDs) are a popular way to display multiple comparisons when there are more than few group means to compare. However, they are problematic as they are more prone to misinterpretation. The R package multcompView (Graves et al., 2019) provides an implementation of CLDs creating a display where any two means associated with same symbol are not statistically different.\nThe cld() function from the multcomp package is used to implement CLDs in the form of symbols or letters. The emmeans package provides a emmGrid objects for cld() method.\nLet‚Äôs start evaluating CLDs for main effects. We will use emmean objects m1 (for variety) and m2 (for nitrogen) for this. In the output below, groups sharing a letter in the \\(.group\\) are not statistically different from each other.\n\ncld(m1, alpha=0.05, Letters=letters)\n\n V           emmean  SE df lower.CL upper.CL .group\n Victory       97.6 7.8  5     77.6      118  a    \n Golden.rain  104.5 7.8  5     84.5      125  a    \n Marvellous   109.8 7.8  5     89.7      130  a    \n\nResults are averaged over the levels of: N \nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \nP value adjustment: tukey method for comparing a family of 3 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\n\ncld(m2, alpha=0.05, Letters=letters)\n\n N      emmean   SE df lower.CL upper.CL .group\n 0.0cwt   79.4 7.17  5     60.9     97.8  a    \n 0.2cwt   98.9 7.17  5     80.4    117.3   b   \n 0.4cwt  114.2 7.17  5     95.8    132.7    c  \n 0.6cwt  123.4 7.17  5    104.9    141.8    c  \n\nResults are averaged over the levels of: V \nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \nP value adjustment: tukey method for comparing a family of 4 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nLet‚Äôs have a look at the CLDs for the interaction effect:\n\ncld3 &lt;- cld(m3, alpha=0.05, Letters=letters)\ncld3\n\n V           N      emmean   SE df lower.CL upper.CL .group    \n Victory     0.0cwt   71.5 9.11  5     48.1     94.9  a        \n Golden.rain 0.0cwt   80.0 9.11  5     56.6    103.4  abcde    \n Marvellous  0.0cwt   86.7 9.11  5     63.3    110.1  abc  fg  \n Victory     0.2cwt   89.7 9.11  5     66.3    113.1  ab d f h \n Golden.rain 0.2cwt   98.5 9.11  5     75.1    121.9  abcdefghi\n Marvellous  0.2cwt  108.5 9.11  5     85.1    131.9  abcdefghi\n Victory     0.4cwt  110.8 9.11  5     87.4    134.2   bcdefghi\n Golden.rain 0.4cwt  114.7 9.11  5     91.3    138.1       fghi\n Marvellous  0.4cwt  117.2 9.11  5     93.8    140.6     de  hi\n Victory     0.6cwt  118.5 9.11  5     95.1    141.9    c e g i\n Golden.rain 0.6cwt  124.8 9.11  5    101.4    148.2       fghi\n Marvellous  0.6cwt  126.8 9.11  5    103.4    150.2         hi\n\nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \nP value adjustment: tukey method for comparing a family of 12 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nInterpretation of these letters is: Here we have a significant difference in grain yield with varieties victory, with N treatments of 0.0 cwt, 0.2 cwt, 0.4 cwt, and 0.6 wt. Grain yield for Golden rain variety was significantly lower with 0.0 cwt N treatment compared to the 0.2 cwt, 0.4 cwt, and 0.6 wt treatments.\nIn the data set we used for demonstration here, we had equal number of observations in each group. However, this might not be a case every time as it is common to have missing values in the data set. In such cases, readers usually struggle to interpret significant differences among groups. For example, estimated means of two groups are substantially different but they are no statistically different. This normally happens when SE of one group is large due to its small sample size, so it‚Äôs hard for it to be statistically different from other groups. In such cases, we can use alternatives to CLDs as shown below.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Marginal Means and Contrasts</span>"
    ]
  },
  {
    "objectID": "chapters/means-and-contrasts.html#alternatives-to-cld",
    "href": "chapters/means-and-contrasts.html#alternatives-to-cld",
    "title": "13¬† Marginal Means & Contrasts",
    "section": "13.5 Alternatives to CLD",
    "text": "13.5 Alternatives to CLD\n\nEquivalence test\n\nLet‚Äôs assume based on subject matter considerations, if mean yield of two groups differ by less than 30 can be considered equivalent. Let‚Äôs try equivalence test on clds of nitrogen treatment emmeans (m2).\n\ncld(m2, delta = 30, adjust = \"none\")\n\n N      emmean   SE df lower.CL upper.CL .equiv.set\n 0.0cwt   79.4 7.17  5     60.9     97.8  1        \n 0.2cwt   98.9 7.17  5     80.4    117.3  12       \n 0.4cwt  114.2 7.17  5     95.8    132.7   23      \n 0.6cwt  123.4 7.17  5    104.9    141.8    3      \n\nResults are averaged over the levels of: V \nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \nStatistics are tests of equivalence with a threshold of 30 \nP values are left-tailed \nsignificance level used: alpha = 0.05 \nEstimates sharing the same symbol test as equivalent \n\n\nHere, two treatment groups 0.0 cwt and 0.2 cwt, 0.4 cwt and 0.6 cwt can be considered equivalent.\n\nSignificance Sets\n\nAnother alternative is to simply reverse all the boolean flags we used in constructing CLDs for m3 first time.\n\ncld(m2, signif = TRUE)\n\n N      emmean   SE df lower.CL upper.CL .signif.set\n 0.0cwt   79.4 7.17  5     60.9     97.8  12        \n 0.2cwt   98.9 7.17  5     80.4    117.3  12        \n 0.4cwt  114.2 7.17  5     95.8    132.7  1         \n 0.6cwt  123.4 7.17  5    104.9    141.8   2        \n\nResults are averaged over the levels of: V \nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \nP value adjustment: tukey method for comparing a family of 4 estimates \nsignificance level used: alpha = 0.05 \nEstimates sharing the same symbol are significantly different \n\n\n\n\n\n\n\n\nCautionary Note about CLD\n\n\n\nIt‚Äôs important to note that we cannot conclude that treatment levels with the same letter are the same. We can only conclude that they are not different.\nThere is a separate branch of statistics, equivalence testing that is for ascertaining if things are sufficiently similar to conclude they are equivalent.\nSee Section 2.0.4 for additional warnings about problems with using compact letter display.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Marginal Means and Contrasts</span>"
    ]
  },
  {
    "objectID": "chapters/means-and-contrasts.html#export-emmeans-to-excel-sheet",
    "href": "chapters/means-and-contrasts.html#export-emmeans-to-excel-sheet",
    "title": "13¬† Marginal Means & Contrasts",
    "section": "13.6 Export emmeans to excel sheet",
    "text": "13.6 Export emmeans to excel sheet\nThe outputs from emmeans() or cld() objects can exported by first converting outputs to a data frame and then using writexlsx() function from the ‚Äòwritexl‚Äô package to export the outputs.\n\nresult_n &lt;- as.data.frame(summary(m1))\n\n\nwritexl::write_xlsx(result_n)",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Marginal Means and Contrasts</span>"
    ]
  },
  {
    "objectID": "chapters/means-and-contrasts.html#graphical-display-of-emmeans",
    "href": "chapters/means-and-contrasts.html#graphical-display-of-emmeans",
    "title": "13¬† Marginal Means & Contrasts",
    "section": "13.7 Graphical display of emmeans",
    "text": "13.7 Graphical display of emmeans\nThe easiest option is to use default plotting functions that accompany the emmeans package.\n\nplot(m1)\n\n\n\n\n\n\n\nplot(m4)\n\n\n\n\n\n\n\npwpp(m1) # Harpreet: do we need to keep it?\n\n\n\n\n\n\n\n\nEventually, you are likely to want customized plots of model estimates. To do this, the means will have to be manually extracted and converted to a dataframe for usage by plotting libraries. The outputs from the cld3 object (yield of variety within each nitrogen level) can be visualized in ggplot, with variety on x-axis and estimated means of yield on y-axis. Different N treatments are presented using different colors.\n\nggplot(cld3) +\n  aes(x = V, y = emmean, color = N) +\n  geom_point(position = position_dodge(width = 0.9)) +\n  geom_errorbar(mapping = aes(ymin = lower.CL, ymax = upper.CL), \n                              position = position_dodge(width = 1),\n                width = 0.1) +\n  geom_text(mapping = aes(label = .group, y = upper.CL * 1.05), \n            position = position_dodge(width = 0.8), \n            show.legend = F)+\n  theme_bw()+\n  theme(axis.text= element_text(color = \"black\",\n                                size =12))\n\n\n\n\n\n\n\n\nRecall: groups that do not differ significantly from each other share the same letter.\nwe can also use emmip() built in emmeans package to look at the trend in interaction of variety and nitrogen factors.\n\nemmip(model1, N ~ V)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMore details on emmeans\n\n\n\nIf you want to read more about emmeans, please refer to vignettes on this CRAN page.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Marginal Means and Contrasts</span>"
    ]
  },
  {
    "objectID": "chapters/means-and-contrasts.html#conclusion",
    "href": "chapters/means-and-contrasts.html#conclusion",
    "title": "13¬† Marginal Means & Contrasts",
    "section": "13.8 Conclusion",
    "text": "13.8 Conclusion\nBe cautious with the terms ‚Äúsignificant‚Äù and ‚Äúnon-significant‚Äù, and don‚Äôt ever interpret a ‚Äúnon-significant‚Äù result as saying that there is no effect. Follow good statistical practices such as getting the model right first, and using adjusted P values for appropriately chosen families of comparisons or contrasts.\n\n\n\n\n\n\nP values, ‚Äúsignificance‚Äù, and recommendations\n\n\n\nP values are often misinterpreted, and the term ‚Äústatistical significance‚Äù can be misleading. Please refer to this link to read more about basic principles outlined by the American Statistical Association when considering p-values.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Marginal Means and Contrasts</span>"
    ]
  },
  {
    "objectID": "chapters/variance-components.html",
    "href": "chapters/variance-components.html",
    "title": "14¬† Variance & Variance Components",
    "section": "",
    "text": "14.1 Variance component estimation for 2+ random effects\nMixed, hierarchical or multilevel models provide the advantage of being able to estimate the variance of random variables and model correlations within the grouping structure of random variables. Instead of looking at a variable as a collection of specific levels to estimate, random effects view variables as being a random draw from a probability distribution.\nThe decision of how to designate a variable as random or fixed depends on (1) your experimental aims with regard to inference and (2) your data structure. 1 There is a philosophical consider and practical consideration. The philosophical approach is that random variable represents a small sample of a population you want to make inference on. The practical consideration is that when there are few sample levels, the random estimation procedure is not very reliable for those conditions. Ben Bolker has an excellent summary that we strongly recommend that you read to learn more about this. Below is an excerpt from that on the consequences of too few levels to estimate a random effect:",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Variance and Variance Components</span>"
    ]
  },
  {
    "objectID": "chapters/variance-components.html#multiple-crossed-variance-components",
    "href": "chapters/variance-components.html#multiple-crossed-variance-components",
    "title": "14¬† Variance & Variance Components",
    "section": "14.2 Multiple crossed variance components",
    "text": "14.2 Multiple crossed variance components\nThis is a common scenario: an experiment is conducted and their are multiple grouping levels, none of which are nested within each other, but the observations have multiple group memberships. A common example is a multi-environmental trial where multiple crop genotypes are evaluated at multiple locations and years. We can consider genotypes, locations and years as random effects depending on our experimental aims. Very few R packges can handle crossed random effects, but lme4 can!\nThe data used in this example is a collection of potato field trials conducted across 3 locations and 17 years. Each year, the same potato clones[^vc-1] are evaluated across all 3 locations, but each year, different clones are evaluated. Some clones are evaluted for multiple years, and a small number are evaluted each year. These potato clones were evaluated for the length-to-width ratio (LxW). We want to know how much location, year and clone contribute to this trait.\n[^vc-1] A clone is a genetically distinct potato genotype that is vegetatively propagated. It may be a released variety or an experimental breeding line.\n\n\n\nyear\nyear of trial, 16 levels\n\n\nstate\nlocation of trial, 3 levels\n\n\nclone\npotato genotype\n\n\nLxW\nlength-to-width ratio\n\n\n\n\npotato &lt;- read.csv(here::here(\"data\", \"potato_tuber_size.csv\"))\n\nNumber of observations for each location and year:\n\ntable(potato$year, potato$state)\n\n      \n       ID OR WA\n  2005 30 30 30\n  2006 30 30 30\n  2007 29 29 29\n  2008 25 25 25\n  2009 16 17 17\n  2010 28 28 28\n  2011 18 24 26\n  2012 30 32 32\n  2013 24 25 25\n  2014 33 33 33\n  2015 29 31 31\n  2016 25 25 25\n  2017 18 21 22\n  2018 25 25 25\n  2019 22 23 23\n  2020 23 24 24\n\n\nTotal number of clones evaluated:\n\nlength(unique(potato$clone))\n\n[1] 181\n\n\nTotal counts for how often invividual clones were evaluated:\n\npotato |&gt; count(clone, name = \"Frequency of Evaluation\") |&gt; count(`Frequency of Evaluation`, name = \"No. of Clones\") \n\n   Frequency of Evaluation No. of Clones\n1                        2             4\n2                        3            92\n3                        4             1\n4                        6            30\n5                        8             2\n6                        9            20\n7                       10             1\n8                       11             2\n9                       12            11\n10                      13             1\n11                      14             4\n12                      15             8\n13                      16             1\n14                      17             1\n15                      18             1\n16                      91             1\n17                      93             1\n\n\nDistribution of the dependent variable, LxW or length-to-width ratio:\n\n\n\n\n\n\nDistribution of Length-to-Width Ratio\n\n\n\n\n14.2.1 Model fitting\nFor this analysis, location (‚Äústate‚Äù) is a fixed effect because it only has 3 levels, and the researchers are only interested in understanding clone performance in those locations (Idaho, Oregon and Washington). The remaining effects are random because (1) they represent a sample of the full population that inference is desired for, and (2) each has a sufficient number of levels to make estimation feasible.\n\npotato_m1 &lt;- lmer(LxW ~ state + (1|year) + (1|state:year) + (1|clone),\n                  data = potato)\n\n\nperformance::check_model(potato_m1, check = c('qq', 'linearity'), detrend=FALSE, alpha = 0)\n\n\n\n\n\n\n\n\n\n\n14.2.2 Inference on random effects\nThe variance components for each random effect can be extracted most easily using the tidy() function from broom.mixed. We can also a bit of extra code to calculate the percent of variance explained by each component.\n\nvar_comps &lt;- tidy(potato_m1, \"ran_pars\", scales = \"vcov\") |&gt; \n  dplyr::select(group, variance = \"estimate\") |&gt; \n  mutate(`percent variance` = round(variance/sum(variance) * 100, 1))\n\nvar_comps\n\n# A tibble: 4 √ó 3\n  group      variance `percent variance`\n  &lt;chr&gt;         &lt;dbl&gt;              &lt;dbl&gt;\n1 clone      0.00344                61.6\n2 state:year 0.000468                8.4\n3 year       0.000301                5.4\n4 Residual   0.00138                24.6\n\n\n::: note-information, collapse=false ## log likelihood ratio tests ANOVA as classically defined (F-tests contrasting the between group and within group variation) is not an option for random effects. There are several ways to test if random effects are impactful on the model overall. One of the most reliable and popular methods is the log likelihood ratio test. In brief, a reduced model is refit from a full specified model omitting a random variable. The log likelihood from the two models (the full specified and reduced models) are compared and a p-value is computed for that difference given the change in number of parameters estimated. The null hypothesis is that the models are equivalent and the alternative hypothesis is that the models are not equivalent. Hence, low p-values provide evidence that the omitted factor is impactful on the dependent variable of interest. :::\nThe function ranova() in lmerTest conducts log-likelihood ratio tests for all random effects in a model.[^vc-2]\n\nranova(potato_m1)\n\nANOVA-like table for random-effects: Single term deletions\n\nModel:\n(1/LxW) ~ state + (1 | year) + (1 | state:year) + (1 | clone)\n                 npar logLik     AIC     LRT Df Pr(&gt;Chisq)    \n&lt;none&gt;              7 2041.9 -4069.7                          \n(1 | year)          6 2039.3 -4066.7    5.07  1    0.02436 *  \n(1 | state:year)    6 1953.7 -3895.4  176.34  1    &lt; 2e-16 ***\n(1 | clone)         6 1477.3 -2942.7 1129.03  1    &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n[^vc-2] It is also possible to conduct likelihood tests manually by constructing reduced models and comparing it to the fully specificed (or more specified) model. In such cases, the fixed effects need to be identical between models, and they need to fit using maximimun likelihood instead of REML.\nIt is possible to make inferences on specific levels of random effects. This take a different analytical approach than fixed effects and has a different term, predictions, or more specifically Best Linear Unbiased Predictions, commonly called ‚ÄúBLUPs‚Äù (rhymes with ‚Äúcups‚Äù). The estimated marginal means from fixed effects are technically BLUEs, or Best Linear Unbiased Estimates. By their nature, BLUPs ‚Äúshrink‚Äù the estimates towards zero, reducing their overall spread compared to fixed effects. VSNi has a short summary on BLUPs and BLUEs.\nRecall that random effects are distributed with a mean of zero and a standard deviation, \\(\\sigma_b\\) that is estimated in the model fitting procedure. We can add the overall model intercept to all BLUPs in order to shift them to a scale that may be more intuitive for some. Since this constant (the model intercept) was added to all BLUPs, the overall relationship between the them (in this example, potato clones) is unchanged.\n\n# the model intercept\nintercept = fixef(potato_m1)[1]\n\n# all random effects from the model\nrandom_effects &lt;- REextract(potato_m1)\n\n# filter to clone BLUPs and add the intercept\nclone_re &lt;- filter(random_effects, groupFctr == \"clone\") |&gt; \n  rename(clone = groupID, BLUP = \"(Intercept)\", SE = \"(Intercept)_se\") |&gt; \n  mutate(blup_adj = BLUP + intercept)\n\n\n\n\n\n\n\nHistogram of Clone BLUPs\n\n\n\nBelow are the BLUPs (in red) and standard errors for each clone, arranged from lowest to highest length-to-width ratio. The horizontal gray dotted line indicates the average clone effect.\n\nggplot(clone_re, aes(x = reorder(clone, blup_adj), y = blup_adj)) +\n  geom_hline(yintercept = intercept, color = \"gray\", linetype = 2) + \n  geom_linerange(aes(ymax = blup_adj + SE, ymin = blup_adj - SE)) +\n    geom_point(size = 0.7, col = \"violetred1\") + \n  theme_classic(base_size = 14) +\n  ggtitle(\"length-to-width ratio\") + \n  ylab(\"BLUP + intercept\") + \n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(), \n        axis.title.x = element_blank())",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Variance and Variance Components</span>"
    ]
  },
  {
    "objectID": "chapters/variance-components.html#nested-variance-components",
    "href": "chapters/variance-components.html#nested-variance-components",
    "title": "14¬† Variance & Variance Components",
    "section": "14.3 Nested variance components",
    "text": "14.3 Nested variance components\nIt is possible to have a nested data set: learners in a classroom and classrooms in institutions, cows in dairies and dairies in counties or other regional indicators. These end up being truly hierarchical models, where we expect that there is some covariance between observations at the different levels. Cows in the same dairy are likely to be more correlated with each other than cows from another dairy. But how correlated? And how much does each grouping level influence the variation of whatever dependent variable is being studied.\nThis example is looking at pay at institutions of higher education in Washington. All annual earnings less than $25,000 were excluded from the data set, and only earners with a minimum of four years of data were included. Although this is public information, all individual names were anonymized.2\n2¬†This data set (source) was incompletely curated and hence no substantive conclusions should be drawn from this particular analysis. This example is for teaching and demonstration purposes only.For this data set, we are concerned with what is contributing to employee salaries over time - to what extent did year, the institution itself and the individual employee contribute to salary changes over time. While this reflects a complete data set for Washington State (all institutions, all employees), the interest is inference for other U.S. higher education institutions. This is an example where we will use random slopes to examine how an individual‚Äôs salary changed over time. The data set:\n\n\n\nagency\neducational institution, 36 levels\n\n\nid\nanonymized employee within each agency\n\n\nsalary\nannual salary\n\n\nyear\nyear of salary, set to 0 for 2019\n\n\n\nThis is a rather large data set, and it requires quite a bit of computing power to analyze. Hence, the analysis was done locally, and the data set and model object were saved as a .RData file. Now let‚Äôs load this object into an R session.\n\nload(here::here(\"data/salary_modeldata.RData\"))\n\n\n14.3.1 Model fitting\nSalary is very skewed, where the 70% of salaries are less than $100,000, but there are a few very high salaries, up to a maximum value of $3.1 million. We will log transform the data in a log-linear model for analysis. This is a special case of generalized linear mixed model (GLMM) and the only GLMM model used because it still follow the assumption of normally-distributed residuals terms when a the dependent variable is log-transformed.\n\nhist(log(wa_salary$salary), ylab=NULL, xlab = NULL, main = \"Washington Higher Ed Salaries\")\n\n\n\n\n\n\n\nHistogram of Log of Salaries\n\n\n\n\nboxplot(log(salary) ~ year, data = wa_salary, ylab=NULL, xlab = NULL, main = \"Salaries by Year\")\n\n\n\n\n\n\n\nBoxpplot of Log Salaries by Year\n\n\n\nThe analysis is derived from this model:\n\\[y_ij = \\mu + x_i +   \\]\n\nm1 &lt;- lmer(log(salary) ~ factor(year) + (year|agency/id), \n           data = wa_salary, REML = FALSE)\n\nrandom_effects &lt;- REextract(m1)\n\nsave(m1, wa_salary, random_effects, file = here::here(\"data\", \"salary_modeldata.RData\"))\n\n\nm1 &lt;- lmer(log(salary) ~ factor(year) + (year|agency/id), \n           data = wa_salary, REML = FALSE)\n\nRandom intercepts and slopes were fit for agency and id nested within agency.\n\n\n14.3.2 Inference\n\n14.3.2.1 Look at Variance Components\n\nVarCorr(m1)\n\n Groups    Name        Std.Dev. Corr  \n id:agency (Intercept) 0.473969       \n           year        0.079368 -0.368\n agency    (Intercept) 0.087550       \n           year        0.016624 -0.674\n Residual              0.158676       \n\n\n\nvar_comps_salary &lt;- tidy(m1, \"ran_pars\", scales = \"vcov\") |&gt; \n  filter(grepl(\"^var.*\", term)) |&gt; \n  dplyr::select(group, term, variance = \"estimate\") |&gt; \n  mutate(`percent variance` = round(variance/sum(variance) * 100, 1))\n\nvar_comps_salary\n\n# A tibble: 5 √ó 4\n  group     term             variance `percent variance`\n  &lt;chr&gt;     &lt;chr&gt;               &lt;dbl&gt;              &lt;dbl&gt;\n1 id:agency var__(Intercept) 0.225                  85.1\n2 id:agency var__year        0.00630                 2.4\n3 agency    var__(Intercept) 0.00767                 2.9\n4 agency    var__year        0.000276                0.1\n5 Residual  var__Observation 0.0252                  9.5\n\n\n\n\n14.3.2.2 Extract BLUPs\n\n# the model intercept\nintercept = fixef(m1)[1]\n\n# all random effects from the model\nrandom_effects &lt;- REextract(m1)\n\n\n\n14.3.2.3 Visualize Random Effects\n(intercepts and slopes)\n\nFilter BLUPs to a single factor, ‚Äúagency‚Äù (i.e.¬†institute of higher education).\n\n\nagency_re &lt;- filter(random_effects, groupFctr == \"agency\") |&gt; \n  rename(Institution = groupID, BLUP = \"(Intercept)\", SE = \"(Intercept)_se\") |&gt; \n  mutate(blup_adj = BLUP + intercept) |&gt; \n  mutate(`salary slope` = ifelse(year &gt; 0, \"+\", \"-\"))\n\n\nPlot Salaries by Institution:\n\n\nggplot(agency_re, aes(x = reorder(Institution, blup_adj), y = blup_adj)) +\n  geom_hline(yintercept = intercept, color = \"gray\", linetype = 2) + \n  geom_linerange(aes(ymax = blup_adj + SE, ymin = blup_adj - SE)) +\n    geom_point(size = 3, col = \"violetred\", shape = 18, alpha = 0.6) + \n  labs(title = \"Institutional Salaries, 2019 - 2024\", subtitle = \"Log of Salaries\") + \n  ylab(\"BLUP + mean and standard error\") + \n  theme_classic(base_size = 14) +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(), \n        axis.title.x = element_blank())\n\n\n\n\n\n\n\n\n\nPlot Salary Changes by Institution:\n\n\nggplot(agency_re) +  # need sequence -0.4 to 0.4\n  geom_abline(aes(slope = year, intercept = blup_adj, col = `salary slope`), alpha = 0.7, linewidth = 0.75) +\n  geom_hline(yintercept = intercept, color = \"black\", linetype = 2) + \n  labs(title = \"Changes in Institutional Salaries, 2019 - 2024\", subtitle = \"Log of Salaries\") + \n  xlim(c(0, 4)) +\n  ylim(c(10.75, 11.3)) + \n  scale_color_manual(values = c(\"springgreen3\", \"violetred\")) +\n  #ylim(c(-0.25, 0.3)) +\n  ylab(\"log salary\") + xlab(\"year\") + \n  theme_bw(base_size = 14) +\n  theme(legend.position = c(0.98, 0.98),\n        legend.justification = c(1,1),\n        legend.background = element_rect(color = \"gray60\", fill = rgb(1, 1, 1, alpha = 0.8)),\n        legend.key = element_rect(fill = \"transparent\"))\n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\n‚Ñπ Please use the `legend.position.inside` argument of `theme()` instead.\n\n\n\n\n\n\n\n\n\n\nExamine Relationship Between Employee Salary and Changes in Salary:\n\n\nperson_re &lt;- filter(random_effects, groupFctr == \"id:agency\") |&gt; \n  rename(employee = groupID, BLUP = \"(Intercept)\", SE = \"(Intercept)_se\") |&gt; \n  mutate(blup_adj = BLUP + intercept) |&gt; \n  mutate(`salary slope` = ifelse(year &gt; 0, \"+\", \"-\"))\n\n\nggplot(person_re, aes(x = BLUP, y = year)) +\n  geom_point(alpha = 0.4, color = \"royalblue\") +\n  geom_hline(yintercept = 0, col = \"black\", linetype = 2) +\n  geom_vline(xintercept = 0, col = \"black\", linetype = 2) +\n  labs(title = \"Changes in Employee Salaries, 2019 - 2024\", subtitle = \"(Log of Salaries)\") +\n  xlab(\"random intercept (BLUP) for Employee Salary\") +\n  ylab(\"random slope for Employee salary increase\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nThere is a negative correlation between employee salary and employee salary change, which is also indicated when VarCorr(m1) was run.\nThere are other functions in merTools to explore random effects and variances from an lme4 analysis.",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Variance and Variance Components</span>"
    ]
  },
  {
    "objectID": "chapters/variance-components.html#variance-estimation-under-heteroscedasticity",
    "href": "chapters/variance-components.html#variance-estimation-under-heteroscedasticity",
    "title": "14¬† Variance & Variance Components",
    "section": "14.4 Variance estimation under heteroscedasticity",
    "text": "14.4 Variance estimation under heteroscedasticity\n\nIn the ‚Äúgeneral‚Äù linear model days, when a premium was placed on the i.i.d. error paradigm, if we did reject \\(H_0\\), it would set off a minor crisis in the form of a hunt for a variance stabilizing transformation. In contemporary modeling, we simply proceed with inference on estimable functions using the equal variance model.\n\n‚Äì Walt Stroup, Marina Ptukhiuna and Julie Garai (2024), Generalized Linear Mixed Models, 2nd Ed, Section 7.2.3.1\nIn previous sections, we have assumed the error terms or residuals were ‚Äúi.i.d.‚Äù, that is ‚Äúindependently and identically distributed. This means they were shared the same distribution (identical) and were uncorrelated (independent). Longitudinal studies, that is, those with repeated measures, do have correlated residuals, so we relax the independence assumption and model those correlations. However, residuals can be unexpectedly related to the their observations, particular treatments or the order data was gathered from the experimental units (among other causes). As mentioned in the previous quote, we now have tools for handling this rather than trying to transform the data. Below are examples on how to model heteroscedasticity.\n\n14.4.1 Case 1: unequal variance due to a factor\nThis data set is from a set of canola variety trials conducted in a single year across multiple locations. The trials included 38 varieties that were evaluated at 9 locations using a RCBD design.\n\nvar_ex1 &lt;- read.csv(here::here(\"data\", \"MET_trial_variance.csv\")) |&gt; \n  mutate(block = as.character(block)) |&gt; \n  tidyr::drop_na()\n\nExploratory data visualizations indicate that the dependent variable, seed yield, varied greatly overall and certain locations had smaller variance compared to others.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhist(var_ex1$yield, main=NA)\nboxplot(yield ~ site, data = var_ex1)\n\nThe study is not fully crossed; all sites did not include all varieties, although there is substantial overlap. As a result, only variety and the site-by-variety interaction are included in the statistical model.\nWhile it may be possible to implement heterogenous error structures with lme4, it requires a good working knowledge of that R package. is requires extensive comfort programming in lme4. However, the package nlme contains relatively straightforward implementations that are illustrated below.\n\nm1_a &lt;- lme(yield ~ site:variety + variety, \n                random = ~ 1 |site/block, \n                na.action = na.exclude, \n                data = var_ex1)\n\nThe residual plot indicates some association between the residuals and fitted values, violating the i.i.d. model assumption.\n\nplot(m1_a)\n\n\n\n\n\n\n\n\nWe can add a term to model the variance by site.\n\\[Var(\\epsilon_{ij}) = \\sigma^2 \\delta^2_{s_{ij}} \\] Details on the implementation can be found in (Pinheiro and Bates 2000).\n\nm1_b &lt;- update(m1_a, weights = varIdent(form = ~1|site))\n\nThe function varIdent() is used to set the stratifying variable at which to estimate variance. Like many functions in R, there are additional arguments to consider for more complex scenarios (type ?varIdent in an R console to check).\n\n\nm1_b &lt;- update(m1_a, weights = varIdent(form = ~1|site))\n\nis equivalent to\n\nm1_b &lt;- lme(yield ~ site:variety + variety, \n                random = ~ 1 |site/block,\n                weights = varIdent(form = ~1|site), \n                na.action = na.exclude, \n                data = var_ex1)\n\n\nThe residual plot is now much cleaner. The result is a better-fitting model and with that, better inference for variety at the site level.\n\nplot(m1_b)\n\n\n\n\n\n\n\nanova(m1_a, m1_b)\n\n     Model  df      AIC      BIC    logLik   Test  L.Ratio p-value\nm1_a     1 345 14826.99 16524.62 -7068.493                        \nm1_b     2 353 14499.68 16236.68 -6896.840 1 vs 2 343.3059  &lt;.0001\n\n\n\nm1_c &lt;- lmer(yield ~ site:variety + variety + (1|site/block),\n                #weights = varIdent(form = ~1|site), \n                na.action = na.exclude, \n                data = var_ex1)\n\n\n\n14.4.2 Case 2: Variance is related to the fitted values\nBelow is the infamous ‚Äòhorn‚Äô pattern in the residuals-vs-fitted values plot. This analysis is from a canola variety trial of 38 cultivars conducted at single location for one field season.\n\nvar_ex2 &lt;- read.csv(here::here(\"data\", \"single_trial_variance.csv\")) |&gt; \n  dplyr::mutate(block = as.character(block))\n\nA histogram does indicate there is anything unusual about the dependent variable (seed yield), except that is varies quite a bit, ranging from 24 to nearly 950 units.\n\nhist(var_ex2$yield, main = NA)\n\n\n\n\n\n\n\n\n\n\nSince this experiment has a single fixed effect and is arranged using a RCBD, the model is same as described in the RCBD chapter.\n\nm2_a &lt;- lme(yield ~ variety, \n               random = ~ 1 |block, \n               na.action = na.exclude, \n               data = var_ex2)\n\nAn inspection of the residual plot indicates a clear mean-variance relationship.\n\nplot(m2_a)\n\n\n\n\n\n\n\n\nThis mean-variance relationship can be mitigated by modelling the variance directly as a function of any covariate in the model using a power function.\n\\[ Var(\\epsilon_{ij}) = \\sigma^2|\\nu_{ij}|^{2\\delta}\\]\nWe can accomplish this using the nlme function varPower(). This function can take other covariates, but when there is no argument provided, it defaults to using the fitted values.\n\nm2_b &lt;- update(m2_a, weights = varPower())\n\nThe model fit is substantially improved according to visual inspection of the residuals and the results of a likelihood ratio test.\n\nplot(m2_b)\n\n\n\n\n\n\n\n\n\nanova(m2_a, m2_b)\n\n     Model df      AIC      BIC    logLik   Test  L.Ratio p-value\nm2_a     1 40 1545.596 1655.044 -732.7978                        \nm2_b     2 41 1519.199 1631.383 -718.5996 1 vs 2 28.39636  &lt;.0001\n\n\nThere are many other ways of using these functions for modeling heteroscedasticity. For example, varIdent() can include a covariate, and varPower() can include a stratifying variable. All or some of the parameters can be fixed at set values. It‚Äôs worth reading the documentation to understand what is possible.\n\n\n14.4.3 Case 3: Variance is related to a factor under other complex circumstances\nRecall the repeated measures/split-split plot example that indicated some evidence of heteroscedasticity.\n\ncheck_model(fit1, check = c('qq', 'linearity'), line_size = 0, detrend=FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†14.1: Boxplot of P concentration by stage\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can model the variance as a function of time point by using the varIdent() function shown earlier.\n\nfit1_b &lt;- update(fit1, weights = varIdent(form = ~1|time))\n\n\ncheck_model(fit1_b, check = c('qq', 'linearity'), line_size = 0, detrend=FALSE)\n\n\n\n\n\n\n\n\nA comparison of models indicates that ‚Äôfit1_b` (with heteroscedascity modelled) is a better fit.\n\nanova(fit1, fit1_b)\n\n       Model df      AIC      BIC    logLik   Test  L.Ratio p-value\nfit1       1 66 2511.987 2722.722 -1189.993                        \nfit1_b     2 68 2508.314 2725.435 -1186.157 1 vs 2 7.672945  0.0216\n\n\nThis change impacts the standard errors:\n\nsuppressMessages({\n  em1 &lt;- as.data.frame(emmeans(fit1, ~ time)) |&gt; mutate(model = \"equal variance\")\n  em2 &lt;- as.data.frame(emmeans(fit1_b, ~ time)) |&gt; mutate(model = \"unequal variance\")\n})\n\nbind_rows(em1, em2) |&gt; dplyr::select(model, time, emmean, SE, df) |&gt; arrange(time, model)\n\n             model time    emmean       SE df\n1   equal variance  PT1 3095.5645 47.21842  3\n2 unequal variance  PT1 3095.5645 39.92148  3\n3   equal variance  PT2 2269.5734 47.21842  3\n4 unequal variance  PT2 2269.5734 39.48225  3\n5   equal variance  PT3  197.6457 47.21842  3\n6 unequal variance  PT3  197.6458 37.57384  3",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Variance and Variance Components</span>"
    ]
  },
  {
    "objectID": "chapters/variance-components.html#coefficient-of-variation",
    "href": "chapters/variance-components.html#coefficient-of-variation",
    "title": "14¬† Variance & Variance Components",
    "section": "14.5 Coefficient of Variation",
    "text": "14.5 Coefficient of Variation\nThe coefficient of variation can be manually calculated as thus:\n\\[ \\frac {\\sigma}{\\mu} * 100 \\]\n\nm1_ave &lt;- mean(var_ex1$yield, na.rm = TRUE)\nm1_cv = sigma(m1_b)/m1_ave*100\nround(m1_cv, 1)\n\n[1] 23.3\n\n\nHowever, in cases of unequal variance, the overall error term can be larger than expected under homoscedasticity (with varIdent()) or much much smaller (e.g.¬†with varPower()). Interpret the coefficient of variation from mixed models with caution.\n\n\n\n\nPinheiro, Jos√© C., and Douglas M. Bates. 2000. Mixed-Effects Models in s and s-PLUS. New York: Springer. https://doi.org/10.1007/b98882.",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Variance and Variance Components</span>"
    ]
  },
  {
    "objectID": "chapters/troubleshooting.html",
    "href": "chapters/troubleshooting.html",
    "title": "15¬† Troubleshooting",
    "section": "",
    "text": "15.1 Common errors we encounter\nIn this tutorial, we have demonstrated how to use linear mixed models for different experiment designs using examples we have already tested and know work. However, it is likely that you will eventually have issues in fitting mixed models in R. This section discusses common issues and errors that might come up when implementing mixed models.\nboundary (singular) fit: see ?isSingular\nThis can happen when trying to fit a model that is too complex for the amount of data available, or when the random effects are very small and cannot be distinguished from zero.\nThis error occurs because one of the variance component is approximately zero. If a model is ‚Äúsingular‚Äù, this means that some dimensions of the variance-covariance matrix have been estimated as nearly zero or as close to zero as your computer‚Äôs precision will allow.\nThere is no single solution for this problem. Here are the possible solutions that you can try:\nFirst, look at the summary of the fitted model using summary() function, If random effect variance is near zero, simplify the model by removing it. However, by removing the random effect we are loosing the model complexity and potentially important variation that the random effect was initially intended to capture.\nOr other option is to include the block as a fixed effects and re-analyze the experiment. If there are no other random effects in the model, you can use the function gls() in nlme that uses generalized least squares for model fit and allows for correlated and unequal variances.\nThis error from nlme is indicating a rank deficient model that cannot be estimated as it is currently specified.\nIf you are getting a rank deficiency warning in lmer() (from the lme4 package), it means your fixed-effect model matrix contains redundant or collinear predictors, preventing the model from estimating unique coefficients.\nHere are methods to diagnose and fix it: - check rank of model matrix using model.matrix() function. If the rank is less than the number of columns, the model matrix is deficient.\nThere are several different error messages for non-convergence, depending on what happened:\nconvergence error code = 1 message = iteration limit reached without convergence (10)\nHessian that is not positive definite\nThere are several causes for lack of convergence. Model fitting is an iterative process where in each iteration, the latest estimates are compared to the estimates from the previous iteration. When the difference becomes in estimates beacome substantially small, modelling packages declare that convergence criteria were met. There are several reasons why a model may not converge: (1) it needed more iterations, (2) the model does not fit the data well, (3) the software did not have a decent starting point for this model and data; among other plausible explanations.\nRunning this code in an R console will bring up a useful help file with details on how to address convergence issues.\n?convergence\nPossible solutions to fix this error are: - Try a different optimizer\nmodel &lt;- lmer(y ~ x + (1|block), data = dataset)\nmodel1 &lt;- update(model, control = lmerControl(optimizer = \"bobyqa\"))\nThe default optimizer used in lmer() is ‚ÄúNelder_Mead‚Äù, ‚Äúbobyqa‚Äù optimizer is recommended for stability.\nWe can update our existing model by increasing max iterations to 100,000.\nmodel &lt;- lmer(y ~ x + (1|block), data = dataset)\nmodel2 &lt;- update(model1, control = lmerControl(optCtrl = list(maxfun = 100000)))",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Troubleshooting</span>"
    ]
  },
  {
    "objectID": "chapters/troubleshooting.html#common-errors-we-encounter",
    "href": "chapters/troubleshooting.html#common-errors-we-encounter",
    "title": "15¬† Troubleshooting",
    "section": "",
    "text": "Singular fit error\n\n\n\n\n\n\n\n\nfixed-effect model matrix is rank deficient\n\nError in MEestimate(lmeSt, grps) :\n  Singularity in backsolve at level 0, block 1\n\nfixed-effect model matrix is rank deficient so dropping 1 column / coefficient\nboundary (singular) fit: see help('isSingular')\n\n\n\nModel failed to converge.\n\n\n\n\n\n\n\n\n\n\n\nIncrease iterations for convergence.\n\n\n\n\nLast, we can simplify the model by removing random effects that do not appear to be detectable.\n\n\n15.1.1 Other Tools\nTroubleshooting LMM model failures is difficult. One thing that helps is to browse through your data and make sure it looks like what you expect, and that your expectations of the variables are how R seem them as well.\nHaving an understanding of the moving parts of lme4 and how they each contribute to model fit can help. A help ful lme4 control structures for model fitting can be accessed via ?lmerControl.\nPosit (n√©e RStudio) has published a rather useful guide for advanced troubleshooting of convergence issues in lme4. Phillip Alday wrote a detailed tutorial on lme4 convergence warnings.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Troubleshooting</span>"
    ]
  },
  {
    "objectID": "chapters/additional-resources.html",
    "href": "chapters/additional-resources.html",
    "title": "16¬† Additional Resources",
    "section": "",
    "text": "16.1 Further Reading on Mixed Models",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Additional Resources</span>"
    ]
  },
  {
    "objectID": "chapters/additional-resources.html#further-reading-on-mixed-models",
    "href": "chapters/additional-resources.html#further-reading-on-mixed-models",
    "title": "16¬† Additional Resources",
    "section": "",
    "text": "UCLA Short Guide\nlme4 vignette for fitting linear mixed models\nMixed-Effects Models in S and S-PLUS thee book for nlme, by Jos√© C. Pinheiro and Douglas M. Bates (2000). We used this book extensively for developing this guide. Sadly, it‚Äôs both out of print and we could not find a free copy online. However, there are affordable used copies available.\nData Analysis Using Regression and Multilevel/Hierarchical Models by Andrew Gelman and Jennifer Hill (2006). Good general introduction with a major focus on Bayesian models. Free PDF\nMixed Effects Models and Extensions in Ecology with R by Alain F. Zuur, Elena N. Ieno, Neil Walker, Anatoly A. Saveliev, and Graham M. Smith (2009).\nANOVA and Mixed Models by Lukas Meier (2022).",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Additional Resources</span>"
    ]
  },
  {
    "objectID": "chapters/additional-resources.html#other-related-programming-resources",
    "href": "chapters/additional-resources.html#other-related-programming-resources",
    "title": "16¬† Additional Resources",
    "section": "16.2 Other Related Programming Resources",
    "text": "16.2 Other Related Programming Resources\n\nEasy Stats a collection of R packages to assist in statistical modelling with a major focus on linear models.\nMixed Model CRAN Task View a curated list of R packages relevant to mixed modelling. This is a great place to start if you‚Äôre looking for specialized modeling functionality.\nR-SIG-mixed-models mailing list for help and discussion of mixed-model-related questions, course announcements, etc.\nGrammar of Experimental Designs by Emi Tanaka. This has a great description of basic principles of experimental design.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Additional Resources</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bates, Douglas, Martin M√§chler, Ben Bolker, and Steve Walker. 2015.\n‚ÄúFitting Linear Mixed-Effects Models Using lme4.‚Äù Journal of Statistical\nSoftware 67 (1): 1‚Äì48. https://doi.org/10.18637/jss.v067.i01.\n\n\nBolker, Ben, and David Robinson. 2024. Broom.mixed: Tidying Methods\nfor Mixed Models. https://CRAN.R-project.org/package=broom.mixed.\n\n\nBroman, Karl W., and Kara H. Woo. 2018. ‚ÄúData\nOrganization in Spreadsheets.‚Äù The\nAmerican Statistician 72 (1): 2‚Äì10. https://doi.org/10.1080/00031305.2017.1375989.\n\n\nDushoff, Jonathan, Morgan P. Kain, and Benjamin M. Bolker. 2019.\n‚ÄúI Can See Clearly Now: Reinterpreting Statistical\nSignificance.‚Äù Methods in Ecology and Evolution 10 (6):\n756‚Äì59. https://doi.org/https://doi.org/10.1111/2041-210X.13159.\n\n\nGreenland, Sander, Stephen J. Senn, Kenneth J. Rothman, John B. Carlin,\nCharles Poole, Steven N. Goodman, and Douglas G. Altman. 2016.\n‚ÄúStatistical Tests, P Values, Confidence Intervals,\nand Power: A Guide to Misinterpretations.‚Äù European Journal\nof Epidemiology 31 (4): 337‚Äì50. https://doi.org/10.1007/s10654-016-0149-3.\n\n\nHartig, Florian. 2022. DHARMa: Residual Diagnostics for Hierarchical\n(Multi-Level / Mixed) Regression Models. https://CRAN.R-project.org/package=DHARMa.\n\n\nJohn, JA, and ER Williams. 1995. Cyclic and Computer\nGenerated Designs. 2nd ed. New York:\nChapman; Hall/CRC Press. https://doi.org/10.1201/b15075.\n\n\nKuznetsova, Alexandra, Per B. Brockhoff, and Rune H. B. Christensen.\n2017. ‚ÄúlmerTest Package: Tests in\nLinear Mixed Effects Models.‚Äù Journal of Statistical\nSoftware 82 (13): 1‚Äì26. https://doi.org/10.18637/jss.v082.i13.\n\n\nLenth, Russell V. 2022. Emmeans: Estimated Marginal Means, Aka\nLeast-Squares Means. https://CRAN.R-project.org/package=emmeans.\n\n\nL√ºdecke, Daniel, Mattan S. Ben-Shachar, Indrajeet Patil, Philip\nWaggoner, and Dominique Makowski. 2021. ‚Äúperformance: An R Package for\nAssessment, Comparison and Testing of Statistical Models.‚Äù\nJournal of Open Source Software 6 (60): 3139. https://doi.org/10.21105/joss.03139.\n\n\nNair, K. R. 1952. ‚ÄúAnalysis of Partially Balanced Incomplete Block\nDesigns Illustrated on the Simple Square and Rectangular\nLattices.‚Äù Biometrics 8 (2): 122‚Äì55. https://doi.org/10.2307/3001929.\n\n\nPatterson, H. D., and E. R. Williams. 1976. ‚ÄúA New\nClass of Resolvable Incomplete\nBlock Designs.‚Äù Biometrika 63\n(1): 83‚Äì92. https://doi.org/10.2307/2335087.\n\n\nPinheiro, Jos√© C., and Douglas M. Bates. 2000. Mixed-Effects Models\nin s and s-PLUS. New York: Springer. https://doi.org/10.1007/b98882.\n\n\nPinheiro, Jos√©, Douglas Bates, and R Core Team. 2023. Nlme: Linear\nand Nonlinear Mixed Effects Models. https://CRAN.R-project.org/package=nlme.\n\n\nWasserstein, Ronald L., and Nicole A. Lazar. 2016. ‚ÄúThe ASA\nStatement on p-Values: Context, Process, and Purpose.‚Äù The\nAmerican Statistician 70 (2): 129‚Äì33. https://doi.org/10.1080/00031305.2016.1154108.\n\n\nYates, F. 1936. ‚ÄúA New Method of Arranging Variety Trials\nInvolving a Large Number of Varieties.‚Äù J Agric Sci 26:\n424‚Äì55.",
    "crumbs": [
      "References"
    ]
  }
]