[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Field Guide to the R Mixed Model Wilderness",
    "section": "",
    "text": "Preface\n“Path in the Wilderness” by Erich Taeubel, Jr.\nRunning mixed models in R is no easy task. There are dozens of packages supporting these aims, each with varying functionality, syntax, and conventions. The linear mixed model ecosystem in R consists of over 80 libraries that either construct and solve mixed model equations or helper packages the process the results from mixed model analysis. These libraries provide a patchwork of overlapping and unique functionality regarding the fundamental structure of mixed models: allowable distributions, nested and crossed random effects, heterogeneous error structures and other facets. No single library has all possible functionality enabled.\nThis patchwork of packages makes it very challenging for statisticians to conduct mixed model analysis and to teach others how to run mixed models in R. The purpose of this guide to to provide some recipes for handling common analytical scenario’s that require mixed models. As a field guide, it is intended to be succinct, and help researchers meet their analytic goals.\nIn general, the content from this website may not be copied or reproduced. However, the example code and required data sets to run the code are MIT licensed. These can be accessed on GitHub.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#recent-updates",
    "href": "index.html#recent-updates",
    "title": "Field Guide to the R Mixed Model Wilderness",
    "section": "Recent Updates",
    "text": "Recent Updates\nThis is a work-in-progress and will be updated over time.\n\nRCBD lesson has been completed.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/intro.html",
    "href": "chapters/intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Terms\nPlease read this section and refer back to if when you forget what these terms mean.\n|———-|—————————————-|\n|random effect | an independent variable where the levels being estimated compose a random sample from a population whose variance will be estimated |\n|fixed effect. | an independent variable with specific, predefined levels to estimate |\n|experimental unit. | the smallest unit being used for analysis. This could be an animal, a field plot, a person, a meat or muscle sample. The unit may be assessed multitple times or through multiple point in time. When the analysis is all said and done, the predictions occur at this level |\n: Terms definitions {#tbl-terms}",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/intro.html#packages",
    "href": "chapters/intro.html#packages",
    "title": "1  Introduction",
    "section": "1.2 Packages",
    "text": "1.2 Packages\n\n1.2.0.1 Table of required packagesn for modelling\n|———-|——————————|\n|lme4 | main package for linear mixed models |\n|lmerTest | for computing p-values when using lme4 |\n|nlme | main package for linar mixed models |\n|emmeans | for estimating fixed effects, their confidence intervals and conducting contrasts |\n|glmmTMB | package for generalized linear mixed models |\n|DHARMa | for evaluating residuals (error terms) in generalized linear models |\n: Table of required packages {#tbl-req_pkg}\n\n\n1.2.0.2 Optional packages\n|———-|——————————| |here | for setting the working directory |\n|ggplot | plotting | |desplot | plotting |\n: Table of optional packages {tbl-opt-pkg}\nThis entire guide will use the here package for loading data. If you can load your data fine without ths package, please carry on. {here} is certainly not required for running mixed models.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/rcbd.html",
    "href": "chapters/rcbd.html",
    "title": "2  Randomized Complete Block Design",
    "section": "",
    "text": "2.1 Background\nThe statistical model:\n\\[y_{ij} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}\\] Where:\n\\(\\mu\\) = overall experimental mean \\(\\alpha\\) = treatment effects (fixed) \\(\\beta\\) = block effects (random) \\(\\epsilon\\) = error terms\n\\[ \\epsilon \\sim N(0, \\sigma)\\]\n\\[ \\beta \\sim N(0, \\sigma_b)\\]\nBoth the overall error and the block effects are assumed to be normally distributed with a mean of zero and standard deviations of \\(\\sigma\\) and \\(sigma_B\\), respectively.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Randomized Complete Block Design</span>"
    ]
  },
  {
    "objectID": "chapters/rcbd.html#background",
    "href": "chapters/rcbd.html#background",
    "title": "2  Randomized Complete Block Design",
    "section": "",
    "text": "‘iid’ assumption for error terms\n\n\n\nIn this model, the error terms, \\(\\epsilon\\) are assumed to be “iid”, that is, independently and identically distributed. This means they have constant variance and they each individual error term is independent from the others.\nThis guide will later address examples when this assumption is violated and how to handle it.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Randomized Complete Block Design</span>"
    ]
  },
  {
    "objectID": "chapters/rcbd.html#example-analysis",
    "href": "chapters/rcbd.html#example-analysis",
    "title": "2  Randomized Complete Block Design",
    "section": "2.2 Example Analysis",
    "text": "2.2 Example Analysis\nFirst, load the libraries for analysis and estimation:\n\nlme4tidymodels\n\n\n\nlibrary(lme4); library(lmerTest); library(emmeans)\nlibrary(dplyr)\n\n\n\n\nlibrary(dplyr)\nlibrary(multilevelmod); library(broom)\n\n\n\n\nNext, let’s load some data. It is located here if you want to download it yourself (recommended).\nThis data set is for a single wheat variety trial conducted in Aberdeen, Idaho in 2015. The trial includes 4 blocks and 42 different treatments (wheat varieties in this case). This experiment consists of a series of plots (the experimental unit) laid out in a rectangular grid in a farm field. The goal of this analysis is the estimate the yield and test weight of each variety and the determine the rankings of each variety with regard to yield.\n\nvar_trial &lt;- read.csv(here::here(\"data\", \"aberdeen2015.csv\"))\n\n\nTable of variables in the data set\n\n\n\n\n\n\nblock\nblocking unit\n\n\nrange\ncolumn position for each plot\n\n\nrow\nrow position for each plot\n\n\nvariety\ncrop variety (the treatment) being evaluated\n\n\nstand_pct\npercentage of the plot with actual plants growing in them\n\n\ndays_to_heading_julian\nJulian days (starting January 1st) until plot “headed” (first spike emerged)\n\n\nlodging\npercentage of plants in the plot that fell down and hence could not be harvested\n\n\nyield_bu_a\nyield (bushels per acre)\n\n\n\nThere are several variables present that are not useful for this analysis. The only thing we are concerned about is block, variety, yield_bu_a, and test_weight.\n\n2.2.1 Data integrity checks\nThe first thing is to make sure the data is what we expect. There are two steps:\n\nmake sure data are the expected data type\ncheck the extent of missing data\ninspect the independent variables and make sure the expected levels are present in the data\ninspect the dependent variable to ensure its distribution is following expectations\n\n\nstr(var_trial)\n\n'data.frame':   168 obs. of  10 variables:\n $ block                 : int  4 4 4 4 4 4 4 4 4 4 ...\n $ range                 : int  1 1 1 1 1 1 1 1 1 1 ...\n $ row                   : int  1 2 3 4 5 6 7 8 9 10 ...\n $ variety               : chr  \"DAS004\" \"Kaseberg\" \"Bruneau\" \"OR2090473\" ...\n $ stand_pct             : int  100 98 96 100 98 100 100 100 99 100 ...\n $ days_to_heading_julian: int  149 146 149 146 146 151 145 145 146 146 ...\n $ height                : int  39 35 33 31 33 44 30 36 36 29 ...\n $ lodging               : int  0 0 0 0 0 0 0 0 0 0 ...\n $ yield_bu_a            : num  128 130 119 115 141 ...\n $ test_weight           : num  56.4 55 55.3 54.1 54.1 56.4 54.7 57.5 56.1 53.8 ...\n\n\nThese look okay except for block, which is currently coded as integer (numeric). We don’t want run a regression of block, where block 1 has twice the effect of block 2, and so on. So, converting it to a character will fix that. It can also be converted to a factor, but I find character easier to work with, and ultimately, equivalent to factor conversion\n\nvar_trial$block &lt;- as.character(var_trial$block)\n\nNext, check the independent variables. Running a cross tabulations is often sufficient to ascertain this.\n\ntable(var_trial$variety, var_trial$block)\n\n                        \n                         1 2 3 4\n  06-03303B              1 1 1 1\n  Bobtail                1 1 1 1\n  Brundage               1 1 1 1\n  Bruneau                1 1 1 1\n  DAS003                 1 1 1 1\n  DAS004                 1 1 1 1\n  Eltan                  1 1 1 1\n  IDN-01-10704A          1 1 1 1\n  IDN-02-29001A          1 1 1 1\n  IDO1004                1 1 1 1\n  IDO1005                1 1 1 1\n  Jasper                 1 1 1 1\n  Kaseberg               1 1 1 1\n  LCS Artdeco            1 1 1 1\n  LCS Biancor            1 1 1 1\n  LCS Drive              1 1 1 1\n  LOR-833                1 1 1 1\n  LOR-913                1 1 1 1\n  LOR-978                1 1 1 1\n  Madsen                 1 1 1 1\n  Madsen / Eltan (50/50) 1 1 1 1\n  Mary                   1 1 1 1\n  Norwest Duet           1 1 1 1\n  Norwest Tandem         1 1 1 1\n  OR2080637              1 1 1 1\n  OR2080641              1 1 1 1\n  OR2090473              1 1 1 1\n  OR2100940              1 1 1 1\n  Rosalyn                1 1 1 1\n  Stephens               1 1 1 1\n  SY  Ovation            1 1 1 1\n  SY 107                 1 1 1 1\n  SY Assure              1 1 1 1\n  UI Castle CLP          1 1 1 1\n  UI Magic CLP           1 1 1 1\n  UI Palouse             1 1 1 1\n  UI Sparrow             1 1 1 1\n  UI-WSU Huffman         1 1 1 1\n  WB 456                 1 1 1 1\n  WB 528                 1 1 1 1\n  WB1376 CLP             1 1 1 1\n  WB1529                 1 1 1 1\n\n\nThere are 42 varieties and there appears to be no misspellings among them that might confuse R into thinking varieties are different when they are actually the same. R is sensitive to case and white space, which can make it easy to create near duplicate treatments, such as “eltan” and “Eltan” and “Eltan”. There is no evidence of that in this data set. Additionally, it is perfectly balanced, with exactly one observation per treatment per rep. Please note that this does not tell us anything about the extent of missing data.\nHere is a quick check I run to count the number of missing data in each column.\n\napply(var_trial, 2, function(x) sum(is.na(x)))\n\n                 block                  range                    row \n                     0                      0                      0 \n               variety              stand_pct days_to_heading_julian \n                     0                      0                      0 \n                height                lodging             yield_bu_a \n                     0                      0                      0 \n           test_weight \n                     0 \n\n\nAlas, no missing data!\nIf there were independent variables with a continuous distribution (a covariate), I would plot those data.\nLast, check the dependent variable. A histogram is often quite sufficient to accomplish this. This is designed to be a quick check, so no need to spend time making the plot look good.\n\nhist(var_trial$yield_bu_a, main = \"\", xlab = \"yield\")\n\n\n\n\n\n\n\n\nFigure 2.1: Histogram of the dependent variable.\n\n\n\n\nThe range is roughly falling into the range we expect. I know this from talking with the person who generated the data, not through my own intuition. I do not see any large spikes of points at a single value (indicating something odd), nor do I see any extreme values (low or high) that might indicate some larger problems.\nData are not expected to be normally distributed at this point, so don’t bother running any Shapiro-Wilk tests. This histogram is a check to ensure the the data are entered correctly and they appear valid. It requires a mixture of domain knowledge and statistical training to know this, but over time, if you look at these plots with regularity, you will gain a feel for what your data should look like at this stage.\nThese are not complicated checks. They are designed to be done quickly and should be done for every analysis if you not previously already inspected the data as thus. I do this before every analysis and often discover surprising things! Best to discover these things early, since they are likely to impact the final analysis.\nThis data set is ready for analysis!\n\n\n2.2.2 Model Building\n\n\nRecall the model:\n\\[y_{ij} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}\\]\nFor this model, \\(\\alpha_i\\) is the variety effect (fixed) and \\(\\beta_j\\) is the block effect (random).\nHere is the R syntax for the RCBD statistical model:\n\nlme4tidymodels\n\n\n\nmodel_rcbd &lt;- lmer(yield_bu_a ~ variety + (1|block),\n                   data = var_trial, \n                   na.action = na.exclude)\n\n\n\n\ntidy_rcbd &lt;- linear_reg() %&gt;% \n  set_engine(\"lmer\") %&gt;%\n  fit(yield_bu_a ~ variety + (1|block), data = var_trial, na.action = na.exclude)\n\n\n\n\nThe parentheses are used to indicate that ‘block’ is a random effect, and this particular notation (1|block) indicates that a ‘random intercept’ model is being fit. This is the most common approach. It means there is one overall effect fit for each block. I use the argument na.action = na.exclude as instruction for how to handle missing data: conduct the analysis, adjusting as needed for the missing data, and when prediction or residuals are output, please pad them in the appropriate places for missing data so they can be easily merged into the main data set if need be.\n\n\n\n\n\n\nFormula notation\n\n\n\nFormula notation is often used in the R syntax for linear models. It looks like this: \\(Y ~ X\\), where Y is the dependent variable (the response) and X is/are the independent variable(s) (e.g. the experimental treatments).\n\nmy_formula &lt;- formula(Y ~ treatment1 + treatment2)\nclass(my_formula)\n\n[1] \"formula\"\n\n\nThe package ‘lmer’ has some additional conventions regarding the formula. Random effects are put in parentheses and a 1| is used to denote random intercepts (rather than random slopes).\n\n\n\n\n2.2.3 Check Model Assumptions\nRemember those iid assumptions? Let’s make sure we actually met them.\nThere is a special plotting function written for lme4 object for checking the homoscedasticity (constant variance):\n\nplot(model_rcbd)\n\n\n\n\n\n\n\n\nWe are looking for a random and uniform distribution of points. This looks good!\nChecking normality requiring first extracting the model residuals with resid() and then generaing a qq-plot and line.\n\nqqnorm(resid(model_rcbd)); qqline(resid(model_rcbd))\n\n\n\n\n\n\n\n\nThis is reasonably good. Things do tend to fall apart at the tails.\n\n\n2.2.4 Inference\nEstimates for each treatment level can be obtained with the ‘emmeans’ package.\n\nrcbd_emm &lt;- emmeans(model_rcbd, ~ variety)\nas.data.frame(rcbd_emm) %&gt;% arrange(desc(emmean))\n\n variety                  emmean       SE    df  lower.CL upper.CL\n Rosalyn                155.2703 7.212203 77.85 140.91149 169.6292\n IDO1005                153.5919 7.212203 77.85 139.23310 167.9508\n OR2080641              152.6942 7.212203 77.85 138.33536 167.0530\n Bobtail                151.6403 7.212203 77.85 137.28149 165.9992\n UI Sparrow             151.6013 7.212203 77.85 137.24245 165.9601\n Kaseberg               150.9768 7.212203 77.85 136.61794 165.3356\n IDN-01-10704A          148.9861 7.212203 77.85 134.62729 163.3450\n 06-03303B              148.8300 7.212203 77.85 134.47116 163.1888\n WB1529                 148.2445 7.212203 77.85 133.88568 162.6034\n DAS003                 145.2000 7.212203 77.85 130.84116 159.5588\n IDN-02-29001A          144.5755 7.212203 77.85 130.21665 158.9343\n Bruneau                143.9900 7.212203 77.85 129.63116 158.3488\n SY 107                 143.6387 7.212203 77.85 129.27987 157.9975\n WB 528                 142.9752 7.212203 77.85 128.61633 157.3340\n OR2080637              141.7652 7.212203 77.85 127.40633 156.1240\n Jasper                 141.2968 7.212203 77.85 126.93794 155.6556\n UI Magic CLP           139.5403 7.212203 77.85 125.18149 153.8992\n Madsen                 139.2671 7.212203 77.85 124.90826 153.6259\n LCS Biancor            139.1110 7.212203 77.85 124.75213 153.4698\n SY  Ovation            138.6426 7.212203 77.85 124.28375 153.0014\n OR2090473              137.8229 7.212203 77.85 123.46407 152.1817\n Madsen / Eltan (50/50) 136.9642 7.212203 77.85 122.60536 151.3230\n UI-WSU Huffman         135.4810 7.212203 77.85 121.12213 149.8398\n Mary                   134.8564 7.212203 77.85 120.49762 149.2153\n Norwest Tandem         134.3490 7.212203 77.85 119.99020 148.7079\n Brundage               134.0758 7.212203 77.85 119.71697 148.4346\n IDO1004                132.5145 7.212203 77.85 118.15568 146.8733\n DAS004                 132.2413 7.212203 77.85 117.88245 146.6001\n Norwest Duet           132.0852 7.212203 77.85 117.72633 146.4440\n Eltan                  131.4606 7.212203 77.85 117.10181 145.8195\n LCS Artdeco            130.8361 7.212203 77.85 116.47729 145.1950\n UI Palouse             130.4848 7.212203 77.85 116.12600 144.8437\n LOR-978                130.4458 7.212203 77.85 116.08697 144.8046\n LCS Drive              128.7674 7.212203 77.85 114.40858 143.1262\n Stephens               127.1671 7.212203 77.85 112.80826 141.5259\n OR2100940              126.1523 7.212203 77.85 111.79342 140.5111\n UI Castle CLP          125.5277 7.212203 77.85 111.16891 139.8866\n WB1376 CLP             123.6932 7.212203 77.85 109.33439 138.0521\n LOR-833                122.7565 7.212203 77.85 108.39762 137.1153\n LOR-913                118.7752 7.212203 77.85 104.41633 133.1340\n WB 456                 118.4629 7.212203 77.85 104.10407 132.8217\n SY Assure              111.0468 7.212203 77.85  96.68794 125.4056\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\nThis table indicates the estimated marginal means (“emmean”, sometimes called “least squares means”), the standard error (“SE”) of those means, the degrees of freedom and the upper and lower bounds of the 95% confidence interval. As an additional step, the emmeans were sorted from largest to smallest.\nAt this point, the analysis goals have been met: we know the estimated means for each treatment and their rankings.\n\n\n2.2.5 Flotsam & Jetsam\nSometimes, researchers want to conduct an ANOVA or add the letters for indicating differences among treatments, even though we have reached the original goals of analysis. It is important to evaluate why you want to do these extra things, what extra information it will bring and what you plan to do with those results.\nRunning an ANOVA may increase or decrease confidence in the results, depending on what results. That is not at all what ANOVA is intended to do, nor is this what p-values can tell us!\nLabelling each treatment, especially when there are this many (42 in total), has its own perils. The biggest problem is that this creates a multiple testing problem: with 42 treatments, a total of 861 comparison are being run (=\\(42*(42-1)/2\\)), and then adjusted for multiple tests. With that many tests, a severe adjustment is likely and hence things that are different are not detected. With so many tests, it could be that there is an overall effect due to variety, but they all share the same letter!\nThe second problem is one of interpretation. Just because two treatments or varieties share a letter does not mean they are equivalent. It only means that they were not found to be different. A funny distinction, but alas. There is an entire branch of statistics, ‘equivalence testing’ devoted to just this topic - how to test if two things are actually the same. This involves the user declaring a maximum allowable numeric difference for a variable in order to determine if two items are statistically different or equivalent - something that these pairwise comparisons are not doing.\nIf you want to run ANOVA, it can be done quite easily:\n\nanova(model_rcbd)\n\nType III Analysis of Variance Table with Satterthwaite's method\n        Sum Sq Mean Sq NumDF DenDF F value    Pr(&gt;F)    \nvariety  18354  447.65    41   123  2.4528 8.017e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBut, please be thoughtful in your usage of it.\n\n\n\n\n\n\nna.action = na.exclude\n\n\n\nYou may have noticed the final argument for na.action in the model statement:\nmodel_rcbd &lt;- lmer(yield_bu_a ~ variety + (1|block),\n                   data = var_trial, \n                   na.action = na.exclude)\nI use the argument na.action = na.exclude as instruction for how to handle missing data: conduct the analysis, adjusting as needed for the missing data, and when prediction or residuals are output, please pad them in the appropriate places for missing data so they can be easily merged into the main data set if need be.\nSince there are no missing data, this step was not strictly necessary, but it’s a good habit to be in.\n\n\n\n\n2.2.6 ADD RCBD Repeated Measures Model\n\n\n2.2.7 Incomplete BLock design\nhttps://search.r-project.org/CRAN/refmans/agridat/html/burgueno.alpha.html",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Randomized Complete Block Design</span>"
    ]
  },
  {
    "objectID": "chapters/glmm.html",
    "href": "chapters/glmm.html",
    "title": "3  Generalized Linear Mixed Models",
    "section": "",
    "text": "3.1 Zero-inflated & hurdle models\nWhen data have a large number of zeros, that can skew the results very dramatically and are most certainly violating standard assumptions of linear models (constant variance, normality, iid). What is a large number? That depends (of course) but I suggest 15% to 60% of the total data being zeros is high. Anything more than 60% starts to be too high - and it begs the question if statistics are really needed to understand what is going on.\nAdditionally, the occurence of zero’s does matter. If there are all occurring for a particular treatment (e.g. a negative control), estimation is impossible for that treatment level and running a conditional analysis may be a better choice. This mean filtering the data set to the treatments that are not completely all zero’s and running the analysis as a condition of a limited number of treatments.\nZero-inflated models are currently best developed for count variable and less ammenable (although not impossible) for continuous variable. I find these models helpful for studies in plant pathology, entomology, etc when pathogen/disease/pest occurence is spotty.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generalized Linear Mixed Models</span>"
    ]
  },
  {
    "objectID": "chapters/glmm.html#zero-inflated-hurdle-models",
    "href": "chapters/glmm.html#zero-inflated-hurdle-models",
    "title": "3  Generalized Linear Mixed Models",
    "section": "",
    "text": "3.1.0.1 Zero-inflated versus hurdle models\nKeeping with this tutorial self-imposed rule, I will not go into theory, but really, you ought to read up on these models because they are complicated. The Wikipedia entries for zero-inflated and hurdle models are a good source for an introductory overview.\n\n\n3.1.1 Data import & preparation\n\ninsect_exp &lt;- read.csv(here::here(\"data\", \"insect_count_data_glmm.csv\")) %&gt;% \n  mutate(sampling_date = as.Date(sampling_date, format = \"%m/%d/%y\")) %&gt;% \n  mutate(Date = as.character(sampling_date), \n         block = as.character(block),\n         treatment = as.character(treatment))\n\n|———-|—————————————-|\n|plot | a unique number referring to each experimental unit |\n|treatment | 6 pesticide treatments (converted to a ) |\n|row | plot position for row |\n|col | plot positions for column or range |\n|block | the blocking unit (converted to character) |\n| insect_counts | response variable, number of insects counted |\n|sampling_date | dates when each experimental unit were evaluated for insect counts |\n|Date | sampling date converted to a character variable |\n\n\n3.1.2 Data integrity checks\nData types:\n\nstr(insect_exp)\n\n'data.frame':   270 obs. of  8 variables:\n $ plot         : int  101 102 103 104 201 202 203 204 301 302 ...\n $ treatment    : chr  \"2\" \"5\" \"1\" \"6\" ...\n $ row          : int  1 1 1 1 2 2 2 2 3 3 ...\n $ column       : int  1 2 3 4 1 2 3 4 1 2 ...\n $ block        : chr  \"1\" \"1\" \"1\" \"1\" ...\n $ insect_counts: int  4 1 0 0 0 0 2 1 2 1 ...\n $ sampling_date: Date, format: \"1988-06-17\" \"1988-06-17\" ...\n $ Date         : chr  \"1988-06-17\" \"1988-06-17\" \"1988-06-17\" \"1988-06-17\" ...\n\n\nData balance:\n\ntable(insect_exp$sampling_date, insect_exp$treatment)\n\n            \n             1 2 3 4 5 6\n  1988-06-17 5 5 5 5 5 5\n  1988-06-22 5 5 5 5 5 5\n  1988-06-27 5 5 5 5 5 5\n  1988-06-29 5 5 5 5 5 5\n  1988-07-06 5 5 5 5 5 5\n  1988-07-13 5 5 5 5 5 5\n  1988-07-21 5 5 5 5 5 5\n  1988-07-27 5 5 5 5 5 5\n  1988-08-03 5 5 5 5 5 5\n\n\nMissingness:\n\napply(insect_exp, 2, function(x) sum(is.na(x)))\n\n         plot     treatment           row        column         block \n            0             0             0             0             0 \ninsect_counts sampling_date          Date \n            0             0             0 \n\n\nData visualization:\nHistograms are often not helpful for zero-inflated data since the zero’s dominate the distribution. Stem-and-leaf plots can be better. It’s also helpful to count the total number of zero’s.\n\nstem(insect_exp$insect_counts)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n   0 | 00000000000000000000000000000000000000000000000000000000000000000000+137\n   1 | 01222333455566779\n   2 | 001122234577789\n   3 | 00013568\n   4 | 48\n   5 | 13\n   6 | 0125\n   7 | 6\n   8 | 45\n   9 | 4\n  10 | \n  11 | \n  12 | \n  13 | 5\n\nsum(insect_exp$insect_counts == 0)/nrow(insect_exp)\n\n[1] 0.4185185\n\n\nRoughly 42% of the data are zero’s. The remaining non-zero data look like it might folow a Poisson or negative binomial distribution.\n\n# all data\nggplot(insect_exp, aes(x = sampling_date, y = insect_counts, color = treatment, group = plot)) +\n  geom_point(size = 2) +\n  geom_line() +\n  ggtitle(\"all data\") + \n  theme_classic()\n\n\n\n\n\n\n\n# mean of each treatment\ninsect_exp %&gt;% group_by(sampling_date, treatment) %&gt;% \n  summarise(mean_counts = mean(insect_counts)) %&gt;% \n  ggplot(., aes(x = sampling_date, y = mean_counts, color = treatment)) +\n    geom_point(size = 2) +\n    geom_line() +\n    ggtitle(\"mean data\") + \n    theme_classic()\n\n\n\n\n\n\n\n\n\n\n3.1.3 Statistical modelling\nModel statement:\n\\[y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\delta_i + \\epsilon_{ijk}\\]\nwhere:\n\\(\\mu\\) = overall mean \\(\\alpha_i\\) = effect of the \\(i^{th}\\) pesticide treatment\n\\(\\beta_j\\) = effect of the \\(j^{th}\\) block \\(\\gamma_k\\) = effect of the \\(k^{th}\\) sampling date \\(\\delta_i\\) = effect of the \\(i^{th}\\) pesticide treatment on becoming non-zero\nand\n\n3.1.3.1 Model fitting\nAs mentioned, this is hard and often takes many tries.\n\nm1 = glmmTMB(\n  insect_counts ~ treatment + Date + ar1(Date + 0|plot) + (1|block),\n  ziformula = ~ treatment,\n  data = insect_exp, \n  na.action = na.exclude, \n  family = nbinom2)\n\nThis model is using the correlation structure for autoregressive correlated error terms, ar1(). There are several other specialized covariance structures implmented by glmmTMB. In general, repeated measures syntax follow this convention: (time + 0 | grouping).\nFitting glmm is hard. The glmmTMB writers have written some guidance on model fitting.\n\nm1\n\nFormula:          \ninsect_counts ~ treatment + Date + ar1(Date + 0 | plot) + (1 |      block)\nZero inflation:                 ~treatment\nData: insect_exp\n      AIC       BIC    logLik  df.resid \n1298.7328 1385.0949 -625.3664       246 \nRandom-effects (co)variances:\n\nConditional model:\n Groups Name           Std.Dev. Corr      \n plot   Date1988-06-17 0.7748   0.49 (ar1)\n block  (Intercept)    0.3333             \n\nNumber of obs: 270 / Conditional model: plot, 30; block, 5\n\nDispersion parameter for nbinom2 family (): 1.76 \n\nFixed Effects:\n\nConditional model:\n   (Intercept)      treatment2      treatment3      treatment4      treatment5  \n       2.39231        -0.04978        -1.53159        -2.75395        -2.50652  \n    treatment6  Date1988-06-22  Date1988-06-27  Date1988-06-29  Date1988-07-06  \n      -1.48975         0.24054         0.26618         0.62692         1.17067  \nDate1988-07-13  Date1988-07-21  Date1988-07-27  Date1988-08-03  \n       0.83442         0.19962        -0.96749        -1.11938  \n\nZero-inflation model:\n(Intercept)   treatment2   treatment3   treatment4   treatment5   treatment6  \n     -2.608       -1.200        1.568        2.607        1.542        2.134  \n\n\n\n\n3.1.3.2 Model diagnostics\nLook at residuals over space\n\ninsect_exp$model_resids &lt;- residuals(m1)\n\nggplot(insect_exp, aes(x = row, y = column, fill = model_resids)) +\n  geom_tile() + \n  facet_wrap(facets = vars(Date), nrow = 3, ncol = 3) + \n  scale_fill_viridis_c(direction = -1) + \n  theme_minimal()\n\n\n\n\n\n\n\n\nUse DHARMa to conduct residual tests\n\nsimulated_resids &lt;- simulateResiduals(m1)\ntestDispersion(simulated_resids)\n\n\n\n\n\n\n\n\n\n    DHARMa nonparametric dispersion test via sd of residuals fitted vs.\n    simulated\n\ndata:  simulationOutput\ndispersion = 0.35248, p-value = 0.408\nalternative hypothesis: two.sided\n\nplot(simulated_resids)\n\n\n\n\n\n\n\n\n\n\n3.1.3.3 Inference\nANOVA\nThe package car is needed to conduct ANOVA tests on glmmTMB object. It conducts a chi-square test rather than an F-test. These tend to be more sensitive than F-tests, resulting in lower p-values.\n\ncar::Anova(m1)\n\nWarning in printHypothesis(L, rhs, names(b)): one or more coefficients in the hypothesis include\n     arithmetic operators in their names;\n  the printed representation of the hypothesis will be omitted\n\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: insect_counts\n           Chisq Df Pr(&gt;Chisq)    \ntreatment 54.358  5  1.769e-10 ***\nDate      41.652  8  1.574e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nEstimates.\nglmmTMB is compatible with emmeans and effects.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generalized Linear Mixed Models</span>"
    ]
  },
  {
    "objectID": "chapters/special-conditions.html",
    "href": "chapters/special-conditions.html",
    "title": "4  Combining Scenarios",
    "section": "",
    "text": "4.1 Split plot with repeated measures\nNormally-distributed data",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Combining Scenarios</span>"
    ]
  },
  {
    "objectID": "chapters/special-conditions.html#split-plot-with-repeated-measures",
    "href": "chapters/special-conditions.html#split-plot-with-repeated-measures",
    "title": "4  Combining Scenarios",
    "section": "",
    "text": "4.1.0.1 Load data\nThis is an alfalfa study consisting of two treatments: irrigation level and planting mix with four reps. Each plot (the experimental unit) was harvested three times and the yield was measured.\n\nalfalfa &lt;- read.csv(here::here(\"data\", \"alfalfa2021_data.csv\"))\n\n|———-|—————————————-| | cut | a cutting (harvest) of alfalfa within a single growing season. This is a temporal unit for repeated measures analysis. There were three cuttings in total for that year and field. The dates are not known, so we cannot assume they are evenly spaced apart |\n|irrigation | main plot, irrigation treatment (“Full” or “Deficit”) |\n|plot | a number referring to each experimental unit |\n|block | the blocking unit |\n|yield | alfalfa yield (the response variable) |\n|row | plot position for row |\n|col | plot positions for column or range. |\nTwo new variables need to be created:\n\nblock: character version of the original ‘block’\ncut_num: integer version of ‘cut’. This is required by nlme::lme for specialized correlation structures. The numeric order of this variable matches the cut order.\n\n\nalfalfa_sp &lt;- alfalfa %&gt;% \n  mutate(block = as.character(block)) %&gt;% \n  mutate(cut_num = case_when(\n    cut == \"First\" ~ 1L,\n    cut == \"Second\" ~ 2L,\n    cut == \"Third\" ~ 3L,\n    is.na(cut) ~ NA_integer_)) \n\n\n\n4.1.0.2 Data integrity checks\nData type check:\n\nstr(alfalfa_sp)\n\n'data.frame':   240 obs. of  9 variables:\n $ cut       : chr  \"First\" \"First\" \"First\" \"First\" ...\n $ irrigation: chr  \"Full\" \"Full\" \"Full\" \"Full\" ...\n $ plot      : int  1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 ...\n $ block     : chr  \"1\" \"1\" \"1\" \"1\" ...\n $ mix       : chr  \"50A+50O\" \"75A+25O\" \"50A+50F\" \"75A+25M\" ...\n $ yield     : num  221 289 467 557 423 ...\n $ row       : int  1 1 1 1 1 2 2 2 2 2 ...\n $ col       : int  1 2 3 4 5 1 2 3 4 5 ...\n $ cut_num   : int  1 1 1 1 1 1 1 1 1 1 ...\n\n\nData balance check:\n\ntable(alfalfa_sp$irrigation, alfalfa_sp$mix, alfalfa_sp$cut)\n\n, ,  = First\n\n         \n          100A 50A+50F 50A+50F_AR 50A+50M 50A+50M_AR 50A+50O 50A+50O_AR 75A+25F\n  Deficit    4       4          4       4          4       4          4       4\n  Full       4       4          4       4          4       4          4       4\n         \n          75A+25M 75A+25O\n  Deficit       4       4\n  Full          4       4\n\n, ,  = Second\n\n         \n          100A 50A+50F 50A+50F_AR 50A+50M 50A+50M_AR 50A+50O 50A+50O_AR 75A+25F\n  Deficit    4       4          4       4          4       4          4       4\n  Full       4       4          4       4          4       4          4       4\n         \n          75A+25M 75A+25O\n  Deficit       4       4\n  Full          4       4\n\n, ,  = Third\n\n         \n          100A 50A+50F 50A+50F_AR 50A+50M 50A+50M_AR 50A+50O 50A+50O_AR 75A+25F\n  Deficit    4       4          4       4          4       4          4       4\n  Full       4       4          4       4          4       4          4       4\n         \n          75A+25M 75A+25O\n  Deficit       4       4\n  Full          4       4\n\n\nCheck missingness:\n\napply(alfalfa_sp, 2, function(x) sum(is.na(x)))\n\n       cut irrigation       plot      block        mix      yield        row \n         0          0          0          0          0          0          0 \n       col    cut_num \n         0          0 \n\n\nCheck dependent data:\n\nhist(alfalfa_sp$yield)\n\n\n\n\n\n\n\n\nThere are some very high values that we should keep an eye on.\nExperimental layout:\n\nalfalfa_sp %&gt;% filter(cut == \"First\") %&gt;% \n  ggplot(aes(x = col, y = row)) +\n    geom_raster(aes(fill = irrigation)) +\n    geom_tileborder(aes(group = 1, grp = block), lwd = 1.5) + \n    theme_classic()\n\n\n\n\n\n\n\n\n\n\n4.1.0.3 Data analysis\nModel statement:\n\\[y_{ijk} = \\mu + \\alpha_i+ \\beta_j + \\gamma_k + (\\alpha\\beta)_{ij} + (\\alpha\\gamma)_{ik} + (\\beta\\gamma)_{jk} + (\\alpha\\beta\\gamma)_{ijk} +\\epsilon_{ijk}\\]\nwhere\n\\(\\mu\\) = overall mean\n\\(\\alpha_i\\) = effect of the \\(i^{th}\\) irrigation treatment\n\\(\\beta_j\\) = effect of the \\(j^{th}\\) planting mix treatment\n\\(\\gamma_k\\) = effect of the \\(k^{th}\\) cutting\nAnd the remaining terms reflect two-way and three-way interactions.\nThe error terms are assumed to follow this distribution, \\(\\epsilon \\sim N(0, \\sigma)\\), and each plot is assumed to follow an auto-regressive correlation structure.\nThe starting model is very similar to the other split plot example in this guide where the main plot “irrigation” is nested with “block”. An additional level of nesting is used for “plot” since that is the experimental unit we are designating for the repeated measures term.\nI usually build the model in two steps: first the basic model is estimated, and next a correlation structure is added. This is not strictly needed; the model can be estimated in one step.\n\nm1 &lt;- lme(yield ~ mix*irrigation*cut,\n          random = ~ 1|block/irrigation/plot,\n          data = alfalfa_sp)\n\nSince we don’t know the temporal spacing for each cutting, a compound symmetry correlation structure will be used. This type assumes a single correlation across time. This has a starting value of 0.3 and this may chagne during the fitting process since fixed = FALSE.\n\ncorstr &lt;- corCompSymm(value = 0.3, \n                      form = ~ cut|block/irrigation/plot,\n                      fixed = FALSE)\n\nIt is required by nlme that two terms match after the “|” in the random and form arguments match exactly. The plot term is needed because this is the unit to calculate correlations at.\n\ncorstr &lt;- corCompSymm(value = 0.3, \n                      form = ~ cut|block/irrigation/plot,\n                      fixed = FALSE)\n\nm1 &lt;- lme(yield ~ mix*irrigation*cut,\n          random = ~ 1|block/irrigation/plot,\n          data = alfalfa_sp)\n\nUpdate the model with the correlation structure:\n\nm2 &lt;- update(m1, cor = corstr)\n\n\n\n4.1.0.4 Check model assumpotion\n\nplot(m2)\n\n\n\n\n\n\n\nqqnorm(m2, ~ resid(., type = \"p\"), abline = c(0, 1))\n\n\n\n\n\n\n\n\nThere are some very large outliers at the right side of the plot.\n\n\n4.1.0.5 Run ANOVA\n\nanova(m2)\n\n                   numDF denDF   F-value p-value\n(Intercept)            1   102 1432.6369  &lt;.0001\nmix                    9   102   13.6932  &lt;.0001\nirrigation             1     3    4.8770  0.1143\ncut                    2   102    6.0434  0.0033\nmix:irrigation         9   102    0.5256  0.8530\nmix:cut               18   102    0.8029  0.6927\nirrigation:cut         2   102   14.2649  &lt;.0001\nmix:irrigation:cut    18   102    1.0226  0.4418\n\n\nAlways check the degrees of freedom (denominator and numerator) to make sure the model is specified correctly.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Combining Scenarios</span>"
    ]
  },
  {
    "objectID": "chapters/analysis-tips.html",
    "href": "chapters/analysis-tips.html",
    "title": "5  Tips on Analysis",
    "section": "",
    "text": "5.0.0.1 Think about your analytical goals\nThroughout this guide, I have tried to explicitly state the goals of each analysis. This helps inform us on how to approach the analysis of an experiment. It can be difficult, especially for new scientists-in-training (i.e. graduate students), to understand what it is they want to estimate. You may have been handed a data set you had no role in generating and told to “analyze this” with no additional context. Or perhaps you may have conducted a large study that has some overall goals that are lofty, yet vague.\nIt can helpful to think about the exact results you are hoping to get. What does this look like exactly? Do you want to estimate the changes in plant diversity as the result of a herbicide spraying program? Do you want to find out if a fertilizer treatment changed protein content in a crop and by how much? Do you want to know about changes in human diet due to an intervention? What are quantifiable difference that you and/or experts in your domain would find meaningful?\nConsider what the results would look like on the best case scenario when your wildest dreams come true and null results, when you find out that your treatment or invention had no effect.\nBy “consider”, I mean: imagine the final plot or table, or summary sentence you want to present, either in a peer-reviewed manuscript, or some output for stakeholders. From this, you work backwards to determine the analytical approach needed to arrive at that final output. Or you may determine that your data are unsuitable to generate the desired output, in which case, it’s best to determine that as soon as possible.\nBy “consider”, I also mean: imagine exactly what the spreadsheet of results would say - what columns are present and what data are in the cells. If you are planning an experiment, this can help ensure you plan it properly to actually test whatever it is you want to evaluate. If the experiment is done, this enables you to evaluate if you have the information present to test your hypothesis.\nBy taking the time to reflect on what it is you exactly want to analyse, this can save time and prevent you from doing unneeded analyses that don’t serve this final goal. There is rarely (never?) one way to analyse an experiment or a data set, so use your limited time wisely and focus on what matters.\n\n\n5.0.0.2 Some reflections on p-values\n\nInformally, a p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value. -American Statistical Association\n\nThe great majority of researched are deeply interested in p-values. This is not a bad thing per se, but sometimes the focus is so strong it comes at the expense of other valuable pieces of information, like treatment estimates!\nThe American Statistics Association recommends that “Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.” That articles also contain explanations regarding what p-values are telling us and how to avoid committing analytical errors and/or misinterpreting p-values. If you have time to read the full article, it will benefit your research!\nThe main problematic behavior I see is researchers using p-values as the sole criteria on whether to present results: “We wanted to test if x, y and z had an effect. We ran some model and found that that only x had a significant effect, and those results indicate…” (results with a p-value &lt; 0.05 are ignored).\nA better option would be to discuss the the results of the analysis and how they addressed the research questions: how did the dependent variable change (or not change) as a result of the treatments/interventions/independent variables? What are the parameters or treatment predictions and what do they tell us with regard to the research goals? And to bolster those estimates, what are the confidence intervals on those estimates? What are the p-values for the statistical tests? P-values can support the results and conclusions, but the main results desired by a researcher are usually the estimates themselves - so lead with that!\n\n\n5.0.0.3 Data cleaning\nThis has and will continue to occupy the majority of researcher’s time when conducting an analysis. Truly, I am sorry for this. But, please know it is not you, it is the nature of data. I have written extensively about this elsewhere, but please prepare yourself to spend time cleaning and preparing your data for analysis. This will take way longer than the actual analysis! It is needed to ensure you can actually get correct results in an analysis, and hence worthy of the time invested.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tips on Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/summary.html",
    "href": "chapters/summary.html",
    "title": "6  Summary",
    "section": "",
    "text": "In summary, mixed models are complicated.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary</span>"
    ]
  }
]