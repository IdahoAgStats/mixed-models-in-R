[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Field Guide to the R Mixed Model Wilderness",
    "section": "",
    "text": "Preface\n“Path in the Wilderness” by Erich Taeubel, Jr.\nRunning mixed models in R is no easy task. There are dozens of packages supporting these aims, each with varying functionality, syntax, and conventions. The linear mixed model ecosystem in R consists of over 80 libraries that either construct and solve mixed model equations or helper packages the process the results from mixed model analysis. These libraries provide a patchwork of overlapping and unique functionality regarding the fundamental structure of mixed models: allowable distributions, nested and crossed random effects, heterogeneous error structures and other facets. No single library has all possible functionality enabled.\nThis patchwork of packages makes it very challenging for statisticians to conduct mixed model analysis and to teach others how to run mixed models in R. The purpose of this guide to to provide some recipes for handling common analytical scenario’s that require mixed models. As a field guide, it is intended to be succinct, and help researchers meet their analytic goals.\nIn general, the content from this website may not be copied or reproduced without attribution. However, the example code and required data sets to run the code are MIT licensed. These can be accessed on GitHub.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-this-does-not-cover",
    "href": "index.html#what-this-does-not-cover",
    "title": "Field Guide to the R Mixed Model Wilderness",
    "section": "What This Does Not Cover",
    "text": "What This Does Not Cover\n\nGeneralized linear models. We do address cases of unequal variance, but if another distribution and/or a link function is required for the model, that is not addressed in this guide.\nBasic principles of experimental design. We assume you know this, but if you do not, please check out the Grammar of Experimental Design.\nInstructions in using R. We assume familiarity with R. If you need help in learning R, there are numerous guides, including our introductory R course.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#notice",
    "href": "index.html#notice",
    "title": "Field Guide to the R Mixed Model Wilderness",
    "section": "Notice!",
    "text": "Notice!\nThis is a work-in-progress and will be updated over time.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/intro.html",
    "href": "chapters/intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Terms\nPlease read this section and refer back to if when you forget what these terms mean.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/intro.html#terms",
    "href": "chapters/intro.html#terms",
    "title": "1  Introduction",
    "section": "",
    "text": "Table 1.1: Terms definitions\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nRandom effect\nAn independent variable where the levels being estimated compose a random sample from a population whose variance will be estimated\n\n\nFixed effect\nAn independent variable with specific, predefined levels to estimate\n\n\nExperimental unit\nThe smallest unit being used for analysis. This could be an animal, a field plot, a person, a meat or muscle sample. The unit may be assessed multiple times or through multiple point in time. When the analysis is all said and done, the predictions occur at this level.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/intro.html#packages",
    "href": "chapters/intro.html#packages",
    "title": "1  Introduction",
    "section": "1.2 Packages",
    "text": "1.2 Packages\n\n1.2.1 Table of required packages for modelling\n\n\n\nTable 1.2: Table of required packages\n\n\n\n\n\nPackage\nPurpose\n\n\n\n\nlme4 (Bates et al. 2015)\nmain package for linear mixed models\n\n\nlmerTest (Kuznetsova, Brockhoff, and Christensen 2017)\nfor computing p-values when using lme4\n\n\nnlme (J. Pinheiro, Bates, and R Core Team 2023; J. C. Pinheiro and Bates 2000)\nmain package for linear mixed models and part of ‘base R’\n\n\nemmeans (Lenth 2022)\nfor estimating fixed effects, their confidence intervals and conducting contrasts\n\n\nbroom.mixed (Bolker and Robinson 2024)\npackage for presenting the model summary output into a tidy workflow.\n\n\nDHARMa (Hartig 2022)\nfor evaluating residuals (error terms) in generalized linear models\n\n\nperformance (Lüdecke et al. 2021)\nFor creating diagnostic plots or to compute fit measures\n\n\n\n\n\n\n\n\n1.2.2 Optional packages\n\n\n\nTable 1.3: Table of optional packages\n\n\n\n\n\nPackage Name\nFunction\n\n\nhere\nFor setting work directory\n\n\nggplot\nplotting\n\n\ndesplot\nplotting\n\n\nagridat\nto download example dataset\n\n\nagricolae\nto download example dataset\n\n\n\n\n\n\nThis entire guide will use the here package for loading data. If you can load your data fine without this package, please carry on. ‘here’ is certainly not required for running mixed models.\n\n\n\n\nBates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015. “Fitting Linear Mixed-Effects Models Using lme4.” Journal of Statistical Software 67 (1): 1–48. https://doi.org/10.18637/jss.v067.i01.\n\n\nBolker, Ben, and David Robinson. 2024. Broom.mixed: Tidying Methods for Mixed Models. https://CRAN.R-project.org/package=broom.mixed.\n\n\nHartig, Florian. 2022. DHARMa: Residual Diagnostics for Hierarchical (Multi-Level / Mixed) Regression Models. https://CRAN.R-project.org/package=DHARMa.\n\n\nKuznetsova, Alexandra, Per B. Brockhoff, and Rune H. B. Christensen. 2017. “lmerTest Package: Tests in Linear Mixed Effects Models.” Journal of Statistical Software 82 (13): 1–26. https://doi.org/10.18637/jss.v082.i13.\n\n\nLenth, Russell V. 2022. Emmeans: Estimated Marginal Means, Aka Least-Squares Means. https://CRAN.R-project.org/package=emmeans.\n\n\nLüdecke, Daniel, Mattan S. Ben-Shachar, Indrajeet Patil, Philip Waggoner, and Dominique Makowski. 2021. “performance: An R Package for Assessment, Comparison and Testing of Statistical Models.” Journal of Open Source Software 6 (60): 3139. https://doi.org/10.21105/joss.03139.\n\n\nPinheiro, José C., and Douglas M. Bates. 2000. Mixed-Effects Models in s and s-PLUS. New York: Springer. https://doi.org/10.1007/b98882.\n\n\nPinheiro, José, Douglas Bates, and R Core Team. 2023. Nlme: Linear and Nonlinear Mixed Effects Models. https://CRAN.R-project.org/package=nlme.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/analysis-tips.html",
    "href": "chapters/analysis-tips.html",
    "title": "2  Tips on Analysis",
    "section": "",
    "text": "Below are some things our office frequently says to researchers.\n\n2.0.1 Think About Your Analytical Goals\nThroughout this guide, we have tried to explicitly state the goals of each analysis. This helps informs how to approach the analysis of an experiment. It can be difficult, especially for new scientists-in-training (i.e. graduate students), to understand what it is they want to estimate. You may have been handed a data set you had no role in generating and told to “analyze this” with no additional context. Or perhaps you may have conducted a large study that has some overall goals that are lofty, yet vague.\nIt can helpful to think about the exact results you are hoping to get. What does this look like exactly? Do you want to estimate the changes in plant diversity as the result of a herbicide spraying program? Do you want to find out if a fertilizer treatment changed protein content in a crop and by how much? Do you want to know about changes in human diet due to an intervention? What are quantifiable difference that you and/or experts in your domain would find meaningful?\nConsider what the results would look like for (1) the best case scenario when your wildest dreams come true, and (2) null results, when you find out that your treatment or invention had no effect. It’s very helpful to understand and recognize both situations.\nBy “consider”, we mean: imagine the final plot or table, or summary sentence you want to present, either in a peer-reviewed manuscript, or some output for stakeholders. From this, you work backwards to determine the analytical approach needed to arrive at that final output. Or you may determine that your data are unsuitable to generate the desired output, in which case, it’s best to determine that as soon as possible.\nBy “consider”, we also mean: imagine exactly what the spreadsheet of results would say - what columns are present and what data are in the cells. If you are planning an experiment, this can help ensure you plan it properly to actually test whatever it is you want to evaluate. If the experiment is done, this enables you to evaluate if you have the information present to test your hypothesis.\nBy taking the time to reflect on what it is you exactly want to analyze, this can save time and prevent you from doing unneeded analyzes that don’t serve this final goal. There is rarely (never?) one way to analyze an experiment or a data set, so use your limited time wisely and focus on what matters.\n\n\n2.0.2 Know That Data Cleaning is Time Consuming\n\n\n\n\n\n\n\n\n\nFigure 2.1: How you will spend your time\n\n\n\n\nThis has and will continue to occupy the majority of researcher’s time when conducting an analysis. Truly, we are sorry for this. But, please know it is not you, it is the nature of data. Please plan for and prepare yourself mentally to spend time cleaning and preparing your data for analysis.1 This will take way longer than the actual analysis! It is needed to ensure you can actually get correct results in an analysis, and hence data cleaning is worth the time it requires.\n1 For an excellent set of basic instructions on data preparation, please see: Broman, K. W., & Woo, K. H. (2018). Data Organization in Spreadsheets. The American Statistician, 72(1), 2–10.\n\n2.0.3 Interpret ANOVA and P-values with Caution\n\nInformally, a p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value.\n---American Statistical Association\n\nThe great majority of researched are deeply interested in p-values. This is not a bad thing per se, but sometimes the focus is so strong it comes at the expense of other valuable pieces of information, like treatment estimates! Russ Leanth, author of the emmeans package refers to this particular practice as “star gazing”.\nIt is important to evaluate why you want to do ANOVA, what extra information it will bring and what you plan to do with those results. Sometimes, researchers want to conduct an ANOVA even though the original goals of analysis were reached without it. Running an ANOVA may increase or decrease confidence in your other results. That is not at all what ANOVA is intended to do, nor is this what p-values can tell us. ANOVA compares across group variation to within group variation. It cannot tell us if anything is the ‘same’ (there’s a separate branch of analysis, ‘equivalence testing’, for that), and it cannot tell us specifically what is different, unless you are fortunate enough to only have 2 levels in your treatment structure. P-values provide no guarantee that something is truly different or not; it only quantifies the probability you could have observed these results by chance.\nThe American Statistics Association recommends that “Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.”2 That article also explains what p-values are telling us and how to avoid committing analytical errors and/or misinterpreting p-values. If you have time to read the full article, it will benefit your research!\n2 Wasserstein, R. L., & Lazar, N. A. (2016). The ASA Statement on p-Values: Context, Process, and Purpose. The American Statistician, 70(2), 129–133.The main problematic behavior I see is researchers using p-values as the sole criteria on whether to present results: “We wanted to test if x, y and z had an effect. We ran some model and found that that only x had a significant effect, and those results indicate…” (while results with a p-value &gt; 0.05 are ignored).\nA better option would be to discuss the the results of the analysis and how they addressed the research questions: how did the dependent variable change (or not change) as a result of the treatments/interventions/independent variables? What are the parameters or treatment predictions and what do they tell us with regard to the research goals? And to bolster those estimates, what are the confidence intervals on those estimates? What are the p-values for the statistical tests? P-values can support the results and conclusions, but the main results desired by a researcher are usually the estimates themselves - so lead with that!\nTo learn more about common pitfalls in interpreting p-values, check out our blog post on the subject and/or this paper3 on the subject.\n3 Greenland S, Senn SJ, Rothman KJ, Carlin JB, Poole C, Goodman SN, Altman DG. (2016) Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations. Eur J Epidemiol. 31(4):337-50.\n\n2.0.4 Comments on Hypothesis Testing and Usage of Treatment Letters\nOften, I see researchers use compact letter display (e.g. “A”, “B”, “C”, ….) for indicating differences among treatments. This makes for concise presentation of results in tables and figures, but it can both kill statistical power and misses nuance in the results.\n\n\n\nImage from a paper published in 2024. Although this was a fully crossed factorial experiment, compact letter display was implemented across all treatment combinations, resulting in some nonsensical comparisons among some more informative contrasts.\nImplementing compact letter display can kill statistical power (the probability of detecting true differences) because it requires that all pairwise comparison being made. Doing this, especially when there are many treatment levels, has its perils. The biggest problem is that this creates a multiple testing problem. The RCBD example in this guide has 42 treatments, resulting in a total of 861 comparisons (\\(=42*(42-1)/2\\)), that are then adjusted for multiple tests. With that many tests, a severe adjustment is likely and hence things that are different are not detected. With so many tests, it could be that there is an overall effect due to treatment, but they all share the same letter!\nThe second problem is one of interpretation. Just because two treatments or varieties share a letter does not mean they are equivalent. It only means that they were not found to be different. A funny distinction, but alas. There is an entire branch of statistics, ‘equivalence testing’ devoted to just this topic - how to test if two things are actually the same. This involves the user declaring a maximum allowable numeric difference for a variable in order to determine if two items are statistically different or equivalent - something that these pairwise comparisons are not doing.]\nAnother problem is that doing all pairwise comparison may not align with experimental goals. In many circumstances, not every pairwise combination is of any interest or relevance to the study. Additionally, complex treatment structure may necessitate custom constrasts that highlight differences between the marginal estimate of multiple treatments versus another. For example, there may be 2 levels of ‘high’ nitrogen fertilizer treatment with two different sources (i.e. types of fertilizer). A researcher may want to contrast those two levels together against ‘low’ nitrogen treatment levels.\nOften, researchers have embedded additional structure in the treatments that is not fully reflected in the statistical model. For example, perhaps a study is looking at five different intercropping mixtures, two that incorporate a legume and 3 that do not. Conducting all pairwise comparisons with miss estimating the difference due to including a legume in an intercropping mix and not incorporating one. Soil fertility and other agronomic studies often have complex treatment structure. When it is not practical or financially feasible to have a full factorial experiment, embedding different treatment combinations in the main factor of analysis can accomplish this. This is a good study design approach, and we have statistical tools to analyze it.\n\n\n\n\nBroman, Karl W., and Kara H. Woo. 2018. “Data Organization in Spreadsheets.” The American Statistician 72 (1): 2–10. https://doi.org/10.1080/00031305.2017.1375989.\n\n\nGreenland, Sander, Stephen J. Senn, Kenneth J. Rothman, John B. Carlin, Charles Poole, Steven N. Goodman, and Douglas G. Altman. 2016. “Statistical Tests, P Values, Confidence Intervals, and Power: A Guide to Misinterpretations.” European Journal of Epidemiology 31 (4): 337–50. https://doi.org/10.1007/s10654-016-0149-3.\n\n\nWasserstein, Ronald L., and Nicole A. Lazar. 2016. “The ASA Statement on p-Values: Context, Process, and Purpose.” The American Statistician 70 (2): 129–33. https://doi.org/10.1080/00031305.2016.1154108.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tao of Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/background.html",
    "href": "chapters/background.html",
    "title": "3  Mixed model theory and background",
    "section": "",
    "text": "3.1 Model\nRecall simple linear regression with intercept (\\(\\beta_0\\)) and slope (\\(\\beta_1\\)) effect for subject i. The slope and intercept are chosen in a way so that the residual sum of squares is minimized.\n\\[  Y = \\beta_0 + \\beta_1 X + \\epsilon \\]\nIf we consider this model in a mixed model framework, \\(\\beta_0\\) and \\(\\beta_0\\) are considered fixed effects (also known as the population-averaged values) and \\(b_i\\) is a random effect for subject i. The random effect can be thought of as each subject’s deviation from the fixed intercept parameter. The key assumption about \\(b_i\\) is that it is independent, identically and normally distributed with a mean of zero and associated variance. Random effects are especially useful when we have (1) lots of levels (e.g., many species or blocks), (2) relatively little data on each level (although we need multiple samples from most of the levels), and (3) uneven sampling across levels.\nFor example, if we let the intercept be a random effect, it takes the form:\n\\[  Y = \\beta_0 + b_i + \\beta_1 X + \\epsilon \\]\nIn this model, predictions would vary depending on each subject’s random intercept term, but slopes would be the same.\nIn second case, we can have a fixed intercept and a random slope. The model will be:\n\\[  Y = \\beta_0 + (\\beta_1 + b_i)(X) + \\epsilon\\]\nIn this model, the bi is a random effect for subject i applied to the slope. Predictions would vary with random slope term, but the intercept will be the same:\nThird case would be the mixed model with random slope and intercept:\n\\[  Y = (\\beta_0 + a_i) + (\\beta_1 + b_i)(X) + \\epsilon\\]\nIn this model, \\(a_i\\) and \\(b_i\\) are random effects for subject i applied to the intercept and slope, respectively. Predictions would vary depending on each subject’s slope and intercept terms:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mixed Model Background</span>"
    ]
  },
  {
    "objectID": "chapters/background.html#model",
    "href": "chapters/background.html#model",
    "title": "3  Mixed model theory and background",
    "section": "",
    "text": "Example mixed model with random intercepts but identical slopes.\n\n\n\n\n\n\n\n\n\n\nMixed model with random slopes but identical intercepts.\n\n\n\n\n\n\n\n\n\n\nMixed Model with random intercept and slope",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mixed Model Background</span>"
    ]
  },
  {
    "objectID": "chapters/background.html#formula-notation",
    "href": "chapters/background.html#formula-notation",
    "title": "3  Mixed model theory and background",
    "section": "3.2 R Formula Syntax for Random and Fixed Effects",
    "text": "3.2 R Formula Syntax for Random and Fixed Effects\nFormula notation is often used in the R syntax for linear models. It looks like this: \\(Y ~ X\\), where Y is the dependent variable (the response) and X is/are the independent variable(s) (e.g. the experimental treatments).\n\nmy_formula &lt;- formula(Y ~ treatment1 + treatment2)\nclass(my_formula)\n\n[1] \"formula\"\n\n\nThe package ‘lme4’ has some additional conventions regarding the formula. Random effects are put in parentheses and a 1| is used to denote random intercepts (rather than random slopes). The table below provides several examples of random effects in mixed models. The names of grouping factors are denoted g, g1, and g2, and covariate as x.\n\n\n\n\n\n\n\n\nFormula\nAlternative\nMeaning\n\n\n\n\n(1 | g)\n1 + (1 | g)\nRandom intercept with a fixed mean\n\n\n(1 | g1/g2)\n(1 | g1) + (1 | g1:g2)\nIntercept varying among g1 and g2 within g1.\n\n\n(1 | g1) + (1 | g2)\n1 + (1 | g1) + (1| g2)\nIntercept varying among g1 and g2.\n\n\nx + (x | g)\n1 + x + (1 + x | g)\nCorrelated random intercept and slope\n\n\nx + (x || g)\n1 + x + (1 | g) + (0 + x | g)\nUncorrelated random intercept and slope.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mixed Model Background</span>"
    ]
  },
  {
    "objectID": "chapters/rcbd.html",
    "href": "chapters/rcbd.html",
    "title": "4  Randomized Complete Block Design",
    "section": "",
    "text": "4.1 Background\nThe statistical model:\n\\[y_{ij} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}\\] Where:\n\\(\\mu\\) = overall experimental mean \\(\\alpha\\) = treatment effects (fixed) \\(\\beta\\) = block effects (random) \\(\\epsilon\\) = error terms\n\\[ \\epsilon \\sim N(0, \\sigma)\\]\n\\[ \\beta \\sim N(0, \\sigma_b)\\]\nBoth the overall error and the block effects are assumed to be normally distributed with a mean of zero and standard deviations of \\(\\sigma\\) and \\(sigma_B\\), respectively.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Randomized Complete Block Design</span>"
    ]
  },
  {
    "objectID": "chapters/rcbd.html#background",
    "href": "chapters/rcbd.html#background",
    "title": "4  Randomized Complete Block Design",
    "section": "",
    "text": "‘iid’ assumption for error terms\n\n\n\nIn this model, the error terms, \\(\\epsilon\\) are assumed to be “iid”, that is, independently and identically distributed. This means they have constant variance and they each individual error term is independent from the others.\nThis guide will later address examples when this assumption is violated and how to handle it.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Randomized Complete Block Design</span>"
    ]
  },
  {
    "objectID": "chapters/rcbd.html#example-analysis",
    "href": "chapters/rcbd.html#example-analysis",
    "title": "4  Randomized Complete Block Design",
    "section": "4.2 Example Analysis",
    "text": "4.2 Example Analysis\nFirst, load the libraries for analysis and estimation:\n\nlme4nlme\n\n\n\nlibrary(lme4); library(lmerTest); library(emmeans)\nlibrary(dplyr); library(performance)\n\n\n\n\nlibrary(nlme); library(performance); library(emmeans)\nlibrary(dplyr)\n\n\n\n\nNext, let’s load some data. It is located here if you want to download it yourself (recommended).\nThis data set is for a single wheat variety trial conducted in Aberdeen, Idaho in 2015. The trial includes 4 blocks and 42 different treatments (wheat varieties in this case). This experiment consists of a series of plots (the experimental unit) laid out in a rectangular grid in a farm field. The goal of this analysis is the estimate the yield and test weight of each variety and the determine the rankings of each variety with regard to yield.\n\nvar_trial &lt;- read.csv(here::here(\"data\", \"aberdeen2015.csv\"))\n\n\nTable of variables in the data set\n\n\n\n\n\n\nblock\nblocking unit\n\n\nrange\ncolumn position for each plot\n\n\nrow\nrow position for each plot\n\n\nvariety\ncrop variety (the treatment) being evaluated\n\n\nstand_pct\npercentage of the plot with actual plants growing in them\n\n\ndays_to_heading_julian\nJulian days (starting January 1st) until plot “headed” (first spike emerged)\n\n\nlodging\npercentage of plants in the plot that fell down and hence could not be harvested\n\n\nyield_bu_a\nyield (bushels per acre)\n\n\n\nThere are several variables present that are not useful for this analysis. The only thing we are concerned about is block, variety, yield_bu_a, and test_weight.\n\n4.2.1 Data integrity checks\nThe first thing is to make sure the data is what we expect. There are two steps:\n\nmake sure data are the expected data type\ncheck the extent of missing data\ninspect the independent variables and make sure the expected levels are present in the data\ninspect the dependent variable to ensure its distribution is following expectations\n\n\nstr(var_trial)\n\n'data.frame':   168 obs. of  10 variables:\n $ block                 : int  4 4 4 4 4 4 4 4 4 4 ...\n $ range                 : int  1 1 1 1 1 1 1 1 1 1 ...\n $ row                   : int  1 2 3 4 5 6 7 8 9 10 ...\n $ variety               : chr  \"DAS004\" \"Kaseberg\" \"Bruneau\" \"OR2090473\" ...\n $ stand_pct             : int  100 98 96 100 98 100 100 100 99 100 ...\n $ days_to_heading_julian: int  149 146 149 146 146 151 145 145 146 146 ...\n $ height                : int  39 35 33 31 33 44 30 36 36 29 ...\n $ lodging               : int  0 0 0 0 0 0 0 0 0 0 ...\n $ yield_bu_a            : num  128 130 119 115 141 ...\n $ test_weight           : num  56.4 55 55.3 54.1 54.1 56.4 54.7 57.5 56.1 53.8 ...\n\n\nThese look okay except for block, which is currently coded as integer (numeric). We don’t want run a regression of block, where block 1 has twice the effect of block 2, and so on. So, converting it to a character will fix that. It can also be converted to a factor, but I find character easier to work with, and ultimately, equivalent to factor conversion\n\nvar_trial$block &lt;- as.character(var_trial$block)\n\nNext, check the independent variables. Running a cross tabulations is often sufficient to ascertain this.\n\ntable(var_trial$variety, var_trial$block)\n\n                        \n                         1 2 3 4\n  06-03303B              1 1 1 1\n  Bobtail                1 1 1 1\n  Brundage               1 1 1 1\n  Bruneau                1 1 1 1\n  DAS003                 1 1 1 1\n  DAS004                 1 1 1 1\n  Eltan                  1 1 1 1\n  IDN-01-10704A          1 1 1 1\n  IDN-02-29001A          1 1 1 1\n  IDO1004                1 1 1 1\n  IDO1005                1 1 1 1\n  Jasper                 1 1 1 1\n  Kaseberg               1 1 1 1\n  LCS Artdeco            1 1 1 1\n  LCS Biancor            1 1 1 1\n  LCS Drive              1 1 1 1\n  LOR-833                1 1 1 1\n  LOR-913                1 1 1 1\n  LOR-978                1 1 1 1\n  Madsen                 1 1 1 1\n  Madsen / Eltan (50/50) 1 1 1 1\n  Mary                   1 1 1 1\n  Norwest Duet           1 1 1 1\n  Norwest Tandem         1 1 1 1\n  OR2080637              1 1 1 1\n  OR2080641              1 1 1 1\n  OR2090473              1 1 1 1\n  OR2100940              1 1 1 1\n  Rosalyn                1 1 1 1\n  Stephens               1 1 1 1\n  SY  Ovation            1 1 1 1\n  SY 107                 1 1 1 1\n  SY Assure              1 1 1 1\n  UI Castle CLP          1 1 1 1\n  UI Magic CLP           1 1 1 1\n  UI Palouse             1 1 1 1\n  UI Sparrow             1 1 1 1\n  UI-WSU Huffman         1 1 1 1\n  WB 456                 1 1 1 1\n  WB 528                 1 1 1 1\n  WB1376 CLP             1 1 1 1\n  WB1529                 1 1 1 1\n\n\nThere are 42 varieties and there appears to be no misspellings among them that might confuse R into thinking varieties are different when they are actually the same. R is sensitive to case and white space, which can make it easy to create near duplicate treatments, such as “eltan” and “Eltan” and “Eltan”. There is no evidence of that in this data set. Additionally, it is perfectly balanced, with exactly one observation per treatment per rep. Please note that this does not tell us anything about the extent of missing data.\n\n\n\n\n\n\nMissing Data\n\n\n\nHere is a quick check to count the number of missing data in each column. This is not neededfor the data sets in this tutorial that have already been comprehensively examined, but it is helpful to check that the level of missingness displayed in an R session is what you expect.\n\napply(var_trial, 2, function(x) sum(is.na(x)))\n\n                 block                  range                    row \n                     0                      0                      0 \n               variety              stand_pct days_to_heading_julian \n                     0                      0                      0 \n                height                lodging             yield_bu_a \n                     0                      0                      0 \n           test_weight \n                     0 \n\n\nAlas, no missing data!\n\n\nIf there were independent variables with a continuous distribution (a covariate), I would plot those data.\nLast, check the dependent variable. A histogram is often quite sufficient to accomplish this. This is designed to be a quick check, so no need to spend time making the plot look good.\n\n\n\n\n\n\n\n\n\nFigure 4.1: Histogram of the dependent variable.\n\n\n\n\n\nhist(var_trial$yield_bu_a, main = \"\", xlab = \"yield\")\n\nThe range is roughly falling into the range we expect. I know this from talking with the person who generated the data, not through my own intuition. I do not see any large spikes of points at a single value (indicating something odd), nor do I see any extreme values (low or high) that might indicate some larger problems.\nData are not expected to be normally distributed at this point, so don’t bother running any Shapiro-Wilk tests. This histogram is a check to ensure the the data are entered correctly and they appear valid. It requires a mixture of domain knowledge and statistical training to know this, but over time, if you look at these plots with regularity, you will gain a feel for what your data should look like at this stage.\nThese are not complicated checks. They are designed to be done quickly and should be done for every analysis if you not previously already inspected the data as thus. We do this before every analysis and often discover surprising things! Best to discover these things early, since they are likely to impact the final analysis.\nThis data set is ready for analysis!\n\n\n4.2.2 Model Building\n\n\nRecall the model:\n\\[y_{ij} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}\\]\nFor this model, \\(\\alpha_i\\) is the variety effect (fixed) and \\(\\beta_j\\) is the block effect (random).\nHere is the R syntax for the RCBD statistical model:\n\nlme4nlme\n\n\n\nmodel_rcbd_lmer &lt;- lmer(yield_bu_a ~ variety + (1|block),\n                   data = var_trial, \n                   na.action = na.exclude)\n\n\n\n\nmodel_rcbd_lme &lt;- lme(yield_bu_a ~ variety,\n                  random = ~ 1|block,\n                  data = var_trial, \n                  na.action = na.exclude)\n\n\n\n\nThe parentheses are used to indicate that ‘block’ is a random effect, and this particular notation (1|block) indicates that a ‘random intercept’ model is being fit. This is the most common approach. It means there is one overall effect fit for each block. I use the argument na.action = na.exclude as instruction for how to handle missing data: conduct the analysis, adjusting as needed for the missing data, and when prediction or residuals are output, please pad them in the appropriate places for missing data so they can be easily merged into the main data set if need be.\n\n\n4.2.3 Check Model Assumptions\n\n\nR syntax for checking model assumptions is the same for lme4 and nlme.\nRemember those iid assumptions? Let’s make sure we actually met them.\n\n4.2.3.1 Old Way\nThere are special plotting function written for lme4 and nlme objects (ie.plot(lmer_object)) for checking the homoscedasticity (constant variance).\n\n\n\n\n\n\n\n\n\nFigure 4.2: Plot of residuals versus fitted values\n\n\n\n\n\nplot(model_rcbd_lmer, resid(., scaled=TRUE) ~ fitted(.), \n     xlab = \"fitted values\", ylab = \"studentized residuals\")\n\nWe are looking for a random and uniform distribution of points. This looks good!\nChecking normality requiring first extracting the model residuals with resid() and then generating a qq-plot and line.\n\n\n\n\n\n\n\n\n\nFigure 4.3: QQ-plot of residuals\n\n\n\n\n\nqqnorm(resid(model_rcbd_lmer), main = NULL); qqline(resid(model_rcbd_lmer))\n\nThis is reasonably good. Things do tend to fall apart at the tails.\n\n\n4.2.3.2 New Way\nNowadays, we can take advantage of the performance package, which provides a comprehensive suite of diagnostic plots.\n\n\nPlease look for check_model() in help tab to find what other checks you can perform using this function. If you would like to check all assumptions you can use check = “all”.\n\ncheck_model(model_rcbd_lmer, check = c('normality', 'linearity'))\n\n\n\n\n\n\n\n\n\n\n\n4.2.4 Inference\n\n\nR syntax for esimating model marginal means is the same for lme4 and nlme.\nEstimates for each treatment level can be obtained with the ‘emmeans’ package.\n\nrcbd_emm &lt;- emmeans(model_rcbd_lmer, ~ variety)\nas.data.frame(rcbd_emm) %&gt;% arrange(desc(emmean))\n\n variety                  emmean       SE    df  lower.CL upper.CL\n Rosalyn                155.2703 7.212203 77.85 140.91149 169.6292\n IDO1005                153.5919 7.212203 77.85 139.23310 167.9508\n OR2080641              152.6942 7.212203 77.85 138.33536 167.0530\n Bobtail                151.6403 7.212203 77.85 137.28149 165.9992\n UI Sparrow             151.6013 7.212203 77.85 137.24245 165.9601\n Kaseberg               150.9768 7.212203 77.85 136.61794 165.3356\n IDN-01-10704A          148.9861 7.212203 77.85 134.62729 163.3450\n 06-03303B              148.8300 7.212203 77.85 134.47116 163.1888\n WB1529                 148.2445 7.212203 77.85 133.88568 162.6034\n DAS003                 145.2000 7.212203 77.85 130.84116 159.5588\n IDN-02-29001A          144.5755 7.212203 77.85 130.21665 158.9343\n Bruneau                143.9900 7.212203 77.85 129.63116 158.3488\n SY 107                 143.6387 7.212203 77.85 129.27987 157.9975\n WB 528                 142.9752 7.212203 77.85 128.61633 157.3340\n OR2080637              141.7652 7.212203 77.85 127.40633 156.1240\n Jasper                 141.2968 7.212203 77.85 126.93794 155.6556\n UI Magic CLP           139.5403 7.212203 77.85 125.18149 153.8992\n Madsen                 139.2671 7.212203 77.85 124.90826 153.6259\n LCS Biancor            139.1110 7.212203 77.85 124.75213 153.4698\n SY  Ovation            138.6426 7.212203 77.85 124.28375 153.0014\n OR2090473              137.8229 7.212203 77.85 123.46407 152.1817\n Madsen / Eltan (50/50) 136.9642 7.212203 77.85 122.60536 151.3230\n UI-WSU Huffman         135.4810 7.212203 77.85 121.12213 149.8398\n Mary                   134.8564 7.212203 77.85 120.49762 149.2153\n Norwest Tandem         134.3490 7.212203 77.85 119.99020 148.7079\n Brundage               134.0758 7.212203 77.85 119.71697 148.4346\n IDO1004                132.5145 7.212203 77.85 118.15568 146.8733\n DAS004                 132.2413 7.212203 77.85 117.88245 146.6001\n Norwest Duet           132.0852 7.212203 77.85 117.72633 146.4440\n Eltan                  131.4606 7.212203 77.85 117.10181 145.8195\n LCS Artdeco            130.8361 7.212203 77.85 116.47729 145.1950\n UI Palouse             130.4848 7.212203 77.85 116.12600 144.8437\n LOR-978                130.4458 7.212203 77.85 116.08697 144.8046\n LCS Drive              128.7674 7.212203 77.85 114.40858 143.1262\n Stephens               127.1671 7.212203 77.85 112.80826 141.5259\n OR2100940              126.1523 7.212203 77.85 111.79342 140.5111\n UI Castle CLP          125.5277 7.212203 77.85 111.16891 139.8866\n WB1376 CLP             123.6932 7.212203 77.85 109.33439 138.0521\n LOR-833                122.7565 7.212203 77.85 108.39762 137.1153\n LOR-913                118.7752 7.212203 77.85 104.41633 133.1340\n WB 456                 118.4629 7.212203 77.85 104.10407 132.8217\n SY Assure              111.0468 7.212203 77.85  96.68794 125.4056\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\nThis table indicates the estimated marginal means (“emmeans”, sometimes called “least squares means”), the standard error (“SE”) of those means, the degrees of freedom and the upper and lower bounds of the 95% confidence interval. As an additional step, the emmeans were sorted from largest to smallest.\nAt this point, the analysis goals have been met: we know the estimated means for each treatment and their rankings.\nIf you want to run ANOVA, it can be done quite easily. By default, the Kenward-Rogers method of degrees of freedom approximation is used.\n\n\nThe Type I method is sometimes referred to as the “sequential” sum of squares, because it involves a process of adding terms to the model one at a time. Type I sum of squares is the default hypothesis testing method used by the anova() function.\n\nlme4nlme\n\n\n\nanova(model_rcbd_lmer, type = \"1\")\n\nType I Analysis of Variance Table with Satterthwaite's method\n        Sum Sq Mean Sq NumDF DenDF F value    Pr(&gt;F)    \nvariety  18354  447.65    41   123  2.4528 8.017e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nanova(model_rcbd_lme, type = \"sequential\")\n\n            numDF denDF   F-value p-value\n(Intercept)     1   123 2514.1283  &lt;.0001\nvariety        41   123    2.4528   1e-04\n\n\n\n\n\n\n\n\n\n\n\nna.action = na.exclude\n\n\n\nYou may have noticed the final argument for na.action in the model statement:\nmodel_rcbd_lmer &lt;- lmer(yield_bu_a ~ variety + (1|block),\n                   data = var_trial, \n                   na.action = na.exclude)\nThe argument na.action = na.exclude provides instructions for how to handle missing data. na.exclude removes the missing data points before proceeding with the analysis. When any obervation-levels model outputs is generated (e.g. predictions, residuals), they are padded in the appropriate place to account for missing data. This is handy because it makes it easier to add those results to the original data set if so desired.\nSince there are no missing data, this step was not strictly necessary, but it’s a good habit to be in.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Randomized Complete Block Design</span>"
    ]
  },
  {
    "objectID": "chapters/factorial-design.html",
    "href": "chapters/factorial-design.html",
    "title": "5  RCBD Design with Several Crossed Factors",
    "section": "",
    "text": "5.1 Background\nFactorial design involves studying the impact of multiple factors simultaneously. Each factor can have multiple levels, and combinations of these levels form the experimental conditions. This design allows us to understand the main effects of individual factors and their interactions on the response variable. The statistical model for factorial design is: \\[y_{ij} = \\mu +  \\tau_i+ \\beta_j + \\tau_i\\beta_j + \\epsilon_{ij}\\] Where: \\(\\mu\\) = experiment mean, \\(\\tau\\) = effect of factor A, \\(\\beta\\) = effect of factor B, and \\(\\tau\\beta\\) = interaction effect of factor A and B.\nAssumptions of this model includes: independent and identically distributed error terms with a constant variance.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Factorial RCBD Design</span>"
    ]
  },
  {
    "objectID": "chapters/factorial-design.html#example-analysis",
    "href": "chapters/factorial-design.html#example-analysis",
    "title": "5  RCBD Design with Several Crossed Factors",
    "section": "5.2 Example Analysis",
    "text": "5.2 Example Analysis\nFirst step is to load the libraries required for the analysis:\n\nlme4nlme\n\n\n\nlibrary(lme4); library(lmerTest); library(emmeans)\nlibrary(dplyr); library(broom.mixed); library(performance)\n\n\n\n\nlibrary(nlme); library(broom.mixed); library(emmeans)\nlibrary(dplyr); library(performance)\n\n\n\n\nNext, we will load the dataset named ‘cochran.factorial’ from the ‘agridat’ package. This data comprises a yield response of beans to different levels of manure (d), nitrogen (n), phosphorus The goal of this analysis is the estimate the effect on d, n, p, k, and their interactions on bean yield.\nNote, while importing the data, d, n, p, and k were converted into factor variables using the mutate() function from dplyr package. This helps in reducing the extra steps of converting each single variable to factor manually.\n\nlibrary(agridat)\ndata1 &lt;- agridat::cochran.factorial %&gt;% \n  mutate(d = as.factor(d),\n         n = as.factor(n),\n         p = as.factor(p),\n         k = as.factor(k))\n\n\nTable of variables in the data set\n\n\nblock\nblocking unit\n\n\nrep\nreplication unit\n\n\ntrt\ntreatment factor, 16 levels\n\n\nd\ndung treatment, 2 levels\n\n\nn\nnitrogen treatment, 2 levels\n\n\np\nphosphorus treatment, 2 levels\n\n\nk\npotassium treatment, 2 levels\n\n\nyield\nyield (lbs)\n\n\n\nThe objective of this example is evaluate the individual and interactive effect of “d”, “n”, “p”, and “k” treatments on yield.\n\n5.2.1 Data Integrity Checks\nVerify the class of variables, where rep, block, d, n, p, and k are supposed to be a factor/character and yield should be numeric/integer.\n\nstr(data1)\n\n'data.frame':   32 obs. of  8 variables:\n $ rep  : Factor w/ 2 levels \"R1\",\"R2\": 1 1 1 1 1 1 1 1 1 1 ...\n $ block: Factor w/ 2 levels \"B1\",\"B2\": 1 1 1 1 1 1 1 1 2 2 ...\n $ trt  : Factor w/ 16 levels \"(1)\",\"d\",\"dk\",..: 15 10 2 14 5 6 9 11 8 12 ...\n $ yield: int  45 55 53 36 41 48 55 42 50 44 ...\n $ d    : Factor w/ 2 levels \"0\",\"1\": 2 2 1 2 1 1 1 2 1 2 ...\n $ n    : Factor w/ 2 levels \"0\",\"1\": 2 2 2 1 1 1 2 1 2 1 ...\n $ p    : Factor w/ 2 levels \"0\",\"1\": 1 2 2 1 2 1 1 2 1 2 ...\n $ k    : Factor w/ 2 levels \"0\",\"1\": 2 1 2 1 1 2 1 2 2 1 ...\n\n\nThis looks good.\nNext step is to inspect the independent variables and make sure the expected levels are present in the data.\n\ntable(data1$d, data1$n, data1$p, data1$k)\n\n, ,  = 0,  = 0\n\n   \n    0 1\n  0 2 2\n  1 2 2\n\n, ,  = 1,  = 0\n\n   \n    0 1\n  0 2 2\n  1 2 2\n\n, ,  = 0,  = 1\n\n   \n    0 1\n  0 2 2\n  1 2 2\n\n, ,  = 1,  = 1\n\n   \n    0 1\n  0 2 2\n  1 2 2\n\n\nThe design looks well balanced.\nLast step is to inspect the dependent variable to ensure its distribution follows the bell-shaped curve and no skewness is there.\n\n\n\n\n\n\n\n\n\nFigure 5.1: Histogram of the dependent variable.\n\n\n\n\n\nhist(data1$yield)\n\nThe range is roughly falling into the expected range. I didn’t observe any extreme observations (too high/low), indicating no issues with data. don’t see\n\n\n5.2.2 Model fitting\nModel fitting with R is exactly the same as shown in previous chapters: we need to include all effect, as well as the interaction, which is represented by using the colon indicator ‘:’. Therefore, model syntax is:\nyield ~ d + n + p + k + d:n + d:p + d:k + n:p + n:k + p:k + d:n:p:k\nwhich can be abbreviated as:\nyield ~ d*n*p*k\n\nlme4nlme\n\n\n\nmodel1_lmer &lt;- lmer(yield ~ d*n*p*k + (1|block),\n                   data = data1, \n                   na.action = na.exclude)\ntidy(model1_lmer)\n\n# A tibble: 18 × 8\n   effect   group    term           estimate std.error statistic    df   p.value\n   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1 fixed    &lt;NA&gt;     (Intercept)      49          3.70   13.2     16.0  4.91e-10\n 2 fixed    &lt;NA&gt;     d1               -9.5        5.24   -1.81    16.0  8.84e- 2\n 3 fixed    &lt;NA&gt;     n1                0.500      5.24    0.0955  16.0  9.25e- 1\n 4 fixed    &lt;NA&gt;     p1              -11.5        5.24   -2.20    16.0  4.31e- 2\n 5 fixed    &lt;NA&gt;     k1                1.00       5.24    0.191   16.0  8.51e- 1\n 6 fixed    &lt;NA&gt;     d1:n1            13.5        7.82    1.73    16.0  1.03e- 1\n 7 fixed    &lt;NA&gt;     d1:p1            15.5        7.82    1.98    16.0  6.49e- 2\n 8 fixed    &lt;NA&gt;     n1:p1             9.50       7.82    1.22    16.0  2.42e- 1\n 9 fixed    &lt;NA&gt;     d1:k1             4.00       7.82    0.512   16.0  6.16e- 1\n10 fixed    &lt;NA&gt;     n1:k1             0.500      7.82    0.0639  16.0  9.50e- 1\n11 fixed    &lt;NA&gt;     p1:k1             3.00       7.82    0.384   16.0  7.06e- 1\n12 fixed    &lt;NA&gt;     d1:n1:p1        -14.5       12.1    -1.19    16.0  2.50e- 1\n13 fixed    &lt;NA&gt;     d1:n1:k1        -17.0       12.1    -1.40    16.0  1.81e- 1\n14 fixed    &lt;NA&gt;     d1:p1:k1         -7.00      12.1    -0.576   16.0  5.72e- 1\n15 fixed    &lt;NA&gt;     n1:p1:k1         -4.50      12.1    -0.370   16.0  7.16e- 1\n16 fixed    &lt;NA&gt;     d1:n1:p1:k1      25.0       19.9     1.26    16.0  2.27e- 1\n17 ran_pars block    sd__(Intercep…    1.26      NA      NA       NA   NA       \n18 ran_pars Residual sd__Observati…    4.92      NA      NA       NA   NA       \n\n\n\n\n\nmodel2_lme &lt;- lme(yield ~ d*n*p*k,\n              random = ~ 1|block,\n              data = data1, \n              na.action = na.exclude)\ntidy(model2_lme)\n\n# A tibble: 18 × 8\n   effect   group    term           estimate std.error    df statistic   p.value\n   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 fixed    &lt;NA&gt;     (Intercept)      49          4.79    15   10.2      3.66e-8\n 2 fixed    &lt;NA&gt;     d1               -9.5        6.77    15   -1.40     1.81e-1\n 3 fixed    &lt;NA&gt;     n1                0.500      6.77    15    0.0739   9.42e-1\n 4 fixed    &lt;NA&gt;     p1              -11.5        6.77    15   -1.70     1.10e-1\n 5 fixed    &lt;NA&gt;     k1                1.00       6.77    15    0.148    8.85e-1\n 6 fixed    &lt;NA&gt;     d1:n1            13.5       11.6     15    1.16     2.63e-1\n 7 fixed    &lt;NA&gt;     d1:p1            15.5       11.6     15    1.34     2.02e-1\n 8 fixed    &lt;NA&gt;     n1:p1             9.50      11.6     15    0.818    4.26e-1\n 9 fixed    &lt;NA&gt;     d1:k1             4.00      11.6     15    0.345    7.35e-1\n10 fixed    &lt;NA&gt;     n1:k1             0.500     11.6     15    0.0431   9.66e-1\n11 fixed    &lt;NA&gt;     p1:k1             3.00      11.6     15    0.258    8.00e-1\n12 fixed    &lt;NA&gt;     d1:n1:p1        -14.5       21.0     15   -0.690    5.01e-1\n13 fixed    &lt;NA&gt;     d1:n1:k1        -17.0       21.0     15   -0.809    4.31e-1\n14 fixed    &lt;NA&gt;     d1:p1:k1         -7.00      21.0     15   -0.333    7.44e-1\n15 fixed    &lt;NA&gt;     n1:p1:k1         -4.50      21.0     15   -0.214    8.33e-1\n16 fixed    &lt;NA&gt;     d1:n1:p1:k1      25.0       39.7     15    0.630    5.38e-1\n17 ran_pars block    sd_(Intercept)    3.28      NA       NA   NA       NA      \n18 ran_pars Residual sd_Observation    4.92      NA       NA   NA       NA      \n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nInstead of summary() function, we used tidy() function from ‘broom.mixed’ package to get a short summary output of the model.\n\n\n\n\n5.2.3 Check Model Assumptions\n\nlme4nlme\n\n\n\ncheck_model(model1_lmer, check = c('normality', 'linearity'))\n\n\n\n\n\n\n\n\n\n\n\ncheck_model(model2_lme, check = c('normality', 'linearity'))\n\n\n\n\n\n\n\n\n\n\n\nThe linearity and homogeneity of variance plots show no trend. The normal Q-Q plots for the overall residuals and for the random effects all fall nearly on a straight line so we can be satisfied with that.\n\n\n5.2.4 Inference\nWe can get an ANOVA table for the linear mixed model using the function anova(), which works for both lmer() and lme() models..\n\nlme4nlme\n\n\n\ncar::Anova(model1_lmer, type = 'III', test.statistic=\"F\")\n\nAnalysis of Deviance Table (Type III Wald F tests with Kenward-Roger df)\n\nResponse: yield\n                   F Df Df.res    Pr(&gt;F)    \n(Intercept) 175.2030  1 20.439 1.729e-11 ***\nd             3.2928  1 20.439   0.08429 .  \nn             0.0091  1 20.439   0.92484    \np             4.8252  1 20.439   0.03974 *  \nk             0.0365  1 20.439   0.85040    \nd:n           2.9812  1 25.421   0.09637 .  \nd:p           3.9300  1 25.421   0.05834 .  \nn:p           1.4763  1 25.421   0.23552    \nd:k           0.2617  1 25.421   0.61335    \nn:k           0.0041  1 25.421   0.94951    \np:k           0.1472  1 25.421   0.70440    \nd:n:p         1.4251  1 37.012   0.24016    \nd:n:k         1.9589  1 37.012   0.16996    \nd:p:k         0.3321  1 37.012   0.56789    \nn:p:k         0.1373  1 37.012   0.71313    \nd:n:p:k       1.5778  1 66.709   0.21346    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nanova(model2_lme, type = \"marginal\")\n\n            numDF denDF   F-value p-value\n(Intercept)     1    15 104.83445  &lt;.0001\nd               1    15   1.97029  0.1808\nn               1    15   0.00546  0.9421\np               1    15   2.88720  0.1099\nk               1    15   0.02183  0.8845\nd:n             1    15   1.35278  0.2630\nd:p             1    15   1.78330  0.2017\nn:p             1    15   0.66990  0.4259\nd:k             1    15   0.11876  0.7352\nn:k             1    15   0.00186  0.9662\np:k             1    15   0.06680  0.7996\nd:n:p           1    15   0.47580  0.5009\nd:n:k           1    15   0.65401  0.4313\nd:p:k           1    15   0.11089  0.7437\nn:p:k           1    15   0.04583  0.8334\nd:n:p:k         1    15   0.39719  0.5380\n\n\n\n\n\nLet’s find estimates for some of the factors such as n, p, and n:k interaction. We will try the random intercept model first.\n\nlme4nlme\n\n\n\nemmeans(model1_lmer, specs = ~ n)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n n emmean   SE df lower.CL upper.CL\n 0   43.8 1.52 37     40.7     46.8\n 1   50.1 1.52 37     47.0     53.2\n\nResults are averaged over the levels of: d, p, k \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nemmeans(model1_lmer, specs = ~ p)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n p emmean   SE df lower.CL upper.CL\n 0   47.4 1.52 37     44.3     50.5\n 1   46.5 1.52 37     43.4     49.6\n\nResults are averaged over the levels of: d, n, k \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nemmeans(model1_lmer, specs = ~ n:k)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n n k emmean   SE   df lower.CL upper.CL\n 0 0   42.4 1.95 25.4     38.4     46.4\n 1 0   50.8 1.95 25.4     46.7     54.8\n 0 1   45.1 1.95 25.4     41.1     49.1\n 1 1   49.5 1.95 25.4     45.5     53.5\n\nResults are averaged over the levels of: d, p \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\n\n\nemmeans(model2_lme, specs = ~ n)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n n emmean   SE df lower.CL upper.CL\n 0   43.8 2.63  1     10.4     77.1\n 1   50.1 2.63  1     16.7     83.5\n\nResults are averaged over the levels of: d, p, k \nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\nemmeans(model2_lme, specs = ~ p)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n p emmean   SE df lower.CL upper.CL\n 0   47.4 2.63  1     14.0     80.8\n 1   46.5 2.63  1     13.1     79.9\n\nResults are averaged over the levels of: d, n, k \nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\nemmeans(model2_lme, specs = ~ n:k)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n n k emmean  SE df lower.CL upper.CL\n 0 0   42.4 2.9  1     5.50     79.2\n 1 0   50.8 2.9  1    13.88     87.6\n 0 1   45.1 2.9  1     8.25     82.0\n 1 1   49.5 2.9  1    12.63     86.4\n\nResults are averaged over the levels of: d, p \nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\n\n\n\n\nUnbalanced factorial design",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Factorial RCBD Design</span>"
    ]
  },
  {
    "objectID": "chapters/split-plot-design.html",
    "href": "chapters/split-plot-design.html",
    "title": "6  Split Plot Design",
    "section": "",
    "text": "6.1 Details for Split Plot Designs\nThe statistical model structure this design:\n\\[y_{ijk} = \\mu + \\alpha_i + \\beta_k + (\\alpha_j\\beta_k) + \\epsilon_{ij} + \\delta_{ijk} \\] Where:\n\\(\\mu\\)= overall experimental mean, \\(\\alpha\\) = main effect of whole plot (fixed), \\(\\beta\\) = main effect of subplot (fixed), \\(\\alpha\\)\\(\\tau\\) = interaction between factors A and B, \\(\\epsilon_{ij}\\) = whole plot error, \\(\\delta_{ijk}\\) = subplot error.\n\\[ \\epsilon \\sim N(0, \\sigma_\\epsilon)\\]\n\\[\\ \\delta  \\sim N(0, \\sigma_\\delta)\\]\nBoth the error and the rep effects are assumed to be normally distributed with a mean of zero and standard deviations of \\(\\sigma_\\epsilon\\) and \\(\\sigma_\\delta\\), respectively.\nThis is also referred as “Split-Block RCB” design. The statistical model structure for split plot design:\n\\[y_{ijk} = \\mu + \\rho_j +  \\alpha_i + \\beta_k + (\\alpha_i\\beta_k) + \\epsilon_{ij} + \\delta_{ijk}\\] Where:\n\\(\\mu\\) = overall experimental mean, \\(\\rho\\) = block effect (random), \\(\\alpha\\) = main effect of whole plot (fixed), \\(\\beta\\) = main effect of subplot (fixed), \\(\\alpha\\)\\(\\beta\\) = interaction between factors A and B, \\(\\epsilon_{ij}\\) = whole plot error, \\(\\delta_{ijk}\\) = subplot error.\n\\[ \\epsilon \\sim N(0, \\sigma_\\epsilon)\\]\n\\[\\ \\delta  \\sim N(0, \\sigma_\\delta)\\]\nBoth the overall error and the rep effects are assumed to be normally distributed with a mean of zero and standard deviations of \\(\\sigma\\) and \\(\\sigma_\\delta\\), respectively.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Split Plot Design</span>"
    ]
  },
  {
    "objectID": "chapters/split-plot-design.html#details-for-split-plot-designs",
    "href": "chapters/split-plot-design.html#details-for-split-plot-designs",
    "title": "6  Split Plot Design",
    "section": "",
    "text": "Whole Plot Randomized as a completely randomized design\n\n\n\n\n\n\n\n\nWhole Plot Randomized as an RCBD\n\n\n\n\n\n\n\n\n\n\n\n\n\n‘iid’ assumption for error terms\n\n\n\nIn these model, the error terms, \\(\\epsilon\\) are assumed to be “iid”, that is, independently and identically distributed. This means they have constant variance and they each individual error term is independent from the others.\n\n\n\n\n\nSplit Plot CRD Design\n\n\n\n\n\nSplit Plot RCBD Design",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Split Plot Design</span>"
    ]
  },
  {
    "objectID": "chapters/split-plot-design.html#analysis-examples",
    "href": "chapters/split-plot-design.html#analysis-examples",
    "title": "6  Split Plot Design",
    "section": "6.2 Analysis Examples",
    "text": "6.2 Analysis Examples\nLoad required libraries\n\nlme4nlme\n\n\n\nlibrary(lme4); library(lmerTest); library(emmeans)\nlibrary(dplyr); library(performance); library(ggplot2)\nlibrary(broom.mixed)\n\n\n\n\nlibrary(nlme); library(performance); library(emmeans)\nlibrary(dplyr); library(ggplot2); library(broom.mixed)\n\n\n\n\n\n6.2.1 Example model for CRD Split Plot Designs\nLet’s import height data. It is located here if you want to download it yourself (recommended).\nThe data (Height data) for this example involves a CRD split plot designed experiment. Treatments are 4 Timings (times) and 8 managements (manage). The whole plots are times and management represents subplot and 3 replications.\n\nheight_data &lt;- readxl::read_excel(here::here(\"data\", \"height_data.xlsx\"))\n\n\nTable of variables in the oat data set\n\n\nrep\nreplication unit\n\n\ntime\nMain plot with 4 levels\n\n\nManage\nSplit-plot with 8 levels\n\n\nsample\ntwo sampling units per each rep\n\n\nheight\nyield (lbs per acre)\n\n\n\n\n6.2.1.1 Data integrity checks\n\nRun a cross tabulation using table() to check the arrangement of whole-plots and sub-plots.\n\n\ntable(height_data$time, height_data$manage)\n\n    \n     M1 M2 M3 M4 M5 M6 M7 M8\n  T1  6  6  6  6  6  6  6  6\n  T2  6  6  6  6  6  6  6  6\n  T3  6  6  6  6  6  6  6  6\n  T4  6  6  6  6  6  6  6  6\n\n\nThe levels of whole plots and subplots are balanced.\n\nLook at structure of the data using str(), this will help in identifying class of the variable. In this data set, class of the whole-plot, sub-plot, and block should be factor/character and response variable (height) should be numeric.\n\n\nstr(height_data)\n\ntibble [192 × 5] (S3: tbl_df/tbl/data.frame)\n $ time  : chr [1:192] \"T1\" \"T1\" \"T1\" \"T1\" ...\n $ manage: chr [1:192] \"M1\" \"M2\" \"M3\" \"M4\" ...\n $ rep   : chr [1:192] \"R1\" \"R1\" \"R1\" \"R1\" ...\n $ sample: chr [1:192] \"S1\" \"S1\" \"S1\" \"S1\" ...\n $ height: num [1:192] 104.5 92.3 96.8 94.7 105.7 ...\n\n\nThe ‘time’, ‘manage’, and ‘rep’ are character and variable height is numeric. The structure of the data is in format as needed. - Check the number of missing values in each column.\n\napply(height_data, 2, function(x) sum(is.na(x)))\n\n  time manage    rep sample height \n     0      0      0      0      0 \n\n\n\nExploratory boxplot to look at the height observations at different times with variable managements.\n\n\nggplot(data = height_data, aes(y = height, x = time)) +\n  geom_boxplot(aes(fill = manage), alpha = 0.6)\n\n\n\n\n\n\n\n\n\n\n6.2.1.2 Model building\n\n\nRecall the model:\n\\[y_{ijk} = \\mu + \\gamma_i +  \\alpha_j + \\beta_k + (\\alpha_j\\beta_k) + \\epsilon_{ijk}\\]\nFor this model, \\(\\gamma\\) = block/rep effect (random), \\(\\alpha\\) = main effect of whole plot (fixed), \\(\\beta\\) = main effect of subplot (fixed), \\(\\alpha\\)\\(\\beta\\) = interaction between factors A and B (fixed).\nIn order to test the main effects of the treatments as well as the interaction between two factors, we can specify that in model as: time + manage + time:manage or time*manage.\nWhen dealing with split plot design across reps or blocks, the random effects needs to be nested hierarchically, from largest unit to smallest. For example, in this example the random effects will be designated as (1 | rep/time). This implies that random intercepts vary with rep and time within rep.\n\nlme4nlme\n\n\n\nmodel_lmer &lt;- lmer(height ~ time*manage + (1|rep/time), data = height_data)\ntidy(model_lmer)\n\n# A tibble: 35 × 8\n   effect group term        estimate std.error statistic     df    p.value\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n 1 fixed  &lt;NA&gt;  (Intercept)   108.        3.19    33.9     4.38 0.00000181\n 2 fixed  &lt;NA&gt;  timeT2          3.18      2.63     1.21  104.   0.229     \n 3 fixed  &lt;NA&gt;  timeT3         -2.25      2.63    -0.855 104.   0.394     \n 4 fixed  &lt;NA&gt;  timeT4          1.28      2.63     0.488 104.   0.627     \n 5 fixed  &lt;NA&gt;  manageM2       -4.45      2.55    -1.74  152.   0.0832    \n 6 fixed  &lt;NA&gt;  manageM3       -5.30      2.55    -2.08  152.   0.0395    \n 7 fixed  &lt;NA&gt;  manageM4       -6.18      2.55    -2.42  152.   0.0166    \n 8 fixed  &lt;NA&gt;  manageM5       -5.02      2.55    -1.97  152.   0.0511    \n 9 fixed  &lt;NA&gt;  manageM6       -3.42      2.55    -1.34  152.   0.183     \n10 fixed  &lt;NA&gt;  manageM7       -9.75      2.55    -3.82  152.   0.000193  \n# ℹ 25 more rows\n\n\n\n\n\nmodel_lme &lt;-lme(height ~ time*manage,\n             random = ~ 1|rep/time, data = height_data)\n\ntidy(model_lme)\n\nWarning in tidy.lme(model_lme): ran_pars not yet implemented for multiple\nlevels of nesting\n\n\n# A tibble: 32 × 7\n   effect term        estimate std.error    df statistic  p.value\n   &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 fixed  (Intercept)   108.        3.19   152    33.9   9.59e-73\n 2 fixed  timeT2          3.18      2.63     6     1.21  2.72e- 1\n 3 fixed  timeT3         -2.25      2.63     6    -0.855 4.25e- 1\n 4 fixed  timeT4          1.28      2.63     6     0.488 6.43e- 1\n 5 fixed  manageM2       -4.45      2.55   152    -1.74  8.32e- 2\n 6 fixed  manageM3       -5.30      2.55   152    -2.08  3.95e- 2\n 7 fixed  manageM4       -6.18      2.55   152    -2.42  1.66e- 2\n 8 fixed  manageM5       -5.02      2.55   152    -1.97  5.11e- 2\n 9 fixed  manageM6       -3.42      2.55   152    -1.34  1.83e- 1\n10 fixed  manageM7       -9.75      2.55   152    -3.82  1.93e- 4\n# ℹ 22 more rows\n\n\n\n\n\n\n\n6.2.1.3 Check Model Assumptions\nBefore interpreting the model we should investigate the assumptions of the model to ensure any conclusions we draw are valid. There are assumptions that we can check are 1. Homogeneity (equal variance) 2. normality of residuals 3. values with high leverage.\nWe will use check_model() function from ‘performance’ package. The plots generated using this code gives a visual check of various assumptions including normality of residuals, normality of random effects, heteroscedasticity, homogeneity of variance, and multicollinearity.\n\nlme4nlme\n\n\n\ncheck_model(model_lmer, check = c('normality', 'linearity'))\n\n\n\n\n\n\n\n\n\n\n\ncheck_model(model_lme, check = c('normality', 'linearity'))\n\n\n\n\n\n\n\n\n\n\n\nIn this case the residuals fit the assumptions of the model well.\n\n\n6.2.1.4 Inference\nThe anova() function prints the the rows of analysis of variance table for whole-plot, sub-plot, and their interactions. We observed a significant effect of manage factor only.\n\nlme4nlme\n\n\n\ncar::Anova(model_lmer, type = 'III', test.statistics = \"F\")\n\nAnalysis of Deviance Table (Type III Wald chisquare tests)\n\nResponse: height\n                Chisq Df Pr(&gt;Chisq)    \n(Intercept) 1148.5658  1    &lt; 2e-16 ***\ntime           4.5139  3    0.21105    \nmanage        15.9090  7    0.02596 *  \ntime:manage   24.3349 21    0.27711    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nanova(model_lme, type = \"marginal\")\n\n            numDF denDF   F-value p-value\n(Intercept)     1   152 1148.6202  &lt;.0001\ntime            3     6    1.5046  0.3061\nmanage          7   152    2.2727  0.0315\ntime:manage    21   152    1.1588  0.2955\n\n\n\n\n\nWe can further compute estimated marginal means for each fixed effect and interaction effect can be obtained using emmeans().\n\nlme4nlme\n\n\n\nm1 &lt;- emmeans(model_lmer, ~ time)\n\nNOTE: Results may be misleading due to involvement in interactions\n\nm1\n\n time emmean  SE   df lower.CL upper.CL\n T1      103 2.7 2.27     92.8      114\n T2      106 2.7 2.27     95.5      116\n T3      100 2.7 2.27     89.8      111\n T4      104 2.7 2.27     94.0      115\n\nResults are averaged over the levels of: manage \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\n\n\nm2 &lt;- emmeans(model_lme, ~ time)\n\nNOTE: Results may be misleading due to involvement in interactions\n\nm2\n\n time emmean  SE df lower.CL upper.CL\n T1      103 2.7  2     91.6      115\n T2      106 2.7  2     94.2      118\n T3      100 2.7  2     88.6      112\n T4      104 2.7  2     92.8      116\n\nResults are averaged over the levels of: manage \nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\n\n\n\nFurther, a pairwise comparison or contrasts can be analyzed using estimated means. In this model, ‘time’ factor has 4 levels. We can use pairs() function to evaluate pairwise comparison among different ‘time’ levels.\nHere’s a example using pairs() function to compare difference in height among different time points.\n\nlme4nlme\n\n\n\npairs(m1)\n\n contrast estimate   SE df t.ratio p.value\n T1 - T2     -2.68 1.11  6  -2.426  0.1719\n T1 - T3      2.95 1.11  6   2.665  0.1287\n T1 - T4     -1.21 1.11  6  -1.091  0.7072\n T2 - T3      5.63 1.11  6   5.091  0.0089\n T2 - T4      1.48 1.11  6   1.334  0.5767\n T3 - T4     -4.15 1.11  6  -3.756  0.0358\n\nResults are averaged over the levels of: manage \nDegrees-of-freedom method: kenward-roger \nP value adjustment: tukey method for comparing a family of 4 estimates \n\n\n\n\n\npairs(m2)\n\n contrast estimate   SE df t.ratio p.value\n T1 - T2     -2.68 1.11  6  -2.426  0.1719\n T1 - T3      2.95 1.11  6   2.665  0.1287\n T1 - T4     -1.21 1.11  6  -1.091  0.7072\n T2 - T3      5.63 1.11  6   5.091  0.0089\n T2 - T4      1.48 1.11  6   1.334  0.5767\n T3 - T4     -4.15 1.11  6  -3.756  0.0358\n\nResults are averaged over the levels of: manage \nDegrees-of-freedom method: containment \nP value adjustment: tukey method for comparing a family of 4 estimates \n\n\n\n\n\n\n\n\n\n\n\npairs()\n\n\n\nThe default p-value adjustment in pairs() function is “tukey”, other options include “holm”, “hochberg”, “BH”, “BY”, and “none”. In addition, it’s okay to use this function when independent variable has few factors (2-4). For variable with multiple levels, it’s better to use custom contrasts. For more information on custom contrasts please check this link.\n\n\n\n\n\n6.2.2 Example model for RCBD Split Plot Designs\nThe oats data used in this example is from the MASS package. The design is RCBD split plot with 6 blocks, 3 main plots and 4 subplots. The primary outcome variable was oat yield.\n\nTable of variables in the oat data set\n\n\nblock\nblocking unit\n\n\nVariety (V)\nMain plot with 3 levels\n\n\nNitrogen (N)\nSplit-plot with 4 levels\n\n\nyield (Y)\nyield (lbs per acre)\n\n\n\nThe objective of this analysis is to study the impact of different varieties and nitrogen application rates on oat yields.\nTo fully examine the yield of oats due to varieties and nutrient levels in a split plots. We will need to statistically analyse and compare the effects of varieties (main plot), nutrient levels (subplot), their interaction.\n\nlibrary(MASS)\ndata(\"oats\")\nhead(oats,5)\n\n  B           V      N   Y\n1 I     Victory 0.0cwt 111\n2 I     Victory 0.2cwt 130\n3 I     Victory 0.4cwt 157\n4 I     Victory 0.6cwt 174\n5 I Golden.rain 0.0cwt 117\n\n\n\n6.2.2.1 Data integrity checks\nLet’s look at the structure of the data. The “B”, “V”, and “N” needs to be ‘factor’ and “Y” should be numeric.\n\nstr(oats)\n\n'data.frame':   72 obs. of  4 variables:\n $ B: Factor w/ 6 levels \"I\",\"II\",\"III\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ V: Factor w/ 3 levels \"Golden.rain\",..: 3 3 3 3 1 1 1 1 2 2 ...\n $ N: Factor w/ 4 levels \"0.0cwt\",\"0.2cwt\",..: 1 2 3 4 1 2 3 4 1 2 ...\n $ Y: int  111 130 157 174 117 114 161 141 105 140 ...\n\n\nNext, run the table() command to verify the levels of main-plots and sub-plots.\n\ntable(oats$V, oats$N)\n\n             \n              0.0cwt 0.2cwt 0.4cwt 0.6cwt\n  Golden.rain      6      6      6      6\n  Marvellous       6      6      6      6\n  Victory          6      6      6      6\n\n\n\n\n6.2.2.2 Model Building the Model\nWe are evaluating the effect of V, N and their interaction on yield. The 1|B/V implies that random intercepts vary with block and V within each block.\n\n\nRecall the model:\n\\[y_{ijk} = \\mu + \\rho_j +  \\alpha_i + \\beta_k + (\\alpha_i\\beta_k) + \\epsilon_{ij} + \\delta_{ijk}\\] Where:\n\\(\\mu\\) = overall experimental mean, \\(\\rho\\) = block effect (random), \\(\\alpha\\) = main effect of whole plot (fixed), \\(\\beta\\) = main effect of subplot (fixed), \\(\\alpha\\)\\(\\beta\\) = interaction between factors A and B, \\(\\epsilon_{ij}\\) = whole plot error, \\(\\delta_{ijk}\\) = subplot error.\n\nlme4nlme\n\n\n\nmodel2_lmer &lt;- lmer(Y ~  V + N + V:N + (1|B/V), \n                   data = oats, \n                   na.action = na.exclude)\ntidy(model2_lmer)\n\n# A tibble: 15 × 8\n   effect   group    term            estimate std.error statistic    df  p.value\n   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 fixed    &lt;NA&gt;     (Intercept)       80.0        9.11    8.78    16.1  1.55e-7\n 2 fixed    &lt;NA&gt;     VMarvellous        6.67       9.72    0.686   30.2  4.98e-1\n 3 fixed    &lt;NA&gt;     VVictory          -8.50       9.72   -0.875   30.2  3.89e-1\n 4 fixed    &lt;NA&gt;     N0.2cwt           18.5        7.68    2.41    45.0  2.02e-2\n 5 fixed    &lt;NA&gt;     N0.4cwt           34.7        7.68    4.51    45.0  4.58e-5\n 6 fixed    &lt;NA&gt;     N0.6cwt           44.8        7.68    5.84    45.0  5.48e-7\n 7 fixed    &lt;NA&gt;     VMarvellous:N0…    3.33      10.9     0.307   45.0  7.60e-1\n 8 fixed    &lt;NA&gt;     VVictory:N0.2c…   -0.333     10.9    -0.0307  45.0  9.76e-1\n 9 fixed    &lt;NA&gt;     VMarvellous:N0…   -4.17      10.9    -0.383   45.0  7.03e-1\n10 fixed    &lt;NA&gt;     VVictory:N0.4c…    4.67      10.9     0.430   45.0  6.70e-1\n11 fixed    &lt;NA&gt;     VMarvellous:N0…   -4.67      10.9    -0.430   45.0  6.70e-1\n12 fixed    &lt;NA&gt;     VVictory:N0.6c…    2.17      10.9     0.199   45.0  8.43e-1\n13 ran_pars V:B      sd__(Intercept)   10.3       NA      NA       NA   NA      \n14 ran_pars B        sd__(Intercept)   14.6       NA      NA       NA   NA      \n15 ran_pars Residual sd__Observation   13.3       NA      NA       NA   NA      \n\n\n\n\n\nmodel2_lme &lt;- lme(Y ~  V + N + V:N ,\n                  random = ~1|B/V,\n                  data = oats, \n                  na.action = na.exclude)\ntidy(model2_lme)\n\nWarning in tidy.lme(model2_lme): ran_pars not yet implemented for multiple\nlevels of nesting\n\n\n# A tibble: 12 × 7\n   effect term                estimate std.error    df statistic  p.value\n   &lt;chr&gt;  &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 fixed  (Intercept)           80          9.11    45    8.78   2.56e-11\n 2 fixed  VMarvellous            6.67       9.72    10    0.686  5.08e- 1\n 3 fixed  VVictory              -8.50       9.72    10   -0.875  4.02e- 1\n 4 fixed  N0.2cwt               18.5        7.68    45    2.41   2.02e- 2\n 5 fixed  N0.4cwt               34.7        7.68    45    4.51   4.58e- 5\n 6 fixed  N0.6cwt               44.8        7.68    45    5.84   5.48e- 7\n 7 fixed  VMarvellous:N0.2cwt    3.33      10.9     45    0.307  7.60e- 1\n 8 fixed  VVictory:N0.2cwt      -0.333     10.9     45   -0.0307 9.76e- 1\n 9 fixed  VMarvellous:N0.4cwt   -4.17      10.9     45   -0.383  7.03e- 1\n10 fixed  VVictory:N0.4cwt       4.67      10.9     45    0.430  6.70e- 1\n11 fixed  VMarvellous:N0.6cwt   -4.67      10.9     45   -0.430  6.70e- 1\n12 fixed  VVictory:N0.6cwt       2.17      10.9     45    0.199  8.43e- 1\n\n\n\n\n\n\n\n6.2.2.3 Check Model Assumptions\nAs shown in example 1, We need to verify the normality of residuals and homogeneous variance. Here we are using the check_model() function from the performance package.\n\nlme4nlme\n\n\n\ncheck_model(model2_lmer, check = c('normality', 'linearity'))\n\n\n\n\n\n\n\n\n\n\n\ncheck_model(model2_lme, check = c('normality', 'linearity'))\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.2.2.4 Inference\nWe can evaluate the model for the analysis of variance, for V, N and their interaction effect.\n\nlme4nlme\n\n\n\ncar::Anova(model2_lmer, type = \"III\", test.statistics = \"F\")\n\nAnalysis of Deviance Table (Type III Wald chisquare tests)\n\nResponse: Y\n              Chisq Df Pr(&gt;Chisq)    \n(Intercept) 77.1664  1  &lt; 2.2e-16 ***\nV            2.4491  2     0.2939    \nN           39.0683  3  1.679e-08 ***\nV:N          1.8169  6     0.9357    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nanova(model2_lme, type = \"marginal\")\n\n            numDF denDF  F-value p-value\n(Intercept)     1    45 77.16729  &lt;.0001\nV               2    10  1.22454  0.3344\nN               3    45 13.02273  &lt;.0001\nV:N             6    45  0.30282  0.9322\n\n\n\n\n\nNext, we can estimate marginal means for V, N, or their interaction (V*N) effect.\n\nlme4nlme\n\n\n\nemm1 &lt;- emmeans(model2_lmer, ~ V *N) \nemm1\n\n V           N      emmean   SE   df lower.CL upper.CL\n Golden.rain 0.0cwt   80.0 9.11 16.1     60.7     99.3\n Marvellous  0.0cwt   86.7 9.11 16.1     67.4    106.0\n Victory     0.0cwt   71.5 9.11 16.1     52.2     90.8\n Golden.rain 0.2cwt   98.5 9.11 16.1     79.2    117.8\n Marvellous  0.2cwt  108.5 9.11 16.1     89.2    127.8\n Victory     0.2cwt   89.7 9.11 16.1     70.4    109.0\n Golden.rain 0.4cwt  114.7 9.11 16.1     95.4    134.0\n Marvellous  0.4cwt  117.2 9.11 16.1     97.9    136.5\n Victory     0.4cwt  110.8 9.11 16.1     91.5    130.1\n Golden.rain 0.6cwt  124.8 9.11 16.1    105.5    144.1\n Marvellous  0.6cwt  126.8 9.11 16.1    107.5    146.1\n Victory     0.6cwt  118.5 9.11 16.1     99.2    137.8\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\n\n\nemm1 &lt;- emmeans(model2_lme, ~ V *N) \nemm1\n\n V           N      emmean   SE df lower.CL upper.CL\n Golden.rain 0.0cwt   80.0 9.11  5     56.6    103.4\n Marvellous  0.0cwt   86.7 9.11  5     63.3    110.1\n Victory     0.0cwt   71.5 9.11  5     48.1     94.9\n Golden.rain 0.2cwt   98.5 9.11  5     75.1    121.9\n Marvellous  0.2cwt  108.5 9.11  5     85.1    131.9\n Victory     0.2cwt   89.7 9.11  5     66.3    113.1\n Golden.rain 0.4cwt  114.7 9.11  5     91.3    138.1\n Marvellous  0.4cwt  117.2 9.11  5     93.8    140.6\n Victory     0.4cwt  110.8 9.11  5     87.4    134.2\n Golden.rain 0.6cwt  124.8 9.11  5    101.4    148.2\n Marvellous  0.6cwt  126.8 9.11  5    103.4    150.2\n Victory     0.6cwt  118.5 9.11  5     95.1    141.9\n\nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\n\n\n\nIn the next chapter we will continue with extension of split plot design called split-split plot design.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Split Plot Design</span>"
    ]
  },
  {
    "objectID": "chapters/split-split-plot.html",
    "href": "chapters/split-split-plot.html",
    "title": "7  Split-Split Plot Design",
    "section": "",
    "text": "7.1 Details for split-split plot designs\nThe statistical model structure this design:\n\\[y_{ijk} = \\mu + \\rho_j +  \\alpha_i + \\beta_k + (\\alpha_i\\beta_k) + \\tau_n + (\\alpha_i\\tau_n) + (\\tau_n\\beta_k) + (\\alpha_i\\beta_k\\tau_n) + \\epsilon_{ijk} + \\delta_{ijkn}\\] Where:\n\\(\\mu\\)= overall experimental mean, \\(\\alpha\\) = main effect of whole plot (fixed), \\(\\beta\\) = main effect of subplot (fixed), \\(\\tau\\) = main effect of sub-subplot, \\(\\epsilon_{ij}\\) = whole plot error, \\(\\delta_{ijk}\\) = subplot error.\n\\[ \\epsilon \\sim N(0, \\sigma_\\epsilon)\\]\n\\[\\ \\delta  \\sim N(0, \\sigma_\\delta)\\]\nThe assumptions of the model includes normal distribution of both the error and the rep effects with a mean of zero and standard deviations of \\(\\sigma_\\epsilon\\) and \\(\\sigma_\\delta\\), respectively.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Split-Split Plot Design</span>"
    ]
  },
  {
    "objectID": "chapters/split-split-plot.html#example-analysis",
    "href": "chapters/split-split-plot.html#example-analysis",
    "title": "7  Split-Split Plot Design",
    "section": "7.2 Example Analysis",
    "text": "7.2 Example Analysis\n\nlme4nlme\n\n\n\nlibrary(dplyr)\nlibrary(lme4); library(lmerTest); library(broom.mixed)\nlibrary(emmeans); library(performance)\n\n\n\n\nlibrary(dplyr)\nlibrary(nlme); library(emmeans)\nlibrary(broom.mixed); library(performance)\n\n\n\n\nIn this example, we have a rice yield data from ‘agricolae’ package. This consists of of 3 different rice varieties grown under 3 management practices and 5 Nitrogen levels in the split-split design. Here, we are using rice yield data from the (agricolae) package.\n\nrice &lt;- read.csv(here::here(\"data\", \"rice_ssp.csv\"))\n\n\nTable of variables in the rice data set\n\n\n\n\n\n\nblock\nblocking unit\n\n\nnitrogen\ndifferent nitrogen fertilizer rates as main plot with 5 levels\n\n\nmanagement\nmanagement practices as subplot with 3 levels\n\n\nvariety\ncrop variety being a sub-subplot with 3 levels\n\n\nyield\nyield (bushels per acre)\n\n\n\n\n7.2.1 Data integrity checks\nLook at the structure of the data, class of block, nitrogen, management and variety should be a character/factor and yield should be numeric.\n\nstr(rice)\n\n'data.frame':   135 obs. of  6 variables:\n $ X         : int  1 2 3 4 5 6 7 8 9 10 ...\n $ block     : int  1 1 1 1 1 1 1 1 1 1 ...\n $ nitrogen  : int  0 0 0 50 50 50 80 80 80 110 ...\n $ management: chr  \"m1\" \"m2\" \"m3\" \"m1\" ...\n $ variety   : int  1 1 1 1 1 1 1 1 1 1 ...\n $ yield     : num  3.32 3.77 4.66 3.19 3.62 ...\n\n\nConvert block, nitrogen, variety, and management to characters.\n\nrice$block &lt;- as.character(rice$block)\nrice$nitrogen &lt;- as.character(rice$nitrogen)\nrice$management &lt;- as.character(rice$management)\nrice$variety &lt;- as.character(rice$variety)\n\nNext, run a cross tabulations to check balance of observations across independent variables:\n\ntable(rice$variety, rice$nitrogen, rice$management)\n\n, ,  = m1\n\n   \n    0 110 140 50 80\n  1 3   3   3  3  3\n  2 3   3   3  3  3\n  3 3   3   3  3  3\n\n, ,  = m2\n\n   \n    0 110 140 50 80\n  1 3   3   3  3  3\n  2 3   3   3  3  3\n  3 3   3   3  3  3\n\n, ,  = m3\n\n   \n    0 110 140 50 80\n  1 3   3   3  3  3\n  2 3   3   3  3  3\n  3 3   3   3  3  3\n\n\nIt looks perfectly balanced, with exactly 3 observation per treatment group.\nLast, check the distribution of the dependent variable by plotting a histogram using hist().\n\nhist(rice$yield)\n\n\n\n\n\n\n\n\n\n\nFigure 7.1: Histogram of the dependent variable.\n\n\n\n\n\n\n7.2.2 Model Building\nThe variance analysis of a split-split plot design is divided into three parts: the main-plot, subplot and sub-subplot analysis. We can use the nesting notation in the random part because nitrogen and management are nested in blocks. We can do blocks as fixed or random.\n\nlme4nlme\n\n\n\nmodel_lmer &lt;- lmer(yield ~ nitrogen * management * variety +\n                     (1 | block / nitrogen / management),\n                   data = rice,\n                   na.action = na.exclude)\n\nboundary (singular) fit: see help('isSingular')\n\ntidy(model_lmer)\n\n# A tibble: 49 × 8\n   effect group term                 estimate std.error statistic    df  p.value\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 fixed  &lt;NA&gt;  (Intercept)             3.90      0.386    10.1    89.7 1.79e-16\n 2 fixed  &lt;NA&gt;  nitrogen110             0.753     0.545     1.38   89.7 1.71e- 1\n 3 fixed  &lt;NA&gt;  nitrogen140             0.165     0.545     0.302  89.7 7.63e- 1\n 4 fixed  &lt;NA&gt;  nitrogen50              0.335     0.545     0.614  89.7 5.41e- 1\n 5 fixed  &lt;NA&gt;  nitrogen80              1.33      0.545     2.44   89.7 1.68e- 2\n 6 fixed  &lt;NA&gt;  managementm2            0.420     0.540     0.779  80.0 4.38e- 1\n 7 fixed  &lt;NA&gt;  managementm3            1.43      0.540     2.65   80.0 9.82e- 3\n 8 fixed  &lt;NA&gt;  variety2                1.45      0.540     2.68   80.0 8.83e- 3\n 9 fixed  &lt;NA&gt;  variety3                1.48      0.540     2.74   80.0 7.49e- 3\n10 fixed  &lt;NA&gt;  nitrogen110:managem…    0.377     0.763     0.493  80.0 6.23e- 1\n# ℹ 39 more rows\n\n\n\n\n\nmodel_lme &lt;- lme(yield ~ nitrogen*management*variety,\n                  random = ~ 1|block/nitrogen/management,\n                  data = rice, \n                  na.action = na.exclude)\ntidy(model_lme)\n\nWarning in tidy.lme(model_lme): ran_pars not yet implemented for multiple\nlevels of nesting\n\n\n# A tibble: 45 × 7\n   effect term                     estimate std.error    df statistic  p.value\n   &lt;chr&gt;  &lt;chr&gt;                       &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 fixed  (Intercept)                 3.90      0.386    60    10.1   1.43e-14\n 2 fixed  nitrogen110                 0.753     0.545     8     1.38  2.05e- 1\n 3 fixed  nitrogen140                 0.165     0.545     8     0.302 7.70e- 1\n 4 fixed  nitrogen50                  0.335     0.545     8     0.614 5.56e- 1\n 5 fixed  nitrogen80                  1.33      0.545     8     2.44  4.08e- 2\n 6 fixed  managementm2                0.420     0.540    20     0.779 4.45e- 1\n 7 fixed  managementm3                1.43      0.540    20     2.65  1.55e- 2\n 8 fixed  variety2                    1.45      0.540    60     2.68  9.38e- 3\n 9 fixed  variety3                    1.48      0.540    60     2.74  7.99e- 3\n10 fixed  nitrogen110:managementm2    0.377     0.763    20     0.493 6.27e- 1\n# ℹ 35 more rows\n\n\n\n\n\n\n\nboundary (singular) fit: We get a message that the fit is singular. What does this mean? Some components of the variance-covariance matrix of the random effects are either exactly zero or exactly one. OK what about in English? Basically it means that the algorithm that fits the model parameters doesn’t have enough data to get a good estimate. This often happens when we are trying to fit a model that is too complex for the amount of data we have, or when the random effects are very small and can’t be distinguished from zero. We still get some output but this message should make us take a close look at the random effects and their variances.\n\n\n7.2.3 Check Model Assumptions\nModel Diagnostics: we are looking for a constant variance and normality of residuals. Checking normality requiring first extracting the model residuals and then generating a qq-plot and qq-line. we can do all at one using one function check_model().\n\nlme4nlme\n\n\n\ncheck_model(model_lmer, check = c('normality', 'linearity'))\n\n\n\n\n\n\n\n\n\n\n\ncheck_model(model_lme, check = c('normality', 'linearity'))\n\n\n\n\n\n\n\n\n\n\n\nHere, we didn’t observe any anomaly in model assumptions.\n\n\n7.2.4 Inference\nAnalysis of variance\n\nlme4nlme\n\n\n\ncar::Anova(model_lmer, type = 'III', test.statistic=\"F\")\n\nAnalysis of Deviance Table (Type III Wald F tests with Kenward-Roger df)\n\nResponse: yield\n                                   F Df Df.res  Pr(&gt;F)    \n(Intercept)                 102.1211  1 89.706 &lt; 2e-16 ***\nnitrogen                      1.9160  4 86.474 0.11496    \nmanagement                    3.6962  2 77.143 0.02932 *  \nvariety                       4.9129  2 60.000 0.01057 *  \nnitrogen:management           0.2118  8 77.143 0.98797    \nnitrogen:variety              2.6681  8 60.000 0.01413 *  \nmanagement:variety            2.2193  4 60.000 0.07754 .  \nnitrogen:management:variety   0.5289 16 60.000 0.92105    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nanova(model_lme, type = \"marginal\")\n\n                            numDF denDF   F-value p-value\n(Intercept)                     1    60 102.12108  &lt;.0001\nnitrogen                        4     8   1.91603  0.2012\nmanagement                      2    20   3.69617  0.0431\nvariety                         2    60   4.91295  0.0106\nnitrogen:management             8    20   0.21177  0.9850\nnitrogen:variety                8    60   2.66810  0.0141\nmanagement:variety              4    60   2.21929  0.0775\nnitrogen:management:variety    16    60   0.52893  0.9210\n\n\n\n\n\nWe can estimated the marginal means for each treatment factor (variety, nitrogen, management) which will averaged across other factors. and their interaction.\n\nlme4nlme\n\n\n\nemmeans(model_lmer, ~ nitrogen)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n nitrogen emmean    SE df lower.CL upper.CL\n 0          5.38 0.139 10     5.08     5.69\n 110        6.94 0.139 10     6.63     7.25\n 140        7.23 0.139 10     6.93     7.54\n 50         6.22 0.139 10     5.91     6.53\n 80         7.00 0.139 10     6.69     7.30\n\nResults are averaged over the levels of: management, variety \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nemmeans(model_lmer, ~ nitrogen*variety|management)\n\nmanagement = m1:\n nitrogen variety emmean    SE   df lower.CL upper.CL\n 0        1         3.90 0.386 89.7     3.13     4.66\n 110      1         4.65 0.386 89.7     3.88     5.42\n 140      1         4.06 0.386 89.7     3.30     4.83\n 50       1         4.23 0.386 89.7     3.47     5.00\n 80       1         5.23 0.386 89.7     4.46     5.99\n 0        2         5.35 0.386 89.7     4.58     6.11\n 110      2         6.25 0.386 89.7     5.49     7.02\n 140      2         6.78 0.386 89.7     6.01     7.54\n 50       2         5.92 0.386 89.7     5.16     6.69\n 80       2         5.98 0.386 89.7     5.21     6.75\n 0        3         5.38 0.386 89.7     4.61     6.14\n 110      3         7.45 0.386 89.7     6.69     8.22\n 140      3         8.62 0.386 89.7     7.85     9.38\n 50       3         6.78 0.386 89.7     6.02     7.55\n 80       3         7.93 0.386 89.7     7.17     8.70\n\nmanagement = m2:\n nitrogen variety emmean    SE   df lower.CL upper.CL\n 0        1         4.32 0.386 89.7     3.55     5.08\n 110      1         5.45 0.386 89.7     4.68     6.21\n 140      1         5.20 0.386 89.7     4.43     5.97\n 50       1         4.58 0.386 89.7     3.81     5.34\n 80       1         5.73 0.386 89.7     4.97     6.50\n 0        2         4.71 0.386 89.7     3.95     5.48\n 110      2         7.00 0.386 89.7     6.24     7.77\n 140      2         7.08 0.386 89.7     6.32     7.85\n 50       2         5.82 0.386 89.7     5.05     6.58\n 80       2         6.50 0.386 89.7     5.73     7.27\n 0        3         6.50 0.386 89.7     5.73     7.26\n 110      3         8.62 0.386 89.7     7.86     9.39\n 140      3         9.40 0.386 89.7     8.63    10.17\n 50       3         7.82 0.386 89.7     7.05     8.58\n 80       3         8.57 0.386 89.7     7.80     9.33\n\nmanagement = m3:\n nitrogen variety emmean    SE   df lower.CL upper.CL\n 0        1         5.33 0.386 89.7     4.56     6.09\n 110      1         6.24 0.386 89.7     5.47     7.00\n 140      1         5.97 0.386 89.7     5.21     6.74\n 50       1         5.48 0.386 89.7     4.72     6.25\n 80       1         6.55 0.386 89.7     5.78     7.31\n 0        2         5.43 0.386 89.7     4.66     6.20\n 110      2         7.52 0.386 89.7     6.75     8.28\n 140      2         8.01 0.386 89.7     7.24     8.77\n 50       2         6.31 0.386 89.7     5.55     7.08\n 80       2         7.29 0.386 89.7     6.52     8.05\n 0        3         7.56 0.386 89.7     6.79     8.33\n 110      3         9.25 0.386 89.7     8.49    10.02\n 140      3         9.99 0.386 89.7     9.22    10.76\n 50       3         9.05 0.386 89.7     8.28     9.81\n 80       3         9.19 0.386 89.7     8.42     9.96\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\n\n\nemmeans(model_lme, ~ nitrogen)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n nitrogen emmean    SE df lower.CL upper.CL\n 0          5.38 0.139  2     4.79     5.98\n 110        6.94 0.139  2     6.34     7.53\n 140        7.23 0.139  2     6.64     7.83\n 50         6.22 0.139  2     5.62     6.82\n 80         7.00 0.139  2     6.40     7.59\n\nResults are averaged over the levels of: management, variety \nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\nemmeans(model_lme, ~ nitrogen*variety|management)\n\nmanagement = m1:\n nitrogen variety emmean    SE df lower.CL upper.CL\n 0        1         3.90 0.386  2     2.24     5.56\n 110      1         4.65 0.386  2     2.99     6.31\n 140      1         4.06 0.386  2     2.40     5.72\n 50       1         4.23 0.386  2     2.57     5.89\n 80       1         5.23 0.386  2     3.57     6.89\n 0        2         5.35 0.386  2     3.69     7.01\n 110      2         6.25 0.386  2     4.59     7.91\n 140      2         6.78 0.386  2     5.12     8.43\n 50       2         5.92 0.386  2     4.26     7.58\n 80       2         5.98 0.386  2     4.32     7.64\n 0        3         5.38 0.386  2     3.72     7.04\n 110      3         7.45 0.386  2     5.79     9.11\n 140      3         8.62 0.386  2     6.96    10.28\n 50       3         6.78 0.386  2     5.12     8.44\n 80       3         7.93 0.386  2     6.27     9.59\n\nmanagement = m2:\n nitrogen variety emmean    SE df lower.CL upper.CL\n 0        1         4.32 0.386  2     2.66     5.98\n 110      1         5.45 0.386  2     3.79     7.11\n 140      1         5.20 0.386  2     3.54     6.86\n 50       1         4.58 0.386  2     2.92     6.24\n 80       1         5.73 0.386  2     4.07     7.39\n 0        2         4.71 0.386  2     3.05     6.37\n 110      2         7.00 0.386  2     5.34     8.66\n 140      2         7.08 0.386  2     5.43     8.74\n 50       2         5.82 0.386  2     4.16     7.47\n 80       2         6.50 0.386  2     4.84     8.16\n 0        3         6.50 0.386  2     4.84     8.16\n 110      3         8.62 0.386  2     6.97    10.28\n 140      3         9.40 0.386  2     7.74    11.06\n 50       3         7.82 0.386  2     6.16     9.48\n 80       3         8.57 0.386  2     6.91    10.23\n\nmanagement = m3:\n nitrogen variety emmean    SE df lower.CL upper.CL\n 0        1         5.33 0.386  2     3.67     6.98\n 110      1         6.24 0.386  2     4.58     7.90\n 140      1         5.97 0.386  2     4.31     7.63\n 50       1         5.48 0.386  2     3.82     7.14\n 80       1         6.55 0.386  2     4.89     8.21\n 0        2         5.43 0.386  2     3.77     7.09\n 110      2         7.52 0.386  2     5.86     9.18\n 140      2         8.01 0.386  2     6.35     9.66\n 50       2         6.31 0.386  2     4.65     7.97\n 80       2         7.29 0.386  2     5.63     8.95\n 0        3         7.56 0.386  2     5.90     9.22\n 110      3         9.25 0.386  2     7.59    10.91\n 140      3         9.99 0.386  2     8.33    11.65\n 50       3         9.05 0.386  2     7.39    10.70\n 80       3         9.19 0.386  2     7.53    10.85\n\nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\n\n\n\nNotice we get a message that the estimated means for ‘nitrogen’ are averaged over the levels of ‘management’ and ‘variety’. So we need to be careful about how we interpret these estimates.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Split-Split Plot Design</span>"
    ]
  },
  {
    "objectID": "chapters/strip-plot.html",
    "href": "chapters/strip-plot.html",
    "title": "8  Strip Plot Design",
    "section": "",
    "text": "8.1 Background\nIn strip plot design each block or replication is divided into number of vertical and horizontal strips depending on the levels of the respective factors. 1. Vertical strip plot for the first factor – vertical factor 2. Horizontal strip plot for the second factor – horizontal factor\nDivide the experimental area into ‘A’ horizontal strips and ‘B’ vertical strips. Each level of factor A is assigned to all the plots in one row, and each level of factor B is assigned to all the plots in one column.\nThe statistical model: The statistical model structure this design:\n\\[y_{ijk} = \\mu + \\alpha_j + \\beta_k + \\alpha_j\\beta_k + b_i + r_{ij} + c_{ik} + \\epsilon_{ijk}\\] Where:\n\\(\\mu\\)= overall experimental mean, \\(\\alpha\\) and \\(\\beta\\) are the main effects applied in a horizontal and vertical direction, and \\(\\alpha\\)\\(\\beta\\) represents the interaction between main factors. The random effects in above equation are \\(b_i\\), the random rep effect, \\(r_{ij}\\), the row within rep random effect, \\(c_{ik}\\), the column within rep random effect.\n\\[ b_i \\sim N(0, \\sigma_1^2)\\]\n\\[ r_{ij}  \\sim N(0, \\sigma_2^2)\\]\n\\[ c_{ik} \\sim N(0, \\sigma_3^2)\\]\n\\[ \\epsilon_{ijk} \\sim N(0, \\sigma^2)\\]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Strip Plot Design</span>"
    ]
  },
  {
    "objectID": "chapters/strip-plot.html#example-analysis",
    "href": "chapters/strip-plot.html#example-analysis",
    "title": "8  Strip Plot Design",
    "section": "8.2 Example Analysis",
    "text": "8.2 Example Analysis\nWe will start by loading the required libraries for this analysis for lme and lmer models, respectively.\n\nlme4nlme\n\n\n\nlibrary(lme4); library(lmerTest); library(emmeans)\nlibrary(dplyr); library(performance); library(desplot)\nlibrary(broom.mixed)\n\n\n\n\nlibrary(nlme); library(performance); library(emmeans)\nlibrary(dplyr); library(desplot); library(broom.mixed)\n\n\n\n\nFor this example, we will use Rice strip-plot experiment data from theagridat package. A strip-plot experiment with three reps, variety as the horizontal strip and nitrogen fertilizer as the vertical strip.\n\ndata1 &lt;- agridat::gomez.stripplot\n\n\nTable of variables in the data set\n\n\nrep\nreplication unit\n\n\nnitro\nnitrogen fertilizer in kg/ha\n\n\ngen\nrice variety\n\n\nrow\nrow (represents gen)\n\n\ncol\ncolumn (represents nitro)\n\n\nyield\ngrain yield in kg/ha\n\n\n\nFor the sake of analysis, ‘row’ and ‘col’ variables are used to represent ‘nitrogen’ and ‘Gen’ factors. The plot below shows the application of treatments in horizontal and vertical direction in a strip plot design.\n\n\n\n\n\n\n\n\n\n\n8.2.1 Data integrity checks\nFirst thing we need to verify is the data types of the variables in data1. The ‘rep’, ‘nitro’, and ‘gen’ needs to be a factor/character variables and ‘yield’ should be numeric.\n\nstr(data1)\n\n'data.frame':   54 obs. of  6 variables:\n $ yield: int  2373 4076 7254 4007 5630 7053 2620 4676 7666 2726 ...\n $ rep  : Factor w/ 3 levels \"R1\",\"R2\",\"R3\": 1 1 1 1 1 1 1 1 1 1 ...\n $ nitro: int  0 60 120 0 60 120 0 60 120 0 ...\n $ gen  : Factor w/ 6 levels \"G1\",\"G2\",\"G3\",..: 1 1 1 2 2 2 3 3 3 4 ...\n $ col  : int  1 3 2 1 3 2 1 3 2 1 ...\n $ row  : int  1 1 1 3 3 3 4 4 4 2 ...\n\n\nLet’s convert ‘nitro’ from numeric to factor.\n\ndata1$nitro &lt;- as.factor(data1$nitro)\n\nLet’s have a look at the balance of treatment factors by running a a cross tabulation, and it looks well balanced.\n\ntable(data1$gen, data1$nitro)\n\n    \n     0 60 120\n  G1 3  3   3\n  G2 3  3   3\n  G3 3  3   3\n  G4 3  3   3\n  G5 3  3   3\n  G6 3  3   3\n\n\n\napply(data1, 2, function(x) sum(is.na(x)))\n\nyield   rep nitro   gen   col   row \n    0     0     0     0     0     0 \n\n\nWe don’t have any missing values in this data set.\nLastly, let’s check the dependent variable by plotting.\n\nhist(data1$yield, main = \"\", xlab = \"yield\")\n\n\n\n\n\n\n\n\n\n\nFigure 8.1: Histogram of the dependent variable.\n\n\n\n\nNo extreme values or skewness is present in the yield values.\n\n\n8.2.2 Model Building\nThe impact of nitro, gen, and their interaction was evaluated on rice yield. The rep, gen nested in rep, and nitro nested in rep were random effects in the model.\nhttps://stackoverflow.com/questions/36643713/how-to-specify-correlated-crossed-random-effects-in-nlme\n\nlme4nlme\n\n\n\nmodel_lmer &lt;- lmer(yield ~  nitro*gen +  (1|rep) + \n                   (1|rep:gen) + (1|rep:nitro), \n                   data = data1)\ntidy(model_lmer)\n\n# A tibble: 22 × 8\n   effect group term           estimate std.error statistic    df     p.value\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n 1 fixed  &lt;NA&gt;  (Intercept)       3572.      572.     6.24   17.8 0.00000732 \n 2 fixed  &lt;NA&gt;  nitro60           1560.      558.     2.80   22.4 0.0104     \n 3 fixed  &lt;NA&gt;  nitro120          3976.      558.     7.13   22.4 0.000000341\n 4 fixed  &lt;NA&gt;  genG2             1363.      717.     1.90   20.9 0.0714     \n 5 fixed  &lt;NA&gt;  genG3              678.      717.     0.945  20.9 0.355      \n 6 fixed  &lt;NA&gt;  genG4              487.      717.     0.679  20.9 0.504      \n 7 fixed  &lt;NA&gt;  genG5              530.      717.     0.739  20.9 0.468      \n 8 fixed  &lt;NA&gt;  genG6             -364.      717.    -0.508  20.9 0.617      \n 9 fixed  &lt;NA&gt;  nitro60:genG2      219.      741.     0.296  20.0 0.771      \n10 fixed  &lt;NA&gt;  nitro120:genG2   -1699.      741.    -2.29   20.0 0.0328     \n# ℹ 12 more rows\n\n\n\n\n\nmodel_lme &lt;-lme(yield ~  nitro*gen,\n                random = list(one = pdBlocked(list(\n        pdIdent(~ 0 + rep), \n         pdIdent(~ 0 + rep:gen), \n        pdIdent(~ 0 + rep:nitro)))),\n        data = data1 %&gt;% mutate(one = factor(1)))\n\nsummary(model_lme)\n\nLinear mixed-effects model fit by REML\n  Data: data1 %&gt;% mutate(one = factor(1)) \n       AIC      BIC    logLik\n  651.4204 686.2578 -303.7102\n\nRandom effects:\n Composite Structure: Blocked\n\n Block 1: repR1, repR2, repR3\n Formula: ~0 + rep | one\n Structure: Multiple of an Identity\n           repR1    repR2    repR3\nStdDev: 393.4278 393.4278 393.4278\n\n Block 2: repR1:genG1, repR2:genG1, repR3:genG1, repR1:genG2, repR2:genG2, repR3:genG2, repR1:genG3, repR2:genG3, repR3:genG3, repR1:genG4, repR2:genG4, repR3:genG4, repR1:genG5, repR2:genG5, repR3:genG5, repR1:genG6, repR2:genG6, repR3:genG6\n Formula: ~0 + rep:gen | one\n Structure: Multiple of an Identity\n        repR1:genG1 repR2:genG1 repR3:genG1 repR1:genG2 repR2:genG2 repR3:genG2\nStdDev:    600.1711    600.1711    600.1711    600.1711    600.1711    600.1711\n        repR1:genG3 repR2:genG3 repR3:genG3 repR1:genG4 repR2:genG4 repR3:genG4\nStdDev:    600.1711    600.1711    600.1711    600.1711    600.1711    600.1711\n        repR1:genG5 repR2:genG5 repR3:genG5 repR1:genG6 repR2:genG6 repR3:genG6\nStdDev:    600.1711    600.1711    600.1711    600.1711    600.1711    600.1711\n\n Block 3: repR1:nitro0, repR2:nitro0, repR3:nitro0, repR1:nitro60, repR2:nitro60, repR3:nitro60, repR1:nitro120, repR2:nitro120, repR3:nitro120\n Formula: ~0 + rep:nitro | one\n Structure: Multiple of an Identity\n        repR1:nitro0 repR2:nitro0 repR3:nitro0 repR1:nitro60 repR2:nitro60\nStdDev:     235.2591     235.2591     235.2591      235.2591      235.2591\n        repR3:nitro60 repR1:nitro120 repR2:nitro120 repR3:nitro120 Residual\nStdDev:      235.2591       235.2591       235.2591       235.2591 641.5963\n\nFixed effects:  yield ~ nitro * gen \n                   Value Std.Error DF   t-value p-value\n(Intercept)     3571.667  572.1257 36  6.242800  0.0000\nnitro60         1560.333  557.9682 36  2.796456  0.0082\nnitro120        3976.333  557.9682 36  7.126452  0.0000\ngenG2           1362.667  717.3336 36  1.899628  0.0655\ngenG3            678.000  717.3336 36  0.945167  0.3509\ngenG4            487.333  717.3336 36  0.679368  0.5012\ngenG5            530.000  717.3336 36  0.738847  0.4648\ngenG6           -364.333  717.3336 36 -0.507899  0.6146\nnitro60:genG2    219.000  740.8516 36  0.295606  0.7692\nnitro120:genG2 -1699.333  740.8516 36 -2.293757  0.0277\nnitro60:genG3    312.333  740.8516 36  0.421587  0.6758\nnitro120:genG3  -357.667  740.8516 36 -0.482778  0.6322\nnitro60:genG4    -65.667  740.8516 36 -0.088637  0.9299\nnitro120:genG4  -941.000  740.8516 36 -1.270160  0.2122\nnitro60:genG5    -28.667  740.8516 36 -0.038694  0.9693\nnitro120:genG5 -2066.000  740.8516 36 -2.788682  0.0084\nnitro60:genG6  -1053.333  740.8516 36 -1.421787  0.1637\nnitro120:genG6 -4691.667  740.8516 36 -6.332802  0.0000\n Correlation: \n               (Intr) nitr60 ntr120 genG2  genG3  genG4  genG5  genG6  n60:G2\nnitro60        -0.488                                                        \nnitro120       -0.488  0.500                                                 \ngenG2          -0.627  0.343  0.343                                          \ngenG3          -0.627  0.343  0.343  0.500                                   \ngenG4          -0.627  0.343  0.343  0.500  0.500                            \ngenG5          -0.627  0.343  0.343  0.500  0.500  0.500                     \ngenG6          -0.627  0.343  0.343  0.500  0.500  0.500  0.500              \nnitro60:genG2   0.324 -0.664 -0.332 -0.516 -0.258 -0.258 -0.258 -0.258       \nnitro120:genG2  0.324 -0.332 -0.664 -0.516 -0.258 -0.258 -0.258 -0.258  0.500\nnitro60:genG3   0.324 -0.664 -0.332 -0.258 -0.516 -0.258 -0.258 -0.258  0.500\nnitro120:genG3  0.324 -0.332 -0.664 -0.258 -0.516 -0.258 -0.258 -0.258  0.250\nnitro60:genG4   0.324 -0.664 -0.332 -0.258 -0.258 -0.516 -0.258 -0.258  0.500\nnitro120:genG4  0.324 -0.332 -0.664 -0.258 -0.258 -0.516 -0.258 -0.258  0.250\nnitro60:genG5   0.324 -0.664 -0.332 -0.258 -0.258 -0.258 -0.516 -0.258  0.500\nnitro120:genG5  0.324 -0.332 -0.664 -0.258 -0.258 -0.258 -0.516 -0.258  0.250\nnitro60:genG6   0.324 -0.664 -0.332 -0.258 -0.258 -0.258 -0.258 -0.516  0.500\nnitro120:genG6  0.324 -0.332 -0.664 -0.258 -0.258 -0.258 -0.258 -0.516  0.250\n               n120:G2 n60:G3 n120:G3 n60:G4 n120:G4 n60:G5 n120:G5 n60:G6\nnitro60                                                                   \nnitro120                                                                  \ngenG2                                                                     \ngenG3                                                                     \ngenG4                                                                     \ngenG5                                                                     \ngenG6                                                                     \nnitro60:genG2                                                             \nnitro120:genG2                                                            \nnitro60:genG3   0.250                                                     \nnitro120:genG3  0.500   0.500                                             \nnitro60:genG4   0.250   0.500  0.250                                      \nnitro120:genG4  0.500   0.250  0.500   0.500                              \nnitro60:genG5   0.250   0.500  0.250   0.500  0.250                       \nnitro120:genG5  0.500   0.250  0.500   0.250  0.500   0.500               \nnitro60:genG6   0.250   0.500  0.250   0.500  0.250   0.500  0.250        \nnitro120:genG6  0.500   0.250  0.500   0.250  0.500   0.250  0.500   0.500\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-1.52993309 -0.52842524  0.05394367  0.51465584  1.46902934 \n\nNumber of Observations: 54\nNumber of Groups: 1 \n\n#tidy(model_lme)\n\n\n\n\n\n\n8.2.3 Check Model Assumptions\n\nlme4nlme\n\n\n\ncheck_model(model_lmer, check = c('normality', 'linearity'))\n\n\n\n\n\n\n\n\n\n\nplot(model_lme, resid(., scaled=TRUE) ~ fitted(.), \n     xlab = \"fitted values\", ylab = \"studentized residuals\")\nqqnorm(residuals(model_lme))\nqqline(residuals(model_lme))\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe residuals fit the assumptions of the model well.\n\n\n8.2.4 Inference\nWe can evaluate the model for the analysis of variance, for main and interaction effects.\n\nlme4nlme\n\n\n\ncar::Anova(model_lmer, type = \"III\", test.statistics = \"F\")\n\nAnalysis of Deviance Table (Type III Wald chisquare tests)\n\nResponse: yield\n              Chisq Df Pr(&gt;Chisq)    \n(Intercept) 38.9728  1  4.298e-10 ***\nnitro       51.5701  2  6.334e-12 ***\ngen          6.8343  5     0.2333    \nnitro:gen   58.0064 10  8.621e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nanova(model_lme, type = \"marginal\")\n\n            numDF denDF  F-value p-value\n(Intercept)     1    36 38.97256  &lt;.0001\nnitro           2    36 25.78512  &lt;.0001\ngen             5    36  1.36687  0.2597\nnitro:gen      10    36  5.80061  &lt;.0001\n\n\n\n\n\nAnalysis of variance showed a significant interaction impact of gen and nitro on rice grain yield.\nNext, We estimate marginal means for nitro and gen interaction effects using emmeans package.\n\nlme4nlme\n\n\n\nemm1 &lt;- emmeans(model_lmer, ~ nitro*gen) \nemm1\n\n nitro gen emmean  SE   df lower.CL upper.CL\n 0     G1    3572 572 17.8     2368     4775\n 60    G1    5132 572 17.8     3929     6335\n 120   G1    7548 572 17.8     6345     8751\n 0     G2    4934 572 17.8     3731     6138\n 60    G2    6714 572 17.8     5510     7917\n 120   G2    7211 572 17.8     6008     8415\n 0     G3    4250 572 17.8     3046     5453\n 60    G3    6122 572 17.8     4919     7326\n 120   G3    7868 572 17.8     6665     9072\n 0     G4    4059 572 17.8     2856     5262\n 60    G4    5554 572 17.8     4350     6757\n 120   G4    7094 572 17.8     5891     8298\n 0     G5    4102 572 17.8     2898     5305\n 60    G5    5633 572 17.8     4430     6837\n 120   G5    6012 572 17.8     4809     7215\n 0     G6    3207 572 17.8     2004     4411\n 60    G6    3714 572 17.8     2511     4918\n 120   G6    2492 572 17.8     1289     3695\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\n\n\nemm1 &lt;- emmeans(model_lme, ~ nitro*gen)\n\nWarning in model.matrix.default(trms, m, contrasts.arg = contrasts): variable\n'rep' is absent, its contrast will be ignored\nWarning in model.matrix.default(trms, m, contrasts.arg = contrasts): variable\n'rep' is absent, its contrast will be ignored\n\nemm1\n\nWarning in qt((1 - level)/adiv, df): NaNs produced\n\n\n nitro gen emmean  SE df lower.CL upper.CL\n 0     G1    3572 572  0      NaN      NaN\n 60    G1    5132 572  0      NaN      NaN\n 120   G1    7548 572  0      NaN      NaN\n 0     G2    4934 572  0      NaN      NaN\n 60    G2    6714 572  0      NaN      NaN\n 120   G2    7211 572  0      NaN      NaN\n 0     G3    4250 572  0      NaN      NaN\n 60    G3    6122 572  0      NaN      NaN\n 120   G3    7868 572  0      NaN      NaN\n 0     G4    4059 572  0      NaN      NaN\n 60    G4    5554 572  0      NaN      NaN\n 120   G4    7094 572  0      NaN      NaN\n 0     G5    4102 572  0      NaN      NaN\n 60    G5    5633 572  0      NaN      NaN\n 120   G5    6012 572  0      NaN      NaN\n 0     G6    3207 572  0      NaN      NaN\n 60    G6    3714 572  0      NaN      NaN\n 120   G6    2492 572  0      NaN      NaN\n\nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\n\n\n\nNote that, confidence intervals were not estimated through emmeans from lme model.\n\n\n\n\n\n\nlme vs lmer\n\n\n\nFor strip plot experiment deisgn, fitting nested and crossed random effects is more complicated through nlme. Therefore, it’s more convinent to use lmer in this case as both models yielded same results in the example shown above.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Strip Plot Design</span>"
    ]
  },
  {
    "objectID": "chapters/incomplete-block-design.html",
    "href": "chapters/incomplete-block-design.html",
    "title": "9  Incomplete Block Design",
    "section": "",
    "text": "9.1 Background\nThe block design in Chapter 4 was complete, meaning that every block contained all the treatments. In practice, it may not be possible to have too many treatments in each block. Sometimes, there are also situations where it is advised to not have many treatments in each block.\nIn such cases, incomplete block designs are used where we have to decide what subset of treatments to be used in an individual block. This will work well if we enough blocks. However, if we only have small number of blocks, there would be the risk that certain quantities are not estimable anymore.\nTo avoid having a disconnected design, a balanced incomplete block design can be used\nThe statistical model for balanced incomplete block design is:\n\\[y_{ij} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}\\] Where:\n\\(\\mu\\) = overall experimental mean \\(\\alpha\\) = treatment effects (fixed) \\(\\beta\\) = block effects (random) \\(\\epsilon\\) = error terms\n\\[ \\epsilon \\sim N(0, \\sigma)\\]\n\\[ \\beta \\sim N(0, \\sigma_b)\\] There are few key points that we need to keep in mind while designing incomplete block designs:\nAn excellent description of incomplete block design is provided in ANOVA and Mixed Models by Lukas Meier.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Incomplete Block Design</span>"
    ]
  },
  {
    "objectID": "chapters/incomplete-block-design.html#background",
    "href": "chapters/incomplete-block-design.html#background",
    "title": "9  Incomplete Block Design",
    "section": "",
    "text": "A drawback of this design is that block effect and treatment effects are confounded.\nTo eliminate of block effects, better compare treatments within a block.\nNo treatment should appear twice in any block as they contributes nothing no within block comparisons.\n\n\n\n\n\n\n\n\nA note\n\n\n\nBecause the blocks are incomplete, the Type I and Type III sums of squares will be different. That is, the missing treatments in each block represent missing observations (but not missing ‘at random’).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Incomplete Block Design</span>"
    ]
  },
  {
    "objectID": "chapters/incomplete-block-design.html#example-analysis",
    "href": "chapters/incomplete-block-design.html#example-analysis",
    "title": "9  Incomplete Block Design",
    "section": "9.2 Example Analysis",
    "text": "9.2 Example Analysis\nWe will demonstrate an example data set designed in a balanced incomplete block design. First, load the libraries for analysis and estimation: ::: panel-tabset ### lme4\n\nlibrary(lme4); library(lmerTest); library(emmeans)\nlibrary(dplyr); library(broom.mixed); library(performance)\n\n\n9.2.1 nlme\n\nlibrary(nlme); library(broom.mixed); library(emmeans)\nlibrary(dplyr); library(performance)\n\n:::\nhttps://kwstat.github.io/agridat/reference/weiss.incblock.html\n\n library(agridat)\n  data(weiss.incblock)\n  dat &lt;- weiss.incblock\n\n\n\n\n\n\n\n\n\n\n\n\n9.2.2 Data integrity checks\nThe first thing is to make sure the data is what we expect. There are two steps:\n\nmake sure data are the expected data type\ncheck the extent of missing data\ninspect the independent variables and make sure the expected levels are present in the data\ninspect the dependent variable to ensure its distribution is following expectations\n\n\nstr(dat)\n\n'data.frame':   186 obs. of  5 variables:\n $ block: Factor w/ 31 levels \"B01\",\"B02\",\"B03\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ gen  : Factor w/ 31 levels \"G01\",\"G02\",\"G03\",..: 24 15 20 18 20 5 22 1 9 14 ...\n $ yield: num  29.8 24.2 30.5 20 35.2 25 23.6 23.6 29.3 25.5 ...\n $ row  : int  42 36 30 24 18 12 6 42 36 30 ...\n $ col  : int  1 1 1 1 1 1 1 2 2 2 ...\n\n\nThese look okay with block and gen being factor variables and yield, row, and col being numeric variables.\nNext, check the independent variables. Running a cross tabulations is often sufficient to ascertain this.\n\ndat$row &lt;- as.factor(dat$row)\n\n\ntable(dat$gen, dat$block)\n\n     \n      B01 B02 B03 B04 B05 B06 B07 B08 B09 B10 B11 B12 B13 B14 B15 B16 B17 B18\n  G01   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   1   1\n  G02   0   0   0   0   0   0   1   1   0   0   0   0   0   0   0   1   0   0\n  G03   0   0   1   0   0   0   0   1   1   1   0   0   1   0   0   0   0   0\n  G04   0   0   0   1   0   0   0   1   0   0   1   0   0   0   0   0   0   0\n  G05   0   0   0   0   1   1   0   1   0   0   0   0   0   1   1   0   0   0\n  G06   0   0   0   0   0   0   0   0   0   1   0   0   0   1   0   0   0   0\n  G07   0   0   1   1   0   0   1   0   0   0   0   0   0   0   0   0   0   0\n  G08   0   0   0   0   0   1   0   0   0   0   0   0   1   0   0   0   1   0\n  G09   0   0   0   0   0   0   0   0   1   0   0   0   0   0   1   0   0   0\n  G10   0   0   0   0   1   0   0   0   0   0   1   0   0   0   0   1   0   1\n  G11   0   1   1   0   0   1   0   0   0   0   1   0   0   0   0   0   0   0\n  G12   0   1   0   0   0   0   1   0   0   0   0   0   0   1   0   0   0   0\n  G13   0   1   0   0   0   0   0   0   0   0   0   0   1   0   1   1   0   0\n  G14   0   1   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   1\n  G15   0   1   0   1   1   0   0   0   1   0   0   0   0   0   0   0   1   0\n  G16   0   0   0   0   0   0   0   0   1   0   0   1   0   0   0   1   0   0\n  G17   0   0   0   0   0   0   1   0   0   1   1   1   0   0   1   0   1   0\n  G18   0   0   0   1   0   0   0   0   0   0   0   1   1   1   0   0   0   1\n  G19   0   0   0   0   0   1   0   0   0   0   0   1   0   0   0   0   0   0\n  G20   0   0   1   0   1   0   0   0   0   0   0   1   0   0   0   0   0   0\n  G21   1   0   0   1   0   0   0   0   0   0   0   0   0   0   1   0   0   0\n  G22   1   0   0   0   0   1   1   0   1   0   0   0   0   0   0   0   0   1\n  G23   1   0   0   0   0   0   0   0   0   0   1   0   1   0   0   0   0   0\n  G24   1   0   1   0   0   0   0   0   0   0   0   0   0   1   0   1   1   0\n  G25   1   0   0   0   1   0   0   0   0   1   0   0   0   0   0   0   0   0\n  G26   1   1   0   0   0   0   0   1   0   0   0   1   0   0   0   0   0   0\n  G27   0   0   0   0   1   0   1   0   0   0   0   0   1   0   0   0   0   0\n  G28   0   0   1   0   0   0   0   0   0   0   0   0   0   0   1   0   0   1\n  G29   0   0   0   1   0   1   0   0   0   1   0   0   0   0   0   1   0   0\n  G30   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0\n  G31   0   0   0   0   0   0   0   0   1   0   1   0   0   1   0   0   0   0\n     \n      B19 B20 B21 B22 B23 B24 B25 B26 B27 B28 B29 B30 B31\n  G01   1   0   0   0   0   0   0   0   0   0   1   1   0\n  G02   0   1   0   0   0   0   0   0   1   1   0   0   0\n  G03   0   0   0   0   1   0   0   0   0   0   0   0   0\n  G04   0   0   1   0   0   0   1   0   0   0   0   0   1\n  G05   0   0   0   0   0   1   0   0   0   0   0   0   0\n  G06   1   1   0   1   0   0   0   0   0   0   0   0   1\n  G07   0   0   0   1   0   1   0   0   0   0   1   0   0\n  G08   0   0   1   1   0   0   0   0   0   1   0   0   0\n  G09   0   0   0   1   0   0   1   0   1   0   0   1   0\n  G10   0   0   0   1   1   0   0   0   0   0   0   0   0\n  G11   1   0   0   0   0   0   0   0   1   0   0   0   0\n  G12   0   0   1   0   1   0   0   0   0   0   0   1   0\n  G13   0   0   0   0   0   0   0   0   0   0   1   0   1\n  G14   0   0   0   0   0   1   1   0   0   1   0   0   0\n  G15   0   1   0   0   0   0   0   0   0   0   0   0   0\n  G16   1   0   1   0   0   1   0   0   0   0   0   0   0\n  G17   0   0   0   0   0   0   0   0   0   0   0   0   0\n  G18   0   0   0   0   0   0   0   0   1   0   0   0   0\n  G19   0   1   0   0   1   0   1   0   0   0   1   0   0\n  G20   0   0   0   0   0   0   0   0   0   1   0   1   1\n  G21   1   0   0   0   1   0   0   0   0   1   0   0   0\n  G22   0   0   0   0   0   0   0   0   0   0   0   0   1\n  G23   0   1   0   0   0   1   0   0   0   0   0   1   0\n  G24   0   0   0   0   0   0   1   0   0   0   0   0   0\n  G25   0   0   1   0   0   0   0   0   1   0   1   0   0\n  G26   0   0   0   1   0   0   0   1   0   0   0   0   0\n  G27   1   0   0   0   0   0   1   1   0   0   0   0   0\n  G28   0   1   1   0   0   0   0   1   0   0   0   0   0\n  G29   0   0   0   0   0   0   0   1   0   0   0   1   0\n  G30   0   0   0   0   1   1   0   1   1   0   0   0   1\n  G31   0   0   0   0   0   0   0   1   0   1   1   0   0\n\n\nThere are 31 varieties and it is perfectly balanced, with exactly one observation per treatment per block.\nHere is a quick check I run to count the number of missing data in each column.\n\napply(dat, 2, function(x) sum(is.na(x)))\n\nblock   gen yield   row   col \n    0     0     0     0     0 \n\n\nWe observed no missing data!\nLast, check the dependent variable. A histogram is often quite sufficient to accomplish this. This is designed to be a quick check, so no need to spend time making the plot look good.\n\nhist(dat$yield, main = \"\", xlab = \"yield\")\n\n\n\n\n\n\n\n\n\n\nFigure 9.1: Histogram of the dependent variable.\n\n\n\n\nThis data set is ready for analysis!\n\n\n9.2.3 Model Building\n\nlme4nlme\n\n\n\nmodel_icbd &lt;- lmer(yield ~ gen + (1|block),\n                   data = dat, \n                   na.action = na.exclude)\ntidy(model_icbd)\n\n# A tibble: 33 × 8\n   effect group term        estimate std.error statistic    df  p.value\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 fixed  &lt;NA&gt;  (Intercept)  24.6        0.922   26.7     153. 2.30e-59\n 2 fixed  &lt;NA&gt;  genG02        2.40       1.17     2.06    129. 4.17e- 2\n 3 fixed  &lt;NA&gt;  genG03        8.04       1.17     6.88    129. 2.31e-10\n 4 fixed  &lt;NA&gt;  genG04        2.37       1.17     2.03    129. 4.42e- 2\n 5 fixed  &lt;NA&gt;  genG05        1.60       1.17     1.37    129. 1.73e- 1\n 6 fixed  &lt;NA&gt;  genG06        7.39       1.17     6.32    129. 3.82e- 9\n 7 fixed  &lt;NA&gt;  genG07       -0.419      1.17    -0.359   129. 7.20e- 1\n 8 fixed  &lt;NA&gt;  genG08        3.04       1.17     2.60    129. 1.04e- 2\n 9 fixed  &lt;NA&gt;  genG09        4.84       1.17     4.14    129. 6.22e- 5\n10 fixed  &lt;NA&gt;  genG10       -0.0429     1.17    -0.0367  129. 9.71e- 1\n# ℹ 23 more rows\n\n\n\n#model_icbd1 &lt;- lmer(yield ~ gen + (1|block) +  (1|row:block),\n#                   data = dat, \n#                   na.action = na.exclude)\n#tidy(model_icbd1)\n\n\n\n\nmodel_icbd &lt;- lme(yield ~ gen,\n                  random = ~ 1|block,\n                  data = dat, \n                  na.action = na.exclude)\ntidy(model_icbd)\n\n\n\n\n\n\n9.2.4 Check Model Assumptions\n\ncheck_model(model_icbd)\n\n\n\n\n\n\n\n\n\n\n9.2.5 Inference\n\nanova(model_icbd)\n\nType III Analysis of Variance Table with Satterthwaite's method\n    Sum Sq Mean Sq NumDF  DenDF F value    Pr(&gt;F)    \ngen 1901.1  63.369    30 129.06  17.675 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nemmeans(model_icbd, ~ gen)\n\n gen emmean    SE  df lower.CL upper.CL\n G01   24.6 0.923 153     22.7     26.4\n G02   27.0 0.923 153     25.2     28.8\n G03   32.6 0.923 153     30.8     34.4\n G04   26.9 0.923 153     25.1     28.8\n G05   26.2 0.923 153     24.4     28.0\n G06   32.0 0.923 153     30.1     33.8\n G07   24.2 0.923 153     22.3     26.0\n G08   27.6 0.923 153     25.8     29.4\n G09   29.4 0.923 153     27.6     31.2\n G10   24.5 0.923 153     22.7     26.4\n G11   27.1 0.923 153     25.2     28.9\n G12   29.3 0.923 153     27.4     31.1\n G13   29.9 0.923 153     28.1     31.8\n G14   24.2 0.923 153     22.4     26.1\n G15   26.1 0.923 153     24.3     27.9\n G16   25.9 0.923 153     24.1     27.8\n G17   19.7 0.923 153     17.9     21.5\n G18   25.7 0.923 153     23.9     27.5\n G19   29.0 0.923 153     27.2     30.9\n G20   33.2 0.923 153     31.3     35.0\n G21   31.1 0.923 153     29.3     32.9\n G22   25.2 0.923 153     23.3     27.0\n G23   29.8 0.923 153     28.0     31.6\n G24   33.6 0.923 153     31.8     35.5\n G25   27.0 0.923 153     25.2     28.8\n G26   27.1 0.923 153     25.3     29.0\n G27   23.8 0.923 153     22.0     25.6\n G28   26.5 0.923 153     24.6     28.3\n G29   24.8 0.923 153     22.9     26.6\n G30   36.2 0.923 153     34.4     38.0\n G31   27.1 0.923 153     25.3     28.9\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Incomplete Block Design</span>"
    ]
  },
  {
    "objectID": "chapters/lattice-design.html",
    "href": "chapters/lattice-design.html",
    "title": "10  Lattice Design",
    "section": "",
    "text": "10.1 Background\nLattice designs are convenient for when there is a large number of treatments so that the block size is too big to control for spatial variation effectively. These lattice designs were first developed by Yates (1936) and later, alpha lattice designs were developed as an extension (Patterson and Williams 1976). A special feature of lattice designs is that the number of treatments, t, is related to the block size, k, in one of three forms: t = k2, t = k3, or t = k(k + 1).\nEven though the number of possible treatments is limited, a lattice design may be an ideal design for field experiments with a large number of treatments.\nStatistical model for lattice design:\n\\(Y_{ijk} = \\mu + \\alpha_i + \\gamma_j + \\tau_t  + \\beta_k + \\epsilon_ijk\\)\nwhere, \\(\\mu\\) is the experiment mean, 𝛽 is the row effect, 𝛾 is the column effect, and 𝜏 is the treatment effect.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Lattice Design</span>"
    ]
  },
  {
    "objectID": "chapters/lattice-design.html#example-analysis",
    "href": "chapters/lattice-design.html#example-analysis",
    "title": "10  Lattice Design",
    "section": "10.2 Example Analysis",
    "text": "10.2 Example Analysis\nThe data used in this example is from a balanced lattice experiment in cotton containing 16 treatments in a 4x4 layout in each of 5 replicates. The response variable in this data is the percentage of young flower buds attacked by boll weevils.\nLet’s start the analysis firstly by loading the required libraries:\n\nlme4nlme\n\n\n\nlibrary(lme4); library(lmerTest); library(emmeans); library(performance)\nlibrary(dplyr); library(broom.mixed); library(agridat); library(desplot)\n\n\n\n\nlibrary(nlme); library(broom.mixed); library(emmeans); library(performance)\nlibrary(dplyr); library(agridat); library(desplot)\n\n\n\n\nImport data from agridat package. The data contains . This is a balanced experiment design\n\ndata(cochran.lattice)\ndat2 &lt;- cochran.lattice\nhead(dat2)\n\n     y rep row col trt\n1  9.0  R1   1   1 T10\n2 20.3  R1   1   2 T12\n3 17.7  R1   1   3 T09\n4 26.3  R1   1   4 T11\n5  4.7  R1   2   1 T02\n6  9.0  R1   2   2 T04\n\nstr(dat2)\n\n'data.frame':   80 obs. of  5 variables:\n $ y  : num  9 20.3 17.7 26.3 4.7 9 7.3 8.3 9 6.7 ...\n $ rep: Factor w/ 5 levels \"R1\",\"R2\",\"R3\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ row: int  1 1 1 1 2 2 2 2 3 3 ...\n $ col: int  1 2 3 4 1 2 3 4 1 2 ...\n $ trt: Factor w/ 16 levels \"T01\",\"T02\",\"T03\",..: 10 12 9 11 2 4 1 3 14 16 ...\n\nlibs(desplot)\ndesplot(dat2, y~row*col|rep,\n        text=trt, # aspect unknown, should be 2 or .5\n         main=\"cochran.lattice\")\n\n\n\n\n\n\n\n\n\ndata(burgueno.rowcol)\ndat &lt;- burgueno.rowcol\nhead(dat)\n\n  rep row col gen  yield\n1  R1   1   1 G05 1.5318\n2  R1   1   2 G19 2.2211\n3  R1   1   3 G55 1.4589\n4  R1   1   4 G23 1.2436\n5  R1   1   5 G27 1.8989\n6  R1   1   6 G38 1.3366\n\n\nHere, we can use the desplot() function from the ‘desplot’ package to visualize the plot plan from lattice design.\n\n# Two contiuous reps in 8 rows, 16 columns\ndesplot(dat, yield ~ col*row,\n        out1=rep, # aspect unknown\n        text=gen, shorten=\"none\", cex=0.75,\n        main=\"lattice design\")\n\n\n\n\n\n\n\n\n\n10.2.1 Data integrity checks\n\n\n\n\n\n\nHistogram of the dependent variable.\n\n\n\n\n\n10.2.2 Data integrity checks\n\nstr(dat)\n\n'data.frame':   128 obs. of  5 variables:\n $ rep  : Factor w/ 2 levels \"R1\",\"R2\": 1 1 1 1 1 1 1 1 1 1 ...\n $ row  : int  1 1 1 1 1 1 1 1 1 1 ...\n $ col  : int  1 2 3 4 5 6 7 8 9 10 ...\n $ gen  : Factor w/ 64 levels \"G01\",\"G02\",\"G03\",..: 5 19 55 23 27 38 64 44 14 13 ...\n $ yield: num  1.53 2.22 1.46 1.24 1.9 ...\n\n\n\ndat2$row &lt;- as.factor(dat2$row)\ndat2$col &lt;- as.factor(dat2$col)\n\ndat$row &lt;- as.factor(dat$row)\ndat$col &lt;- as.factor(dat$col)\n\n\nhist(dat2$y, main = \"\", xlab = \"yield\")\n\n\n\n\n\n\n\n\n\n\nFigure 10.1: Histogram of the dependent variable.\n\n\n\n\n\nlme4nlme\n\n\n\nm1_a &lt;- lmer(yield ~ gen + (1|row) + (1|col:rep) + (1|rep),\n           data = dat,\n           na.action = na.exclude)\nsummary(m1_a) \n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: yield ~ gen + (1 | row) + (1 | col:rep) + (1 | rep)\n   Data: dat\n\nREML criterion at convergence: 168.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.1392 -0.4036  0.0000  0.4036  1.1392 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n col:rep  (Intercept) 0.2189   0.4679  \n row      (Intercept) 0.1646   0.4057  \n rep      (Intercept) 0.1916   0.4378  \n Residual             0.1796   0.4238  \nNumber of obs: 128, groups:  col:rep, 32; row, 8; rep, 2\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)   \n(Intercept)  2.325589   0.509444  4.216418   4.565   0.0091 **\ngenG02       0.349182   0.523392 38.995063   0.667   0.5086   \ngenG03       0.371260   0.531293 41.034622   0.699   0.4886   \ngenG04       0.475842   0.527614 43.470779   0.902   0.3721   \ngenG05      -0.601225   0.513762 38.892580  -1.170   0.2490   \ngenG06       0.574869   0.527284 40.643852   1.090   0.2820   \ngenG07       0.244490   0.534996 42.592433   0.457   0.6500   \ngenG08       0.606486   0.527879 43.602192   1.149   0.2569   \ngenG09       0.010630   0.525899 42.747488   0.020   0.9840   \ngenG10       0.509855   0.527357 43.301826   0.967   0.3390   \ngenG11       0.463014   0.535708 42.977451   0.864   0.3922   \ngenG12       0.340678   0.517892 43.399748   0.658   0.5141   \ngenG13      -0.041178   0.483241 34.257292  -0.085   0.9326   \ngenG14       0.132480   0.523679 39.142513   0.253   0.8016   \ngenG15       0.385104   0.526349 43.013937   0.732   0.4683   \ngenG16      -0.148379   0.483194 34.227894  -0.307   0.7606   \ngenG17      -0.016143   0.536067 42.926995  -0.030   0.9761   \ngenG18       0.358218   0.526325 42.993175   0.681   0.4998   \ngenG19       0.734743   0.533892 41.978194   1.376   0.1761   \ngenG20       0.212299   0.521319 40.753106   0.407   0.6860   \ngenG21       0.150212   0.525313 39.814746   0.286   0.7764   \ngenG22      -0.039713   0.497948 37.586406  -0.080   0.9369   \ngenG23       0.325771   0.484472 34.701705   0.672   0.5058   \ngenG24      -0.194686   0.524899 39.587776  -0.371   0.7127   \ngenG25       0.202462   0.514979 39.325398   0.393   0.6963   \ngenG26       0.089411   0.483188 34.221812   0.185   0.8543   \ngenG27       0.218244   0.536631 43.262734   0.407   0.6862   \ngenG28      -0.284235   0.524825 39.538457  -0.542   0.5911   \ngenG29       0.047110   0.515184 39.470339   0.091   0.9276   \ngenG30      -0.213561   0.484574 34.769665  -0.441   0.6621   \ngenG31      -0.034873   0.535702 42.734026  -0.065   0.9484   \ngenG32       1.000827   0.535020 42.606153   1.871   0.0683 . \ngenG33       0.252960   0.507660 40.377683   0.498   0.6210   \ngenG34       0.242054   0.537217 43.562588   0.451   0.6545   \ngenG35       0.213005   0.515262 39.472578   0.413   0.6816   \ngenG36       0.362633   0.525014 42.290855   0.691   0.4935   \ngenG37       0.282612   0.530472 40.615244   0.533   0.5971   \ngenG38      -0.125437   0.537059 43.462082  -0.234   0.8164   \ngenG39       1.261824   0.537018 43.466180   2.350   0.0234 * \ngenG40       0.346211   0.536855 43.369657   0.645   0.5224   \ngenG41      -0.255692   0.522110 41.202626  -0.490   0.6269   \ngenG42       0.744461   0.483144 34.195322   1.541   0.1326   \ngenG43       0.489907   0.535381 42.795749   0.915   0.3653   \ngenG44       0.445400   0.527076 43.156823   0.845   0.4027   \ngenG45       0.728849   0.531497 41.172558   1.371   0.1777   \ngenG46       0.008386   0.527720 43.541892   0.016   0.9874   \ngenG47      -0.173693   0.525585 42.635003  -0.330   0.7427   \ngenG48       0.364422   0.523287 41.600011   0.696   0.4900   \ngenG49       0.283642   0.535631 42.924562   0.530   0.5992   \ngenG50      -0.160189   0.534315 42.227391  -0.300   0.7658   \ngenG51       0.127042   0.526978 43.139993   0.241   0.8106   \ngenG52      -0.277455   0.534469 42.326672  -0.519   0.6064   \ngenG53      -0.401069   0.525510 42.532998  -0.763   0.4496   \ngenG54      -0.221400   0.533965 42.027349  -0.415   0.6805   \ngenG55       0.479012   0.524627 42.079587   0.913   0.3664   \ngenG56       0.232007   0.536959 43.388811   0.432   0.6678   \ngenG57      -0.153493   0.526605 42.888008  -0.291   0.7721   \ngenG58       0.545562   0.523508 41.715186   1.042   0.3034   \ngenG59       0.691577   0.536175 43.006805   1.290   0.2040   \ngenG60      -0.221321   0.517482 37.741262  -0.428   0.6713   \ngenG61       0.205307   0.514087 39.066489   0.399   0.6918   \ngenG62       0.341897   0.534904 42.530411   0.639   0.5261   \ngenG63       0.701913   0.517290 40.377937   1.357   0.1823   \ngenG64       0.066248   0.526946 40.462383   0.126   0.9006   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCorrelation matrix not shown by default, as p = 64 &gt; 12.\nUse print(x, correlation=TRUE)  or\n    vcov(x)        if you need it\n\n\n\n\n\ndat$dummy &lt;- factor(1)\n\nm1_b &lt;- lme(yield ~ gen,\n          random = list(dummy = pdBlocked(list(\n                                  pdIdent(~row - 1),\n                                  pdIdent(~rep - 1),\n                                  pdIdent(~col:rep)))),\n          data = dat, \n          na.action = na.exclude)\n\nVarCorr(m1_b)\n\ndummy = pdIdent(row - 1), pdIdent(rep - 1), pdIdent(col:rep) \n            Variance  StdDev   \nrow1        0.1645986 0.4057076\nrow2        0.1645986 0.4057076\nrow3        0.1645986 0.4057076\nrow4        0.1645986 0.4057076\nrow5        0.1645986 0.4057076\nrow6        0.1645986 0.4057076\nrow7        0.1645986 0.4057076\nrow8        0.1645986 0.4057076\nrepR1       0.1916416 0.4377688\nrepR2       0.1916416 0.4377688\n(Intercept) 0.2188853 0.4678517\ncol1:repR1  0.2188853 0.4678517\ncol2:repR1  0.2188853 0.4678517\ncol3:repR1  0.2188853 0.4678517\ncol4:repR1  0.2188853 0.4678517\ncol5:repR1  0.2188853 0.4678517\ncol6:repR1  0.2188853 0.4678517\ncol7:repR1  0.2188853 0.4678517\ncol8:repR1  0.2188853 0.4678517\ncol9:repR1  0.2188853 0.4678517\ncol10:repR1 0.2188853 0.4678517\ncol11:repR1 0.2188853 0.4678517\ncol12:repR1 0.2188853 0.4678517\ncol13:repR1 0.2188853 0.4678517\ncol14:repR1 0.2188853 0.4678517\ncol15:repR1 0.2188853 0.4678517\ncol16:repR1 0.2188853 0.4678517\ncol1:repR2  0.2188853 0.4678517\ncol2:repR2  0.2188853 0.4678517\ncol3:repR2  0.2188853 0.4678517\ncol4:repR2  0.2188853 0.4678517\ncol5:repR2  0.2188853 0.4678517\ncol6:repR2  0.2188853 0.4678517\ncol7:repR2  0.2188853 0.4678517\ncol8:repR2  0.2188853 0.4678517\ncol9:repR2  0.2188853 0.4678517\ncol10:repR2 0.2188853 0.4678517\ncol11:repR2 0.2188853 0.4678517\ncol12:repR2 0.2188853 0.4678517\ncol13:repR2 0.2188853 0.4678517\ncol14:repR2 0.2188853 0.4678517\ncol15:repR2 0.2188853 0.4678517\ncol16:repR2 0.2188853 0.4678517\nResidual    0.1795838 0.4237733\n\n\n\n\n\n\n\n10.2.3 Check Model Assumptions\nRemember those iid assumptions? Let’s make sure we actually met them.\n\ncheck_model(m1_a)\n\n\n\n\n\n\n\n\n\n\n10.2.4 Inference\nEstimates for each treatment level can be obtained with the ‘emmeans’ package. And we can extract the ANOVA table from model using anova() function.\n\nanova(m1_a)\n\nType III Analysis of Variance Table with Satterthwaite's method\n    Sum Sq Mean Sq NumDF  DenDF F value Pr(&gt;F)\ngen 9.5245 0.15118    63 34.322  0.8418 0.7274\n\n\nEstimated marginal means\n\nemmeans(m1_a, ~ gen)\n\n gen emmean    SE   df lower.CL upper.CL\n G01   2.33 0.515 4.21    0.923     3.73\n G02   2.67 0.515 4.21    1.272     4.08\n G03   2.70 0.515 4.21    1.294     4.10\n G04   2.80 0.515 4.21    1.399     4.20\n G05   1.72 0.515 4.21    0.322     3.13\n G06   2.90 0.515 4.20    1.497     4.30\n G07   2.57 0.515 4.21    1.167     3.97\n G08   2.93 0.515 4.21    1.529     4.33\n G09   2.34 0.515 4.20    0.933     3.74\n G10   2.84 0.515 4.20    1.432     4.24\n G11   2.79 0.515 4.20    1.385     4.19\n G12   2.67 0.515 4.19    1.263     4.07\n G13   2.28 0.515 4.21    0.882     3.69\n G14   2.46 0.515 4.20    1.055     3.86\n G15   2.71 0.515 4.20    1.307     4.11\n G16   2.18 0.515 4.21    0.775     3.58\n G17   2.31 0.515 4.19    0.906     3.71\n G18   2.68 0.515 4.21    1.281     4.09\n G19   3.06 0.514 4.18    1.657     4.46\n G20   2.54 0.515 4.20    1.135     3.94\n G21   2.48 0.515 4.19    1.072     3.88\n G22   2.29 0.515 4.21    0.883     3.69\n G23   2.65 0.515 4.21    1.249     4.05\n G24   2.13 0.515 4.19    0.727     3.53\n G25   2.53 0.515 4.20    1.125     3.93\n G26   2.42 0.515 4.21    1.013     3.82\n G27   2.54 0.515 4.20    1.141     3.95\n G28   2.04 0.515 4.20    0.638     3.44\n G29   2.37 0.515 4.20    0.970     3.78\n G30   2.11 0.515 4.19    0.708     3.52\n G31   2.29 0.514 4.18    0.887     3.69\n G32   3.33 0.515 4.20    1.923     4.73\n G33   2.58 0.514 4.19    1.175     3.98\n G34   2.57 0.515 4.21    1.165     3.97\n G35   2.54 0.515 4.20    1.136     3.94\n G36   2.69 0.515 4.20    1.285     4.09\n G37   2.61 0.515 4.20    1.205     4.01\n G38   2.20 0.515 4.21    0.797     3.60\n G39   3.59 0.515 4.20    2.184     4.99\n G40   2.67 0.515 4.21    1.269     4.07\n G41   2.07 0.514 4.18    0.666     3.47\n G42   3.07 0.515 4.21    1.668     4.47\n G43   2.82 0.515 4.20    1.412     4.22\n G44   2.77 0.515 4.20    1.368     4.17\n G45   3.05 0.515 4.20    1.651     4.46\n G46   2.33 0.515 4.21    0.931     3.74\n G47   2.15 0.515 4.20    0.749     3.56\n G48   2.69 0.515 4.19    1.286     4.09\n G49   2.61 0.515 4.22    1.207     4.01\n G50   2.17 0.514 4.19    0.762     3.57\n G51   2.45 0.514 4.19    1.049     3.86\n G52   2.05 0.514 4.19    0.644     3.45\n G53   1.92 0.515 4.19    0.521     3.33\n G54   2.10 0.514 4.18    0.700     3.51\n G55   2.80 0.514 4.18    1.401     4.21\n G56   2.56 0.515 4.20    1.155     3.96\n G57   2.17 0.515 4.19    0.769     3.58\n G58   2.87 0.514 4.18    1.467     4.28\n G59   3.02 0.514 4.19    1.613     4.42\n G60   2.10 0.516 4.22    0.702     3.51\n G61   2.53 0.515 4.21    1.128     3.93\n G62   2.67 0.514 4.18    1.264     4.07\n G63   3.03 0.514 4.19    1.624     4.43\n G64   2.39 0.515 4.19    0.988     3.80\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\n\n\n\nPatterson, H. D., and E. R. Williams. 1976. “A New Class of Resolvable Incomplete Block Designs.” Biometrika 63 (1): 83–92. https://doi.org/10.2307/2335087.\n\n\nYates, F. 1936. “A New Method of Arranging Variety Trials Involving a Large Number of Varieties.” J Agric Sci 26: 424–55.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Lattice Design</span>"
    ]
  },
  {
    "objectID": "chapters/repeated-measures.html",
    "href": "chapters/repeated-measures.html",
    "title": "11  Repeated measures mixed models",
    "section": "",
    "text": "12 Example Analysis\nFirst, we will start with the first example from a randomized complete block design with repeated measures.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Repeated Measures</span>"
    ]
  },
  {
    "objectID": "chapters/repeated-measures.html#rcbd-repeated-measures",
    "href": "chapters/repeated-measures.html#rcbd-repeated-measures",
    "title": "11  Repeated measures mixed models",
    "section": "12.1 RCBD Repeated Measures",
    "text": "12.1 RCBD Repeated Measures\nThe example shown below contains data from a sorghum trial laid out as a randomized complete block design (5 blocks) with variety (4 varieties) treatment effect. The response variable ‘y’ is the leaf area index assessed in five consecutive weeks on each plot.\nWe need to have time as numeric and factor variable. In the model, to assess the week effect, week was used as a factor (factweek). For the correlation matrix, week needs to be numeric (week).\n\ndat &lt;- agriTutorial::sorghum %&gt;%   \n  mutate(week = as.numeric(factweek),\n         block = as.character(varblock)) \n\n\nTable of variables in the data set\n\n\nblock\nblocking unit\n\n\nReplicate\nreplication unit\n\n\nWeek\nTime points when data was collected\n\n\nvariety\ntreatment factor, 4 levels\n\n\ny\nyield (lbs)\n\n\n\n\n12.1.1 Data Integrity Checks\nLet’s do preliminary data check including evaluating data structure, distribution of treatments, number of missing values, and distribution of response variable.\n\nstr(dat)\n\n'data.frame':   100 obs. of  9 variables:\n $ y        : num  5 4.84 4.02 3.75 3.13 4.42 4.3 3.67 3.23 2.83 ...\n $ variety  : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Replicate: Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 2 2 2 2 2 ...\n $ factweek : Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 1 2 3 4 5 ...\n $ factplot : Factor w/ 20 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 2 2 2 2 2 ...\n $ varweek  : int  1 2 3 4 5 1 2 3 4 5 ...\n $ varblock : int  1 1 1 1 1 2 2 2 2 2 ...\n $ week     : num  1 2 3 4 5 1 2 3 4 5 ...\n $ block    : chr  \"1\" \"1\" \"1\" \"1\" ...\n\n\nIn this data, we have block, factplot, factweek as factor variables and y & week as numeric.\n\ntable(dat$variety, dat$block)\n\n   \n    1 2 3 4 5\n  1 5 5 5 5 5\n  2 5 5 5 5 5\n  3 5 5 5 5 5\n  4 5 5 5 5 5\n\n\nThe cross tabulation shows a equal number of varieties in each block.\n\nggplot(data = dat, aes(y = y, x = factweek, fill = variety)) +\n  geom_boxplot() +  \n  #scale_fill_brewer(palette=\"Dark2\") +\n  scale_fill_viridis_d(option = \"F\") +\n    theme_bw()\n\n\n\n\n\n\n\n\nLooks like variety ‘1’ has the lowest yield and showed drastic reduction in yield over weeks compared to other varieties.\nOne last step before we fit model is to look at the distribution of response variable.\n\nhist(dat$y, main = \"\", xlab = \"yield\")\n\n\n\n\n\n\n\n\n\n\nFigure 12.1: Histogram of the dependent variable.\n\n\n\n\n\n\n12.1.2 Model Building\nLet’s fit the basic model first using lme() from the nlme package.\n\nlm1 &lt;- lme(y ~ variety + factweek + variety:factweek, random = ~1|block/factplot,\n              data = dat,\n              na.action = na.exclude)\n\nThe model fitted above doesn’t account for the repeated measures effect. To account for the variation caused by repeated measurements, we can model the correlation among responses for a given subject which is plot (factor variable) in this case.\nBy adding this correlation structure, what we are implying is to keep each plot independent, but to allowing AR1 or compound symmetry correlations between responses for a given subject, here time variable is week and it must be numeric.\n\ncs1 &lt;- corAR1(form = ~ week|block/factplot,  value = 0.2, fixed = FALSE)\ncs2 &lt;- corCompSymm(form = ~ week|block/factplot,  value = 0.2, fixed = FALSE)\n\nIn the code chunk above, we fitted two correlation structures including AR1 and compound symmetry matrices. Next we will update the model lm1, with these two matrices. In nlme, please search the help tool to know more about functions for different correlation structure classes.\n\nlm2 &lt;- update(lm1, corr = cs1)\nlm3 &lt;- update(lm1, corr= cs2)\n\nNow let’s compare how model fitness differs among models with no correlation structure (lm1), with AR1 correlation structure (lm2), and with compound symmetry structure (lm3). We will compare these models by using anova() or by compare_performance() function from the ‘performance’ library.\n\nanovaperformance\n\n\n\nanova(lm1, lm2, lm3)\n\n    Model df       AIC      BIC   logLik   Test  L.Ratio p-value\nlm1     1 23 18.837478 73.62409 13.58126                        \nlm2     2 24 -2.347391 54.82125 25.17370 1 vs 2 23.18487  &lt;.0001\nlm3     3 24 20.837478 78.00612 13.58126                        \n\n\n\n\n\nresult &lt;- compare_performance(lm1, lm2, lm3)\n\nSome of the nested models seem to be identical and probably only vary in\n  their random effects.\n\nprint_md(result)\n\n\nComparison of Model Performance Indices\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nAIC (weights)\nAICc (weights)\nBIC (weights)\nR2 (cond.)\nR2 (marg.)\nICC\nRMSE\nSigma\n\n\n\n\nlm1\nlme\n-50.5 (&lt;.001)\n-36.0 (&lt;.001)\n9.4 (&lt;.001)\n0.99\n0.37\n0.98\n0.10\n0.13\n\n\nlm2\nlme\n-77.5 (&gt;.999)\n-61.5 (&gt;.999)\n-15.0 (&gt;.999)\n0.97\n0.41\n0.95\n0.15\n0.18\n\n\nlm3\nlme\n-48.5 (&lt;.001)\n-32.5 (&lt;.001)\n14.0 (&lt;.001)\n0.98\n0.37\n0.98\n0.11\n0.14\n\n\n\n\n\n\n\n\nWe prefer to chose model with lower AIC and BIC values. In this scenario, we will move forward with lm2 model containing AR1 structure.\nLet’s run a tidy() on lm2 model to look at the estimates for random and fixed effects.\n\ntidy(lm2)\n\nWarning in tidy.lme(lm2): ran_pars not yet implemented for multiple levels of\nnesting\n\n\n# A tibble: 20 × 7\n   effect term               estimate std.error    df statistic  p.value\n   &lt;chr&gt;  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 fixed  (Intercept)          4.24      0.291     64    14.6   5.44e-22\n 2 fixed  variety2             0.906     0.114     12     7.94  4.05e- 6\n 3 fixed  variety3             0.646     0.114     12     5.66  1.05e- 4\n 4 fixed  variety4             0.912     0.114     12     8.00  3.78e- 6\n 5 fixed  factweek2           -0.196     0.0571    64    -3.44  1.04e- 3\n 6 fixed  factweek3           -0.836     0.0755    64   -11.1   1.60e-16\n 7 fixed  factweek4           -1.16      0.0867    64   -13.3   4.00e-20\n 8 fixed  factweek5           -1.54      0.0943    64   -16.3   1.57e-24\n 9 fixed  variety2:factweek2   0.0280    0.0807    64     0.347 7.30e- 1\n10 fixed  variety3:factweek2   0.382     0.0807    64     4.73  1.26e- 5\n11 fixed  variety4:factweek2  -0.0140    0.0807    64    -0.174 8.63e- 1\n12 fixed  variety2:factweek3   0.282     0.107     64     2.64  1.03e- 2\n13 fixed  variety3:factweek3   0.662     0.107     64     6.20  4.55e- 8\n14 fixed  variety4:factweek3   0.388     0.107     64     3.64  5.55e- 4\n15 fixed  variety2:factweek4   0.228     0.123     64     1.86  6.77e- 2\n16 fixed  variety3:factweek4   0.744     0.123     64     6.06  7.86e- 8\n17 fixed  variety4:factweek4   0.390     0.123     64     3.18  2.28e- 3\n18 fixed  variety2:factweek5   0.402     0.133     64     3.01  3.70e- 3\n19 fixed  variety3:factweek5   0.672     0.133     64     5.04  4.11e- 6\n20 fixed  variety4:factweek5   0.222     0.133     64     1.66  1.01e- 1\n\n\n\n\n12.1.3 Check Model Assumptions\n\ncheck_model(lm2, check = c('normality', 'linearity'))\n\n\n\n\n\n\n\n\n\n\n12.1.4 Inference\nThe ANOVA table suggests a highly significant effect of the variety, week, and variety x week interaction effect.\n\nanova(lm2, type = \"marginal\")\n\n                 numDF denDF   F-value p-value\n(Intercept)          1    64 212.10509  &lt;.0001\nvariety              3    12  28.28895  &lt;.0001\nfactweek             4    64  74.79758  &lt;.0001\nvariety:factweek    12    64   7.03546  &lt;.0001\n\n\nWe can estimate the marginal means for variety and week effect and their interaction using emmeans() function.\n\nmean_1 &lt;- emmeans(lm2, ~ variety)\n\nNOTE: Results may be misleading due to involvement in interactions\n\nmean_1\n\n variety emmean    SE df lower.CL upper.CL\n 1         3.50 0.288  4     2.70     4.29\n 2         4.59 0.288  4     3.79     5.39\n 3         4.63 0.288  4     3.84     5.43\n 4         4.61 0.288  4     3.81     5.40\n\nResults are averaged over the levels of: factweek \nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\nmean_2 &lt;- emmeans(lm2, ~ variety*factweek)\nmean_2\n\n variety factweek emmean    SE df lower.CL upper.CL\n 1       1          4.24 0.291  4     3.43     5.05\n 2       1          5.15 0.291  4     4.34     5.96\n 3       1          4.89 0.291  4     4.08     5.70\n 4       1          5.15 0.291  4     4.35     5.96\n 1       2          4.05 0.291  4     3.24     4.85\n 2       2          4.98 0.291  4     4.17     5.79\n 3       2          5.07 0.291  4     4.27     5.88\n 4       2          4.94 0.291  4     4.14     5.75\n 1       3          3.41 0.291  4     2.60     4.21\n 2       3          4.59 0.291  4     3.79     5.40\n 3       3          4.71 0.291  4     3.91     5.52\n 4       3          4.71 0.291  4     3.90     5.51\n 1       4          3.09 0.291  4     2.28     3.89\n 2       4          4.22 0.291  4     3.41     5.03\n 3       4          4.48 0.291  4     3.67     5.28\n 4       4          4.39 0.291  4     3.58     5.20\n 1       5          2.70 0.291  4     1.89     3.51\n 2       5          4.01 0.291  4     3.20     4.82\n 3       5          4.02 0.291  4     3.21     4.83\n 4       5          3.83 0.291  4     3.03     4.64\n\nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\n\n\n\n\n\n\nTime variable\n\n\n\nHere is a quick step to make sure your fitting model correctly: make sure to have two time variables in your data one being numeric (e.g. ‘day’ as number) and other being factor/character(e.g. ‘day_factor’ as a factor/character). Where, numeric variable is used for fitting correlation matrix and factor/character variable used in model statement to evaluate the time variable effect on response variable.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Repeated Measures</span>"
    ]
  },
  {
    "objectID": "chapters/repeated-measures.html#split-plot-repeated-measures",
    "href": "chapters/repeated-measures.html#split-plot-repeated-measures",
    "title": "11  Repeated measures mixed models",
    "section": "12.2 Split Plot Repeated Measures",
    "text": "12.2 Split Plot Repeated Measures\nRecall, we have evaluated split plot design Chapter 5. In this example we will use the same methodology used in Chapter 5 and update it with repeated measures component.\nNext, let’s load “Yield” data. It is located here.\n\nYield &lt;- read.csv(here::here(\"data/Yield.csv\"))\n\nThis example contains yield data in a split-plot design. The yield data was collected repeatedly from the same Reps over 5 Sample_times. In this data set, we have:\n\nTable of variables in the data set\n\n\nRep\nreplication unit\n\n\nVariety\nMain plot, 2 levels\n\n\nFertilizer\nSplit plot, 3 levels\n\n\nYield\ncrop yield\n\n\nSample_time\ntime points for data collection\n\n\n\n\n12.2.1 Data Integrity Checks\nFirstly, we need to look at the class of variables in the data set.\n\nstr(Yield)\n\n'data.frame':   120 obs. of  6 variables:\n $ Sample_time: int  1 1 1 1 1 1 1 1 1 1 ...\n $ Variety    : chr  \"VAR1\" \"VAR1\" \"VAR1\" \"VAR1\" ...\n $ Fertilizer : int  1 1 1 1 2 2 2 2 3 3 ...\n $ Rep        : int  1 2 3 4 1 2 3 4 1 2 ...\n $ pH         : num  7.07 7.06 7.08 7.09 7.13 7.12 7.15 7.14 7.18 7.18 ...\n $ Yield      : num  0.604 0.595 3.145 3.091 2.415 ...\n\n\nWe will now convert the fertilizer and Rep into factor. In addition, we need to create a new factor variable (sample_time1) to analyze the time effect.\n\n\nFor lme(), independent variables in a character/factor form works fine. But, for mmrm() independent variables must be a factor. Thus, for sake of consistancy, we will be using independent variables in factor class.\n\nYield$Variety &lt;- factor(Yield$Variety) \nYield$Fertilizer &lt;- factor(Yield$Fertilizer) \nYield$Sample_time1 &lt;- factor(Yield$Sample_time) \nYield$Rep &lt;- factor(Yield$Rep)  \n\nTo fit model, we first need to convert Variety, Fertilizer, and Sample_time as factors. In addition, we need to create a new variable named ‘plot’ with a unique value for each plot. In addition, we need a create variable for each subject which is plot in this case and contains a unique value for each plot. The plot variable is needed to model the variation in each plot over the sampling time. The plot will be used as a subject with repeated measures. The subject variable can be factor or numeric but the time (it could be year, or sample_time) has to be a factor.\n\n##creating a plot variable \nYield$plot &lt;- factor(paste(Yield$Rep, Yield$Fertilizer, Yield$Variety, sep='-')) \nYield$Rep2 &lt;- factor(paste(Yield$Rep, Yield$Variety, sep='-')) \ntable(Yield$plot) \n\n\n1-1-VAR1 1-1-VAR2 1-2-VAR1 1-2-VAR2 1-3-VAR1 1-3-VAR2 2-1-VAR1 2-1-VAR2 \n       5        5        5        5        5        5        5        5 \n2-2-VAR1 2-2-VAR2 2-3-VAR1 2-3-VAR2 3-1-VAR1 3-1-VAR2 3-2-VAR1 3-2-VAR2 \n       5        5        5        5        5        5        5        5 \n3-3-VAR1 3-3-VAR2 4-1-VAR1 4-1-VAR2 4-2-VAR1 4-2-VAR2 4-3-VAR1 4-3-VAR2 \n       5        5        5        5        5        5        5        5 \n\n\n\ntable(Yield$Fertilizer, Yield$Variety) \n\n   \n    VAR1 VAR2\n  1   20   20\n  2   20   20\n  3   20   20\n\n\nLooks like a well balanced design with 2 variety treatments and 3 fertilizer treatments.\nBefore fitting a model, let’s check the distribution of the response variable.\n\n\n\n\n\n\n\n\n\nFigure 12.2: Histogram of the dependent variable.\n\n\n\n\n\nhist(Yield$Yield)\n\n\n\n12.2.2 Model fit\nThis data can be analyze either using nlme or mmrm.\nusing lme() from nlme package.\nLet’s say we want to fit a model using AR1 structure as shown in the RCBD repeated measures example. Previously, we used lme() from nlme package to fit the model. In this example, along with nlme() we will also mmrm() function from the mmrm package. In addition, instead of summary() function we will use tidy() function from the ‘broom.mixed’ packageto look at estimates of mixed and random effects. In this model we used tidy(). This will generate a tidy workflow in particular by providing standardized verbs that provide information on estimates, standard errors, confidence intervals, etc.\n\nnlmemmrm\n\n\n\ncorr_str1 = corAR1(form = ~ Sample_time|Rep/Variety/plot, value = 0.2, fixed = FALSE)\n\nmod &lt;- lme(Yield ~ Sample_time1*Variety*Fertilizer,\n                random = ~ 1|Rep/Variety/plot,\n                corr= corr_str1,\n                data = Yield, na.action= na.exclude)\n\n#summary(mod)\n\ntidy(mod)\n\n# A tibble: 30 × 7\n   effect term                      estimate std.error    df statistic   p.value\n   &lt;chr&gt;  &lt;chr&gt;                        &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 fixed  (Intercept)                  1.86      0.708    72     2.63  0.0105   \n 2 fixed  Sample_time12                0.515     0.688    72     0.748 0.457    \n 3 fixed  Sample_time13                0.787     0.674    72     1.17  0.247    \n 4 fixed  Sample_time14                1.35      0.675    72     2.00  0.0496   \n 5 fixed  Sample_time15                2.84      0.675    72     4.21  0.0000731\n 6 fixed  VarietyVAR2                 -0.996     0.861     3    -1.16  0.331    \n 7 fixed  Fertilizer2                  1.27      0.861    12     1.47  0.167    \n 8 fixed  Fertilizer3                  2.07      0.861    12     2.40  0.0333   \n 9 fixed  Sample_time12:VarietyVAR2    0.739     0.974    72     0.759 0.451    \n10 fixed  Sample_time13:VarietyVAR2    0.269     0.954    72     0.282 0.779    \n# ℹ 20 more rows\n\n\n\n\n\nfit1 &lt;- mmrm(\n  formula = Yield ~ Sample_time1*Variety*Fertilizer +  ar1(Sample_time1|Rep/plot),\n  data = Yield)\n\ntidy(fit1)\n\n# A tibble: 30 × 6\n   term                      estimate std.error    df statistic   p.value\n   &lt;chr&gt;                        &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)                  2.86      0.464 12.7      6.16  0.0000387\n 2 Sample_time12                0.656     0.310  1.81     2.12  0.182    \n 3 Sample_time13                1.40      0.414  2.29     3.39  0.0636   \n 4 Sample_time14                1.46      0.484  2.87     3.01  0.0605   \n 5 Sample_time15                2.47      0.549  3.14     4.50  0.0186   \n 6 VarietyVAR2                 -1.07      0.656 12.7     -1.63  0.128    \n 7 Fertilizer2                  1.67      0.656 12.7      2.55  0.0245   \n 8 Fertilizer3                  0.595     0.656 12.7      0.908 0.381    \n 9 Sample_time12:VarietyVAR2   -0.591     0.438  1.81    -1.35  0.321    \n10 Sample_time13:VarietyVAR2   -0.412     0.586  2.29    -0.704 0.546    \n# ℹ 20 more rows\n\n\n\n\n\n\n\n12.2.3 Model diagnostics\nWe will use check_model() from ‘performance’ package to evaluate the model fitness of model fitted using nlme (mod1). However, the mmrm model class doesn’t work with performance package, so we will evalute the model diagnostics by plotting the residuals using base R functions.\n\nnlmemmrm\n\n\n\ncheck_model(mod, check = c('normality', 'linearity'))\n\n\n\n\n\n\n\n\n\n\n\nplot(residuals(fit1), xlab = \"fitted values\", ylab = \"residuals\")\nqqnorm(residuals(fit1)); qqline(residuals(fit1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese diagnostic plots look great! The linearity and homogeneity of variance plots show no trend. The normal Q-Q plots for the overall residuals and for the random effects all fall on a straight line so we can be satisfied with that.\n\n\n12.2.4 Inference\n\nnlmemmrm\n\n\n\nanova(mod, type = \"marginal\")\n\n                                numDF denDF  F-value p-value\n(Intercept)                         1    72 6.899272  0.0105\nSample_time1                        4    72 5.318690  0.0008\nVariety                             1     3 1.338879  0.3310\nFertilizer                          2    12 2.936073  0.0916\nSample_time1:Variety                4    72 0.998154  0.4143\nSample_time1:Fertilizer             8    72 8.158884  &lt;.0001\nVariety:Fertilizer                  2    12 0.237417  0.7923\nSample_time1:Variety:Fertilizer     8    72 0.731698  0.6631\n\n\n\n\n\n#car::Anova(fit1, type = \"III\")\n#Anova.mmrm(fit1, type = \"III\")\n\n\n\n\nNext, we can estimate marginal means and confidence intervals for the independent variables using emmeans().\n\nnlmemmrm\n\n\n\nemmeans(mod,~ Fertilizer)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n Fertilizer emmean    SE df lower.CL upper.CL\n 1            2.44 0.472  3    0.937     3.94\n 2            6.69 0.472  3    5.192     8.20\n 3            8.06 0.472  3    6.554     9.56\n\nResults are averaged over the levels of: Sample_time1, Variety \nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\nemmeans(mod,~ Variety)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n Variety emmean    SE df lower.CL upper.CL\n VAR1      6.09 0.438  3     4.69     7.48\n VAR2      5.37 0.438  3     3.98     6.77\n\nResults are averaged over the levels of: Sample_time1, Fertilizer \nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\n\n\n\nemmeans(fit1,~ Fertilizer)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n Fertilizer emmean    SE   df lower.CL upper.CL\n 1            3.27 0.246 8.02     2.70     3.83\n 2            7.93 0.246 8.02     7.37     8.50\n 3            8.51 0.246 8.02     7.94     9.07\n\nResults are averaged over the levels of: Sample_time1, Variety, Rep \nConfidence level used: 0.95 \n\n emmeans(fit1,~ Variety)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n Variety emmean  SE   df lower.CL upper.CL\n VAR1      6.89 0.2 8.02     6.42     7.35\n VAR2      6.25 0.2 8.02     5.79     6.71\n\nResults are averaged over the levels of: Sample_time1, Fertilizer, Rep \nConfidence level used: 0.95 \n\n\n\n\n\n\n\nAdd a link for further exploring emmeans and contrast statements.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Repeated Measures</span>"
    ]
  },
  {
    "objectID": "chapters/repeated-measures.html#split-split-plot-repeated-measures",
    "href": "chapters/repeated-measures.html#split-split-plot-repeated-measures",
    "title": "11  Repeated measures mixed models",
    "section": "12.3 Split-split plot repeated measures",
    "text": "12.3 Split-split plot repeated measures\nRecall, we have evaluated the split-split experiment design in Chapter 5, where we had a one factor in main-plot, other in subplot and the third factor in sub-subplot. In this example we will be adding a repeated measures statement.\n\nphos &lt;- read.csv(here::here(\"data\", \"split_split_repeated.csv\"))\n\n\n\n\nplot\nexperimental unit\n\n\nblock\nreplication unit\n\n\nPtrt\nMain plot, 2 levels\n\n\nInoc\nSplit plot, 2 levels\n\n\nCv\nSplit-split plot, 5 levels\n\n\ntime\ntime points for data collection\n\n\nP_leaf\nleaf phosphorous content\n\n\n\n\nFactorial design repeated measures\n\nAs explained in Chapter 6, factorial designs involve examining several factors simultaneously\nhttps://search.r-project.org/CRAN/refmans/agridat/html/gregory.cotton.html\n\ndata3 &lt;- agridat::gregory.cotton\nstr(data3)\n\n'data.frame':   144 obs. of  6 variables:\n $ yield   : num  0.99 1.34 1.26 1.44 1.4 1.36 1.23 1.28 1.56 1.64 ...\n $ year    : Factor w/ 2 levels \"Y1\",\"Y2\": 1 1 1 1 1 1 1 1 1 1 ...\n $ nitrogen: Factor w/ 2 levels \"N0\",\"N1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ date    : Factor w/ 4 levels \"D1\",\"D2\",\"D3\",..: 1 1 1 1 1 1 1 1 1 2 ...\n $ water   : Factor w/ 3 levels \"I1\",\"I2\",\"I3\": 1 1 1 2 2 2 3 3 3 1 ...\n $ spacing : Factor w/ 3 levels \"S1\",\"S2\",\"S3\": 1 2 3 1 2 3 1 2 3 1 ...\n\n\n\ntable(data3$nitrogen, data3$date)\n\n    \n     D1 D2 D3 D4\n  N0 18 18 18 18\n  N1 18 18 18 18\n\n\nThe biggest advantage of mixed models is their incredible flexibility. They handle clustered individuals as well as repeated measures (even in the same model). They handle crossed random factors as well as nested\nThe biggest disadvantage of mixed models, at least for someone new to them, is their incredible flexibility. It’s easy to mis-specify a mixed model, and this is a place where a little knowledge is definitely dangerous.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Repeated Measures</span>"
    ]
  },
  {
    "objectID": "chapters/means-and-contrasts.html",
    "href": "chapters/means-and-contrasts.html",
    "title": "12  Marginal Means & Contrasts",
    "section": "",
    "text": "12.1 Estimated Marginal Means\nCompact letter display\n#library(multcomp); library(multcompView)\n#cld(m1, Letters= letters)\nThe letters indicating significant differences can be generated using cld() function from the ‘multcomp’ package”. In the output below, groups sharing a letter in the .group are not statistically different from each other.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Marginal Means and Contrasts</span>"
    ]
  },
  {
    "objectID": "chapters/means-and-contrasts.html#estimated-marginal-means",
    "href": "chapters/means-and-contrasts.html#estimated-marginal-means",
    "title": "12  Marginal Means & Contrasts",
    "section": "",
    "text": "Cautionary Note about CLD\n\n\n\nIt’s important to note that we cannot conclude that treatment levels with the same letter are the same. We can only conclude that they are not different.\nThere is a separate branch of statistics, “equivalence testing” that is for ascertaining if things are sufficiently similar to conclude they are equivalent.\nSee Section 2.0.4 for additional warnings about problems with using compact letter display.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Marginal Means and Contrasts</span>"
    ]
  },
  {
    "objectID": "chapters/variance-components.html",
    "href": "chapters/variance-components.html",
    "title": "13  Variance & Variance Components",
    "section": "",
    "text": "13.1 Unequal Variance",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Variance and Variance Components</span>"
    ]
  },
  {
    "objectID": "chapters/variance-components.html#unequal-variance",
    "href": "chapters/variance-components.html#unequal-variance",
    "title": "13  Variance & Variance Components",
    "section": "",
    "text": "13.1.1 Case 1: Unequal Variance Due to a Factor\n\nvar_ex1 &lt;- here::here(read.csv(\"data\", \"MET_trial_variance.csv\"))\n\n\nvar_ex1$block &lt;- as.character(var_ex1$block)\nhist(var_ex1$yield)\nboxplot(yield ~ site, data = var_ex1)\n\n\nm1_a &lt;- lme(yield ~ site:variety + variety, \n                random = ~ 1 |site/block, \n                na.action = na.exclude, \n                data = var_ex1)\n\n\nm1_b &lt;- update(m1_a, weights = varIdent(form = ~1|site))\n\n\n\nm1_b &lt;- update(m1_a, weights = varIdent(form = ~1|site))\n\nis equivalent to\n\nm1_b &lt;- lme(yield ~ site:variety + variety, \n                random = ~ 1 |site/block,\n                weights = varIdent(form = ~1|site), \n                na.action = na.exclude, \n                data = var_ex1)\n\n\n\n\n13.1.2 Case 2: Variance is related to the fitted values\n\nvar_ex2 &lt;- read.csv(here::here(\"data\", \"single_trial_variance.csv\"))\n\n\nvar_ex1$block &lt;- as.character(var_ex1$block)\nhist(var_ex2$yield)\n\n\nm2_a &lt;- lme(yield ~ variety, \n               random = ~ 1 |block, \n               na.action = na.exclude, \n               data = var_ex2)\n\n\nm2_b &lt;- update(m2_a, weights = varPower())",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Variance and Variance Components</span>"
    ]
  },
  {
    "objectID": "chapters/variance-components.html#coefficient-of-variation",
    "href": "chapters/variance-components.html#coefficient-of-variation",
    "title": "13  Variance & Variance Components",
    "section": "13.2 Coefficient of Variation",
    "text": "13.2 Coefficient of Variation\n\nm2_ave &lt;- fixef(m2_b)[1]\nnames(m2_b) &lt;- NULL\n\n\nm2_cv = sigma(m2_b)/m2_ave*100\nm2_cv\n\n\n13.2.1 Looking at Variance Components\n\nvar_comps &lt;- read.csv(here::here(\"data\", \"potato_tuber_size.csv\"))",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Variance and Variance Components</span>"
    ]
  },
  {
    "objectID": "chapters/additional-resources.html",
    "href": "chapters/additional-resources.html",
    "title": "14  Additional Resources",
    "section": "",
    "text": "14.1 Further Reading",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Additional Resources</span>"
    ]
  },
  {
    "objectID": "chapters/additional-resources.html#further-reading",
    "href": "chapters/additional-resources.html#further-reading",
    "title": "14  Additional Resources",
    "section": "",
    "text": "lme4 vignette for fitting linear mixed models\nMixed-Effects Models in S and S-PLUS thee book for nlme, by José C. Pinheiro and Douglas M. Bates. We used this book extensively for developing this guide. Sadly, it’s both out of print and we could not find a free copy online. However, there are affordable used copies available.\nMixed Effects Models and Extensions in Ecology with R by Alain F. Zuur, Elena N. Ieno, Neil Walker, Anatoly A. Saveliev, and Graham M. Smith.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Additional Resources</span>"
    ]
  },
  {
    "objectID": "chapters/additional-resources.html#other-resources",
    "href": "chapters/additional-resources.html#other-resources",
    "title": "14  Additional Resources",
    "section": "14.2 Other Resources",
    "text": "14.2 Other Resources\n\nEasy Stats a collection of R packages to assist in statistical modelling, with a big focus on linear models.\nMixed Model CRAN Task View a curated list of R packages relevant to mixed modelling. This is a great place to start\nR-SIG-mixed-models mailing list for help and discussion of mixed-model-related questions, course announcements, etc\nGrammar of Experimental Designs by Emi Tanaka. This has a great description of basic principles of experimental design.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Additional Resources</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015.\n“Fitting Linear Mixed-Effects Models Using lme4.” Journal of Statistical\nSoftware 67 (1): 1–48. https://doi.org/10.18637/jss.v067.i01.\n\n\nBolker, Ben, and David Robinson. 2024. Broom.mixed: Tidying Methods\nfor Mixed Models. https://CRAN.R-project.org/package=broom.mixed.\n\n\nHartig, Florian. 2022. DHARMa: Residual Diagnostics for Hierarchical\n(Multi-Level / Mixed) Regression Models. https://CRAN.R-project.org/package=DHARMa.\n\n\nKuznetsova, Alexandra, Per B. Brockhoff, and Rune H. B. Christensen.\n2017. “lmerTest Package: Tests in\nLinear Mixed Effects Models.” Journal of Statistical\nSoftware 82 (13): 1–26. https://doi.org/10.18637/jss.v082.i13.\n\n\nLenth, Russell V. 2022. Emmeans: Estimated Marginal Means, Aka\nLeast-Squares Means. https://CRAN.R-project.org/package=emmeans.\n\n\nLüdecke, Daniel, Mattan S. Ben-Shachar, Indrajeet Patil, Philip\nWaggoner, and Dominique Makowski. 2021. “performance: An R Package for\nAssessment, Comparison and Testing of Statistical Models.”\nJournal of Open Source Software 6 (60): 3139. https://doi.org/10.21105/joss.03139.\n\n\nPatterson, H. D., and E. R. Williams. 1976. “A New\nClass of Resolvable Incomplete\nBlock Designs.” Biometrika 63\n(1): 83–92. https://doi.org/10.2307/2335087.\n\n\nPinheiro, José C., and Douglas M. Bates. 2000. Mixed-Effects Models\nin s and s-PLUS. New York: Springer. https://doi.org/10.1007/b98882.\n\n\nPinheiro, José, Douglas Bates, and R Core Team. 2023. Nlme: Linear\nand Nonlinear Mixed Effects Models. https://CRAN.R-project.org/package=nlme.\n\n\nYates, F. 1936. “A New Method of Arranging Variety Trials\nInvolving a Large Number of Varieties.” J Agric Sci 26:\n424–55.",
    "crumbs": [
      "References"
    ]
  }
]