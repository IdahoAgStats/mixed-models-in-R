# Randomized Complete Block Design

This is a simple model that can serve as a good entrance point to mixed models.

It is very common design where experimental treatments are applied at random to experimental units within each block. The blocks are intended to control for a nuisance source of variation, such as over time, spatial variance, changes in equipment or operators, or myriad other causes.

## Background

The statistical model:

$$y_{ij} = \mu + \alpha_i + \beta_j + \epsilon_{ij}$$ Where:

$\mu$ = overall experimental mean $\alpha$ = treatment effects (fixed) $\beta$ = block effects (random) $\epsilon$ = error terms

$$ \epsilon \sim N(0, \sigma)$$

$$ \beta \sim N(0, \sigma_b)$$

Both the overall error and the block effects are assumed to be normally distributed with a mean of zero and standard deviations of $\sigma$ and $sigma_B$, respectively.

::: callout-note
## 'iid' assumption for error terms

In this model, the error terms, $\epsilon$ are assumed to be "iid", that is, independently and identically distributed. This means they have constant variance and they each individual error term is independent from the others.

This guide will later address examples when this assumption is violated and how to handle it.
:::

## Example Analysis

First, load the libraries for analysis and estimation:

::: {.panel-tabset}

### lme4

```{r, message=FALSE, warning=FALSE}
library(lme4); library(lmerTest); library(emmeans)
library(dplyr)
```

### tidymodels

```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(multilevelmod); library(broom)
```


:::



Next, let's load some data. It is located [here]() if you want to download it yourself (recommended).

This data set is for a single wheat variety trial conducted in Aberdeen, Idaho in 2015. The trial includes 4 blocks and 42 different treatments (wheat varieties in this case). This experiment consists of a series of plots (the experimental unit) laid out in a rectangular grid in a farm field. The goal of this analysis is the estimate the yield and test weight of each variety and the determine the rankings of each variety with regard to yield.

```{r}
var_trial <- read.csv(here::here("data", "aberdeen2015.csv"))
```

|   |   |
|----------|----------------------------------------|    
|block | blocking unit |    
|range | column position for each plot |    
|row | row position for each plot |     
|variety | crop variety (the treatment) being evaluated | 
|stand_pct| percentage of the plot with actual plants growing in them |     
|days_to_heading_julian | Julian days (starting January 1st) until plot "headed" (first spike emerged)|    |height | plant height at crop maturity |     
|lodging | percentage of plants in the plot that fell down and hence could not be harvested |     
|yield_bu_a | yield (bushels per acre) | |test weight | test weight (lbs per bushel of wheat) |       

: Table of variables in the data set  {tbl-rcbd} 

There are several variables present that are not useful for this analysis. The only thing we are concerned about is **block**, **variety**, **yield_bu_a**, and **test_weight**.

### *Data integrity checks*

The first thing is to make sure the data is what we expect. There are two steps:

1.  make sure data are the expected data type
2.  check the extent of missing data
3.  inspect the independent variables and make sure the expected levels are present in the data
4.  inspect the dependent variable to ensure its distribution is following expectations

```{r}
str(var_trial)
```

These look okay except for block, which is currently coded as integer (numeric). We don't want run a regression of block, where block 1 has twice the effect of block 2, and so on. So, converting it to a character will fix that. It can also be converted to a factor, but I find character easier to work with, and ultimately, equivalent to factor conversion

```{r}
var_trial$block <- as.character(var_trial$block)
```

Next, check the independent variables. Running a cross tabulations is often sufficient to ascertain this.

```{r}
table(var_trial$variety, var_trial$block)
```

There are 42 varieties and there appears to be no misspellings among them that might confuse R into thinking varieties are different when they are actually the same. R is sensitive to case and white space, which can make it easy to create near duplicate treatments, such as "eltan" and "Eltan" and "Eltan". There is no evidence of that in this data set. Additionally, it is perfectly balanced, with exactly one observation per treatment per rep. Please note that this does not tell us anything about the extent of missing data.

Here is a quick check I run to count the number of missing data in each column.

```{r}
apply(var_trial, 2, function(x) sum(is.na(x)))
```

Alas, no missing data!

If there were independent variables with a continuous distribution (a covariate), I would plot those data.

Last, check the dependent variable. A histogram is often quite sufficient to accomplish this. This is designed to be a quick check, so no need to spend time making the plot look good.

```{r}
#| label: fig-rcbd_hist
#| fig-cap: "Histogram of the dependent variable."
#| column: margin
#| 
hist(var_trial$yield_bu_a, main = "", xlab = "yield")
```

The range is roughly falling into the range we expect. I know this from talking with the person who generated the data, not through my own intuition. I do not see any large spikes of points at a single value (indicating something odd), nor do I see any extreme values (low or high) that might indicate some larger problems. 

Data are not expected to be normally distributed at this point, so don't bother running any Shapiro-Wilk tests. This histogram is a check to ensure the the data are entered correctly and they appear valid. It requires a mixture of domain knowledge and statistical training to know this, but over time, if you look at these plots with regularity, you will gain a feel for what your data should look like at this stage.

These are not complicated checks. They are designed to be done quickly and should be done for *every analysis* if you not previously already inspected the data as thus. I do this before every analysis and often discover surprising things! Best to discover these things early, since they are likely to impact the final analysis.

This data set is ready for analysis!

### Model Building


::: {.column-margin}

Recall the model:

$$y_{ij} = \mu + \alpha_i + \beta_j + \epsilon_{ij}$$ 

For this model, $\alpha_i$ is the variety effect (fixed) and $\beta_j$ is the block effect (random).

:::

Here is the R syntax for the RCBD statistical model:

::: {.panel-tabset}

### lme4

```{r}
model_rcbd <- lmer(yield_bu_a ~ variety + (1|block),
                   data = var_trial, 
                   na.action = na.exclude)
```

### tidymodels

```{r}
tidy_rcbd <- linear_reg() %>% 
  set_engine("lmer") %>%
  fit(yield_bu_a ~ variety + (1|block), data = var_trial, na.action = na.exclude)
```


:::


The parentheses are used to indicate that 'block' is a random effect, and this particular notation `(1|block)` indicates that a 'random intercept' model is being fit. This is the most common approach. It means there is one overall effect fit for each block. I use the argument `na.action = na.exclude` as instruction for how to handle missing data: conduct the analysis, adjusting as needed for the missing data, and when prediction or residuals are output, please pad them in the appropriate places for missing data so they can be easily merged into the main data set if need be.

::: callout-note
## Formula notation

Formula notation is often used in the R syntax for linear models. It looks like this: $Y ~ X$, where Y is the dependent variable (the response) and X is/are the independent variable(s) (e.g. the experimental treatments).

```{r}
my_formula <- formula(Y ~ treatment1 + treatment2)
class(my_formula)
```

The package 'lmer' has some additional conventions regarding the formula. Random effects are put in parentheses and a `1|` is used to denote random intercepts (rather than random slopes). 
:::

### Check Model Assumptions

Remember those iid assumptions? Let's make sure we actually met them.

There is a special plotting function written for lme4 objects for checking the homoscedasticity (constant variance):

```{r}
#| label: fig-rcbd_error
#| fig-cap: "Plot of residuals versus fitted values"
#| column: margin
#| 
plot(model_rcbd)
```

We are looking for a random and uniform distribution of points. This looks good!

Checking normality requiring first extracting the model residuals with `resid()` and then generaing a qq-plot and line.

```{r}
#| label: fig-rcbd_norm
#| fig-cap: "QQ-plot of residuals"
#| column: margin
#| 
qqnorm(resid(model_rcbd)); qqline(resid(model_rcbd))
```

This is reasonably good. Things do tend to fall apart at the tails.

### Inference

Estimates for each treatment level can be obtained with the 'emmeans' package. 

```{r}
rcbd_emm <- emmeans(model_rcbd, ~ variety)
as.data.frame(rcbd_emm) %>% arrange(desc(emmean))
```

This table indicates the estimated marginal means ("emmean", sometimes called "least squares means"), the standard error ("SE") of those means, the degrees of freedom and the upper and lower bounds of the 95% confidence interval. As an additional step, the emmeans were sorted from largest to smallest.

At this point, the analysis goals have been met: we know the estimated means for each treatment and their rankings.

### Flotsam & Jetsam

Sometimes, researchers want to conduct an ANOVA or add the letters for indicating differences among treatments, even though we have reached the original goals of analysis. It is important to evaluate why you want to do these extra things, what extra information it will bring and what you plan to do with those results.

Running an ANOVA may increase or decrease confidence in the results, depending on what results. That is not at all what ANOVA is intended to do, nor is this what p-values can tell us!

Labelling each treatment, especially when there are this many (42 in total), has its own perils. The biggest problem is that this creates a multiple testing problem: with 42 treatments, a total of 861 comparison are being run (=$42*(42-1)/2$), and then adjusted for multiple tests. With that many tests, a severe adjustment is likely and hence things that are different are not detected. With so many tests, it could be that there is an overall effect due to variety, but they all share the same letter!

The second problem is one of interpretation. Just because two treatments or varieties share a letter does not mean they are equivalent. It only means that they were not found to be different. A funny distinction, but alas. There is an entire branch of statistics, 'equivalence testing' devoted to just this topic - how to test if two things are actually the same. This involves the user declaring a maximum allowable numeric difference for a variable in order to determine if two items are statistically different or equivalent - something that these pairwise comparisons are not doing.

If you want to run ANOVA, it can be done quite easily:

```{r}
anova(model_rcbd)
```

But, please be thoughtful in your usage of it.

::: callout-note
## `na.action = na.exclude`

You may have noticed the final argument for `na.action` in the model statement:

```         
model_rcbd <- lmer(yield_bu_a ~ variety + (1|block),
                   data = var_trial, 
                   na.action = na.exclude)
```

I use the argument `na.action = na.exclude` as instruction for how to handle missing data: conduct the analysis, adjusting as needed for the missing data, and when prediction or residuals are output, please pad them in the appropriate places for missing data so they can be easily merged into the main data set if need be.

Since there are no missing data, this step was not strictly necessary, but it's a good habit to be in.
:::

