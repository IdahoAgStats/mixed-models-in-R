# Randomized Complete Block Design

```{r, echo=FALSE}
par(mar=c(5.1, 6, 4.1, 2.1))
```

This is a simple model that can serve as a good entrance point to mixed models.

It is very common design where experimental treatments are applied at random to experimental units within each block. The blocks are intended to control for a nuisance source of variation, such as over time, spatial variance, changes in equipment or operators, or myriad other causes.

## Background

The statistical model:

$$y_{ij} = \mu + \alpha_i + \beta_j + \epsilon_{ij}$$ Where:

$\mu$ = overall experimental mean $\alpha$ = treatment effects (fixed) $\beta$ = block effects (random) $\epsilon$ = error terms

$$ \epsilon \sim N(0, \sigma)$$

$$ \beta \sim N(0, \sigma_b)$$

Both the overall error and the block effects are assumed to be normally distributed with a mean of zero and standard deviations of $\sigma$ and $sigma_B$, respectively.

::: callout-note
## 'iid' assumption for error terms

In this model, the error terms, $\epsilon$ are assumed to be "iid", that is, independently and identically distributed. This means they have constant variance and they each individual error term is independent from the others.

This guide will later address examples when this assumption is violated and how to handle it.
:::

## Example Analysis

First, load the libraries for analysis and estimation:

::: panel-tabset
### lme4

```{r, message=FALSE, warning=FALSE}
library(lme4); library(lmerTest); library(emmeans)
library(dplyr); library(broom.mixed)
```

### nlme

```{r, message=FALSE, warning=FALSE, eval=FALSE}
library(nlme); library(broom.mixed); library(emmeans)
library(dplyr)
```
:::

Next, let's load some data. It is located [here](https://raw.githubusercontent.com/IdahoAgStats/mixed-models-in-R/main/data/aberdeen2015.csv) if you want to download it yourself (recommended).

This data set is for a single wheat variety trial conducted in Aberdeen, Idaho in 2015. The trial includes 4 blocks and 42 different treatments (wheat varieties in this case). This experiment consists of a series of plots (the experimental unit) laid out in a rectangular grid in a farm field. The goal of this analysis is the estimate the yield and test weight of each variety and the determine the rankings of each variety with regard to yield.

```{r}
var_trial <- read.csv(here::here("data", "aberdeen2015.csv"))
```

|                        |                                                                                  |
|------------------------|----------------------------------------------------------------------------------|
| block                  | blocking unit                                                                    |
| range                  | column position for each plot                                                    |
| row                    | row position for each plot                                                       |
| variety                | crop variety (the treatment) being evaluated                                     |
| stand_pct              | percentage of the plot with actual plants growing in them                        |
| days_to_heading_julian | Julian days (starting January 1st) until plot "headed" (first spike emerged)     |
| lodging                | percentage of plants in the plot that fell down and hence could not be harvested |
| yield_bu_a             | yield (bushels per acre)                                                         |

: Table of variables in the data set {tbl-rcbd}

There are several variables present that are not useful for this analysis. The only thing we are concerned about is **block**, **variety**, **yield_bu_a**, and **test_weight**.

### *Data integrity checks*

The first thing is to make sure the data is what we expect. There are two steps:

1.  make sure data are the expected data type
2.  check the extent of missing data
3.  inspect the independent variables and make sure the expected levels are present in the data
4.  inspect the dependent variable to ensure its distribution is following expectations

```{r}
str(var_trial)
```

These look okay except for block, which is currently coded as integer (numeric). We don't want run a regression of block, where block 1 has twice the effect of block 2, and so on. So, converting it to a character will fix that. It can also be converted to a factor, but I find character easier to work with, and ultimately, equivalent to factor conversion

```{r}
var_trial$block <- as.character(var_trial$block)
```

Next, check the independent variables. Running a cross tabulations is often sufficient to ascertain this.

```{r}
table(var_trial$variety, var_trial$block)
```

There are 42 varieties and there appears to be no misspellings among them that might confuse R into thinking varieties are different when they are actually the same. R is sensitive to case and white space, which can make it easy to create near duplicate treatments, such as "eltan" and "Eltan" and "Eltan". There is no evidence of that in this data set. Additionally, it is perfectly balanced, with exactly one observation per treatment per rep. Please note that this does not tell us anything about the extent of missing data.

Here is a quick check I run to count the number of missing data in each column.

```{r}
apply(var_trial, 2, function(x) sum(is.na(x)))
```

Alas, no missing data!

If there were independent variables with a continuous distribution (a covariate), I would plot those data.

Last, check the dependent variable. A histogram is often quite sufficient to accomplish this. This is designed to be a quick check, so no need to spend time making the plot look good.

```{r, eval=FALSE}
hist(var_trial$yield_bu_a, main = "", xlab = "yield")
```

```{r, echo=FALSE}
#| label: fig-rcbd_hist
#| fig-cap: "Histogram of the dependent variable."
#| column: margin
par(mar=c(5.1, 5, 2.1, 2.1))
hist(var_trial$yield_bu_a, main = "", xlab = "yield", cex.lab = 1.8, cex.axis = 1.5)
```

The range is roughly falling into the range we expect. I know this from talking with the person who generated the data, not through my own intuition. I do not see any large spikes of points at a single value (indicating something odd), nor do I see any extreme values (low or high) that might indicate some larger problems.

Data are not expected to be normally distributed at this point, so don't bother running any Shapiro-Wilk tests. This histogram is a check to ensure the the data are entered correctly and they appear valid. It requires a mixture of domain knowledge and statistical training to know this, but over time, if you look at these plots with regularity, you will gain a feel for what your data should look like at this stage.

These are not complicated checks. They are designed to be done quickly and should be done for *every analysis* if you not previously already inspected the data as thus. I do this before every analysis and often discover surprising things! Best to discover these things early, since they are likely to impact the final analysis.

This data set is ready for analysis!

### Model Building

::: column-margin
Recall the model:

$$y_{ij} = \mu + \alpha_i + \beta_j + \epsilon_{ij}$$

For this model, $\alpha_i$ is the variety effect (fixed) and $\beta_j$ is the block effect (random).
:::

Here is the R syntax for the RCBD statistical model:

::: panel-tabset
### lme4

```{r}
model_rcbd <- lmer(yield_bu_a ~ variety + (1|block),
                   data = var_trial, 
                   na.action = na.exclude)
```

### nlme

```{r, eval=FALSE}
model_rcbd <- lme(yield_bu_a ~ variety,
                  random = ~ 1|block,
                  data = var_trial, 
                  na.action = na.exclude)
```
:::

The parentheses are used to indicate that 'block' is a random effect, and this particular notation `(1|block)` indicates that a 'random intercept' model is being fit. This is the most common approach. It means there is one overall effect fit for each block. I use the argument `na.action = na.exclude` as instruction for how to handle missing data: conduct the analysis, adjusting as needed for the missing data, and when prediction or residuals are output, please pad them in the appropriate places for missing data so they can be easily merged into the main data set if need be.

### Check Model Assumptions

Remember those iid assumptions? Let's make sure we actually met them.

There is a special plotting function written for lme4 objects for checking the homoscedasticity (constant variance):

```{r, eval=FALSE}
plot(model_rcbd, resid(., scaled=TRUE) ~ fitted(.), 
     xlab = "fitted values", ylab = "studentized residuals")
```

```{r, echo=FALSE}
#| label: fig-rcbd_error
#| fig-cap: "Plot of residuals versus fitted values"
#| column: margin
#| 
par(mar=c(5.1, 5, 2.1, 2.1))
plot(model_rcbd, resid(., scaled=TRUE) ~ fitted(.), 
     xlab = "fitted values", ylab = "studentized residuals",
     cex.lab = 1.8, cex.axis = 1.5)
```

We are looking for a random and uniform distribution of points. This looks good!

::: column-margin
The same code works for nlme and lme4-generated models.
:::

Checking normality requiring first extracting the model residuals with `resid()` and then generating a qq-plot and line.

```{r, eval=FALSE}
qqnorm(resid(model_rcbd), main = NULL); qqline(resid(model_rcbd))
```

```{r, echo=FALSE}
#| label: fig-rcbd_norm
#| fig-cap: "QQ-plot of residuals"
#| column: margin

par(mar=c(5.1, 5, 2.1, 2.1))
qqnorm(resid(model_rcbd), main = NULL, cex.lab = 1.8, cex.axis = 1.5); qqline(resid(model_rcbd))
```

This is reasonably good. Things do tend to fall apart at the tails.

### Inference

Estimates for each treatment level can be obtained with the 'emmeans' package.

```{r}
# same syntax for lme4 & nlme
rcbd_emm <- emmeans(model_rcbd, ~ variety)
as.data.frame(rcbd_emm) %>% arrange(desc(emmean))
```

This table indicates the estimated marginal means ("emmean", sometimes called "least squares means"), the standard error ("SE") of those means, the degrees of freedom and the upper and lower bounds of the 95% confidence interval. As an additional step, the emmeans were sorted from largest to smallest.

At this point, the analysis goals have been met: we know the estimated means for each treatment and their rankings.

If you want to run ANOVA, it can be done quite easily:

```{r}
# same syntax for lme4 & nlme
anova(model_rcbd)
```

::: callout-note
## `na.action = na.exclude`

You may have noticed the final argument for `na.action` in the model statement:

```         
model_rcbd <- lmer(yield_bu_a ~ variety + (1|block),
                   data = var_trial, 
                   na.action = na.exclude)
```

I use the argument `na.action = na.exclude` as instruction for how to handle missing data: conduct the analysis, adjusting as needed for the missing data, and when prediction or residuals are output, please pad them in the appropriate places for missing data so they can be easily merged into the main data set if need be.

Since there are no missing data, this step was not strictly necessary, but it's a good habit to be in.
:::
