---
title: "RCBD Design with Several Crossed Factors"
---

```{r, include=FALSE, echo=FALSE}
source(here::here("settings.r"))
```

## Background

Factorial design involves studying the impact of multiple factors simultaneously. Each factor can have multiple levels, and combinations of these levels form the experimental conditions. This design allows us to understand the main effects of individual factors and their interactions on the response variable. The statistical model for factorial design is:
$$y_{ij} = \mu +  \tau_i+ \beta_j + (\tau\beta)_{ij} + \epsilon_{ij}$$ 
Where: 

$\mu$ = experiment mean

$\tau$ = effect of factor A

$\beta$ = effect of factor B 

$\tau\beta$ = interaction effect of factor A and B.


Assumptions of this model includes: independent and identically distributed error terms with a constant variance $\sigma^2$.

## Example Analysis

First step is to load the libraries required for the analysis:

::: panel-tabset
### lme4

```{r, message=FALSE, warning=FALSE}
library(lme4); library(lmerTest); library(emmeans)
library(dplyr); library(broom.mixed); library(performance)
```

### nlme

```{r, message=FALSE, warning=FALSE}
library(nlme); library(broom.mixed); library(emmeans)
library(dplyr); library(performance)
```
:::

Next, we will load the dataset named 'cochran.factorial' from the **agridat** package. This data comprises a yield response of beans to different levels of manure (d), nitrogen (n), phosphorus. 

The objective of this example is evaluate the individual and interactive effect of "d", "n", "p", and "k" treatments on bean yield.

Note, while importing the data, d, n, p, and k were converted into factor variables using the `mutate()` function from **dplyr** package. This helps in reducing the extra steps of converting each single variable to factor manually.

```{r warning=FALSE, message=FALSE}
library(agridat)
data1 <- agridat::cochran.factorial %>% 
  mutate(d = as.factor(d),
         n = as.factor(n),
         p = as.factor(p),
         k = as.factor(k))
```

|       |                                |
|-------|--------------------------------|
| block | blocking unit                  |
| rep   | replication unit               |
| trt   | treatment factor, 16 levels    |
| d     | dung treatment, 2 levels       |
| n     | nitrogen treatment, 2 levels   |
| p     | phosphorus treatment, 2 levels |
| k     | potassium treatment, 2 levels  |
| yield | yield (lbs)                    |

: Table of variables in the data set {tbl-factorial}

### Data Integrity Checks

First step is to Verify the class of variables, where rep, block, d, n, p, and k are supposed to be a factor/character and yield should be numeric.

```{r}
str(data1)
```

This looks good.

The next step is to inspect the independent variables and make sure the expected levels are present in the data.

```{r}
table(data1$d, data1$n, data1$p, data1$k)
```

The design looks well balanced.

The last step is to inspect the dependent variable to ensure it looks as expected.

```{r, echo=FALSE}
#| label: fig-factorial_hist
#| fig-cap: "Histogram of the dependent variable."
#| column: margin
par(mar=c(5.1, 5, 2.1, 2.1))
hist(data1$yield, main = "", xlab = "yield", cex.lab = 1.8, cex.axis = 1.5, , col = "turquoise")
```

```{r, eval=FALSE}
hist(data1$yield, , col = "turquoise")
```

No extreme values are observed in the dependent variable, and the distribution looks as expected.  

### Model fitting

Model fitting with R is exactly the same as shown in previous chapters: we need to include all fixed effects, as well as the interaction, which is represented by using the colon indicator ‘:’. 

The model syntax is:

`yield ~ d + n + p + k + d:n + d:p + d:k + n:p + n:k + p:k + d:n:p:k`

which can be abbreviated as:

`yield ~ d*n*p*k`

In this analysis, "d", "n", "p", & "k" are fixed factors and block is a random effect.

::: panel-tabset
### lme4

```{r, warning=FALSE}
model1_lmer <- lmer(yield ~ d*n*p*k + (1|block),
                   data = data1, 
                   na.action = na.exclude)
tidy(model1_lmer)
```

### nlme

```{r}
model2_lme <- lme(yield ~ d*n*p*k,
              random = ~ 1|block,
              data = data1, 
              na.action = na.exclude)
tidy(model2_lme)
```
:::

:::: column-margin
::: callout-note
The `tidy()` function from the **broom.mixed** package provides a short summary output of the model.
:::
::::

### Check Model Assumptions

::: panel-tabset
### lme4

```{r, fig.height=3}
check_model(model1_lmer, check = c('normality', 'linearity'))
```

### nlme

```{r, fig.height=3}
check_model(model2_lme, check = c('normality', 'linearity'))
```
:::

The linearity and homogeneity of variance plots show no trend.

this is not qq-plot -- The normal Q-Q plots for the overall residuals and for the random effects all fall nearly on a straight line so we can be satisfied with that.
mention: modest departures

### Inference

We can get an ANOVA table for the linear mixed model using the function `anova()`, which works for both `lmer()` and `lme()` models.

::: panel-tabset
### lme4

```{r}
anova(model1_lmer, type = "3")
```
### nlme
```{r, fig.height=3}
anova(model2_lme, type = "marginal")
```
:::

Here we did not observe any difference in group variance of interaction effects. Among all treatment factors, only "p" had a significant effect on bean yield when evaluated at significance level of 0.05.


Let’s find estimates for some of the factors such as N, P and the N-by-K interactions in order to understand the combined effect of nitrogen and potassium on bean yield.

::: panel-tabset
### lme4
```{r}
emmeans(model1_lmer, specs = ~ n)
emmeans(model1_lmer, specs = ~ p)
emmeans(model1_lmer, specs = ~ n:p)
```

### nlme
```{r}
emmeans(model2_lme, specs = ~ n)
emmeans(model2_lme, specs = ~ p)
emmeans(model2_lme, specs = ~ n:p)
```
:::

The code chunks showed above estimated marginal means for individual effects of "n" and "p" and their interaction (n:p) effect on bean yield. Make sure to give an attention to the warning message that means are averaged over the "d", "p", and "k" levels. It's an important detail that needs to be pointed out while making conclusions. When working with factorial designs make sure to carefully interpret ANOVA and estimated marginal means for main and interaction effects.
