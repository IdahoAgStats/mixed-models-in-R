# Repeated measures mixed models

```{r, include=FALSE, echo=FALSE}
source(here::here("settings.r"))
```

In the previous chapters we have covered how to run linear mixed models for different experiment designs. All of the examples in those chapters were independent measure designs, where each subject was assigned to a different treatment. Now we will move on to experiment with repeated measures effects (also called **"longitudinal data"**).

Studies that involve repeated observations of the exact same experimental units (or subjects) requires a repeated measures component in analysis to properly model correlations across time for each subject. This is common in studies that are evaluated across different time periods. For example, if samples are collected over the different time periods from same subject, we have to model the repeated measures effect while analyzing the main effects.

In these models, the 'iid' assumption (independently and identically distributed) is being violated often, so we need to introduce specialized covariance structures that can account for these correlations between error terms.

Fitting models with correlated observations requires new libraries including **mmrm** and **nlme**. The **lme4** package allows random effects only. In this chapter, we will analyze the data with repeated measures from different experiment designs including randomized complete block design, split plot, and split-split plot design.

There are several types of covariance structures:

| Structure name | nlme function | mmrm function | Assumption |
|------------------------|------------------------|------------------------|
| Autoregressive (AR1) | `corAR1()` | `ar1`  | observations which are more proximate are more correlated than measures that are more distant |
| Compound symmetry | `corCompSymm()` |   | Correlation is equal for all time gaps between observations |
| Unstructured | `CorSymm()` |    | Correlations are different for the various time gaps |

::: column-margin
To read more about selecting appropriate covariance structure based on your data, please refer to this [link](https://www.ars.usda.gov/ARSUserFiles/80000000/StatisticsGroupWebinars/Appendix%20E%20-%20Selecting%20a%20Covariance%20Structure.pdf).
:::

The repeated measures syntax in **nlme** follow this convention: 

`corr = corAR1(value = 0, form = ~ t|g, fixed = FALSE)`. 

The argument for 'value' is the starting value for iterations (zero, unless you specify something else), and if `fixed = FALSE` (the current nlme default), this value will be allowed to change during the model fitting process. The argument structure for `form`, `~ t` or `~ t|g`, specifying a time covariate $t$ and, optionally a grouping factor g (if a group factor is not specified, the observation order will be used). When we use `~t|g` form, the correlation structure is assumed to apply only to observations within the same grouping level. The covariate for this correlation structure must be a **integer value**. `corCompSymm()` and `corSymm()` follow the same argument syntax. 

There are other covariance structures (e.g. `corARMA()`, `corCAR1()`), but we have found that `corAR1()` and `corCompSymm()` work for most circumstances. Type `?cor` in an R console for more options and details on the syntax

For this chapter we will be analyzing using **nlme** and **mmrm** packages. So, let's start with loading the required libraries for this analysis. These packages have the easiest implementation of heterogenous error structures. While [it is possible](https://bbolker.github.io/mixedmodels-misc/notes/varmats.html) to do this with **lme4** is requires extensive comfort programming in **lme4**.

::: panel-tabset
### nlme
```{r, message=FALSE, warning=FALSE}
library(nlme); library(performance); library(emmeans)
library(dplyr); library(broom.mixed); library(ggplot2)
```

### mmrm
```{r, message=FALSE, warning=FALSE}
library(mmrm); library(performance); library(emmeans)
library(dplyr); library(broom.mixed); library(ggplot2)
```
:::

# Example Analysis

First, we will start with the first example with randomized complete block design with repeated measures. 

## RCBD Repeated Measures

The example shown below contains data from a sorghum trial laid out as a randomized complete block design (5 blocks) with variety (4 varieties) treatment effect. The response variable 'y' is the leaf area index assessed in five consecutive weeks on each plot.

We need to have time as numeric and factor variable. In the model, to assess the week effect, week was used as a factor (factweek). For the correlation matrix, week needs to be numeric (week).

```{r, eval=FALSE, echo=FALSE}
#save data from the package
repeated_meas <- agriTutorial::sorghum %>%   
  mutate(week = as.numeric(factweek),
         block = as.character(varblock)) 

write.csv(repeated_meas, here::here("data", "sorghum.csv"), row.names = FALSE)
```

```{r}
sorghum <- read.csv(here::here("data", "sorghum.csv")) |> 
  mutate(block = as.character(varblock),
         factweek = as.character(factweek),
         variety = as.character(variety))
```

|           |                                     |
|-----------|-------------------------------------|
| block     | blocking unit                       |
| Replicate | replication unit                    |
| Week      | Time points when data was collected |
| variety   | treatment factor, 4 levels          |
| y         | leaf area index                        |

: Table of variables in the data set {tbl-rcbd-rp}

### Data Integrity Checks

Let's do preliminary data check including evaluating data structure, distribution of treatments, number of missing values, and distribution of response variable.

- Check structure of the data

```{r}
str(sorghum)
```

In this data, we have block, factplot, factweek as factor variables and y & week as numeric.

- Inspect the independent variables

```{r}
table(sorghum$variety, sorghum$block)
```

The cross tabulation shows a equal number of variety treatments in each block.

- Check the extent of missing data

```{r}
colSums(is.na(sorghum))
```
No missing values 

- Inspect the dependent variable

```{r}
ggplot(data = sorghum, aes(y = y, x = factweek, fill = variety)) +
  geom_boxplot() +  
    theme_bw()
```
Looks like variety '1' has the lowest yield and showed drastic reduction in yield over weeks compared to other varieties.
One last step before we fit model is to look at the distribution of response variable.

```{r, eval=FALSE}
hist(sorghum$y, main = NA, xlab = "leaf area index")
```

```{r, echo=FALSE}
#| label: fig-rcbd_hist
#| fig-cap: "Histogram of the dependent variable."
#| column: margin
par(mar=c(5.1, 5, 2.1, 2.1))
hist(sorghum$y, main = NA, xlab = "leaf area index", cex.lab = cex_lab, cex.axis = cex_axis, col = base_plot_color)
```

### Model Building

Let's fit the model first using `lme()` from the **nlme** package.

```{r}
lm1 <- lme(y ~ variety + factweek + variety:factweek,
           random = ~1|block/factplot,
           data = sorghum,
           na.action = na.exclude)
```

The model fitted above doesn't account for the repeated measures effect. To account for the variation caused by repeated measurements, we can model the correlation among responses for a given subject which is plot (factor variable) in this case.

By adding this correlation structure, we are accounting for variation caused by repeated measurements over weeks for each plot. The AR1 structure assumes that data points collected more proximate are more correlated. Whereas, the compound symmetry structure assumes that correlation is equal for all time gaps. Here, we will fit model with both correlation structures and compare models to find out the best fit model.

In this analysis, time variable is `week` and it must be numeric.

```{r}
cs1 <- corAR1(form = ~ week|block/factplot,  value = 0.2, fixed = FALSE)
cs2 <- corCompSymm(form = ~ week|block/factplot,  value = 0.2, fixed = FALSE)
```

In the code chunk above, we fitted two correlation structures including AR1 and compound symmetry matrices. Next we will update the model lm1, with these two matrices. 

```{r}
lm2 <- update(lm1, corr = cs1)
lm3 <- update(lm1, corr= cs2)
```

Now let's compare how model fitness differs among models with no correlation structure (lm1), with AR1 correlation structure (lm2), and with compound symmetry structure (lm3). We will compare these models by using `anova()` or by `compare_performance()` function from the **performance** library.

::: panel-tabset
### anova

```{r}
anova(lm1, lm2, lm3)
```

Let's compare the models performance to select a model that fits better.

```{r}
result <- compare_performance(lm1, lm2, lm3)
print_md(result)
```
:::

We prefer to chose model with lower AIC and BIC values. In this scenario, we will move forward with lm2 model containing AR1 structure.

Let's run a `tidy()` on lm2 model to look at the estimates for random and fixed effects.

```{r}
tidy(lm2)
```

### Check Model Assumptions

```{r, fig.height=3}
#| message: false
#| warning: false
check_model(lm2,  check = c('linearity','qq', 'reqq'), detrend=FALSE, alpha=0)
```

### Inference

```{r}
anova(lm2, type = "marginal")
```
The ANOVA table suggests a significant effect of the variety, week, and variety x week interaction effect.

We can estimate the marginal means for variety and week effect and their interaction using `emmeans()` function.

```{r}
mean_1 <- emmeans(lm2, ~ variety)
mean_1

mean_2 <- emmeans(lm2, ~ variety*factweek)
mean_2
```

::: callout-tip
## Time variable

Here is a quick step to make sure your fitting model correctly: make sure to have two time variables in your data one being numeric (e.g. 'day' as number) and other being factor/character(e.g. 'day_factor' as a factor/character). Where, numeric variable is used for fitting correlation matrix and factor/character variable used in model statement to evaluate the time variable effect on response variable.
:::

## Split Plot Repeated Measures

Recall, we have evaluated split plot design [**Chapter 7**](split-plot-design.qmd). In this example we will use the same methodology used in Chapter 5 and update it with repeated measures component.

Next, let's load an alfalfa intercropping data set. This was from an irrigation and intercropping experiment was conducted in southern Idaho. Irrigation is the main plot, intercropping is the split plot, and the in-season alfalfa cutting ("cutting") is the unit for repeated measures. 

```{r}
alfalfa <- read.csv(here::here("data/alfalfa_intercropping.csv"))
```

This example contains yield data in a split-plot design. The yield data was collected repeatedly from the same Reps over 5 Sample_times. In this data set, we have:

|             |                                 |
|-------------|---------------------------------|
| cutting       | time points for data collection   |
| irrigation    | Main plot, 2 levels             |
| plot  |   experimental unit          |
| block       |    replication unit     |
| intercrop | Split plot, 3 levels |
| yield | crop yield |
| row |  spatial position by row |
| col | spatial position by column |

: Table of variables in the data set {tbl-split-plot-rp}

### Data Integrity Checks

- Check structure of the data

First, we need to look at the class of variables in the data set.

```{r}
str(alfalfa)
```

We will now convert the fertilizer and rep to factors. In addition, we need to create a new factor variable (sample_time1) to analyze the time effect.

::: column-margin
For `lme()`, independent variables in a character/factor form works fine. But, for `mmrm()` independent variables must be a factor. Thus, for consistency, we will be using independent variables in factor class.
:::

```{r}
alfalfa <- alfalfa |> 
  mutate(cut_num = as.numeric(as.factor(cutting))) |> 
  mutate_at(c("cutting", "irrigation", "plot", "block"), as.factor)
```


To fit the model, we first need to convert Variety, Fertilizer, and Sample_time to factors. In addition, we need to create a variable for each subject which is plot in this case and contains a unique value for each plot. The plot variable is needed to model the variation in each plot over the sampling time. The plot will be used as a subject with repeated measures. The subject variable can be factor or numeric but the time (it could be year, or sample_time) has to be a factor.

- Inspect the independent variables


```{r}
table(alfalfa$intercrop, alfalfa$irrigation) 
```
Looks like a balanced design with 2 irrigation treatments and 10 intercropping treatments.

- Check the extent of missing data

```{r}
colSums(is.na(alfalfa))
```

- Inspect the dependent variable

Before fitting a model, let's check the distribution of the response variable.
```{r, echo=FALSE}
#| label: fig-split-plot_hist
#| fig-cap: "Histogram of the dependent variable"
#| column: margin
par(mar=c(5.1, 5, 2.1, 2.1))
hist(alfalfa$yield, main = NA, xlab = "yield", cex.lab = cex_lab, cex.axis = cex_axis, col = base_plot_color)
```

```{r, eval=FALSE}
hist(Yield$Yield, xlab = "yield", main = NA)
```
### Model fit

This data can be analyzed either using `nlme` or `mmrm`.

Let's say we want to fit a model using AR1 structure as shown in the RCBD repeated measures example. Previously, we used `lme()` from **nlme** package to fit the model. In this example, along with `nlme()` we will also `mmrm()` function from the **mmrm** package. In addition, instead of `summary()` function we will use `tidy()`  function from the **broom.mixed** package to look at estimates of mixed and random effects. This will generate a tidy workflow in particular by providing standardized verbs that provide information on estimates, standard errors, confidence intervals, etc.


```{r}
#| warning: false

corr_str1 = corAR1(form = ~ cut_num|block/irrigation/intercrop/plot, value = 0.2, fixed = FALSE)

fit1 <- lme(yield ~ irrigation*intercrop*cutting,
                random = ~ 1|block/irrigation/intercrop/plot,
                corr= corr_str1,
                data = alfalfa, na.action= na.exclude)
```

### Check Model Assumptions

We will use `check_model()` from **performance** package to evaluate the model fitness of model fitted using nlme (mod1). However, the `mmrm()` model class doesn't work with performance package, so we will evaluate the model diagnostics by plotting the residuals using base R functions.

```{r, fig.height=3}
#| warning: false
#| message: false
check_model(fit1, check = c('linearity', 'qq'), detrend=FALSE, alpha = 0)
```


These diagnostic plots look fine. The linearity and homogeneity of variance plots show no trend. The normal Q-Q plots for the overall residuals and for the random effects fall on a straight line so we can be satisfied with that.

### Inference

```{r}
#| warning: false
#| message: false
anova(fit1, type = "marginal")
```
 
Next, we can estimate marginal means and confidence intervals at different levels of the independent variables using `emmeans()`.

```{r}
emmeans(fit1,~ cutting)
emmeans(fit1,~ intercrop)
```


::: column-margin
To explore more about contrasts and emmeans please refer to [**Chapter 13**](means-and-contrasts.qmd).
:::

## Split-split Plot Repeated Measures {#ssp-rm}

Recall, we have evaluated the split-split experiment design in [**Chapter 8**](split-split-plot-design.qmd), where we had a one factor in main-plot, other in subplot and the third factor in sub-subplot. In this example we will be adding a repeated measures component to the split-split plot design.

```{r}
phos <- read.csv(here::here("data", "split_split_repeated.csv"))
```

|             |                                 |
|-------------|---------------------------------|
| plot        | experimental unit                |
| block     | replication unit            |
| Ptrt  | Main plot, 2 levels           |
| Inoc       | Split plot, 2 levels         |
| Cv    | Split-split plot, 5 levels
| time | time points for data collection |
| P_leaf | leaf phosphorous content  |

### Data Integrity Checks

- Check structure of the data

```{r}
str(phos)
```

We need two variables for time, one formatted as a factor and the other numeric.

```{r, warning=FALSE, message=FALSE}
phos1 <- phos %>%   
  mutate(
    time = as.factor(time), 
    time_num = as.numeric(time),
    rep = as.character(bloc),
    plot = as.character(plot)) 
```

- Inspect the independent variables

```{r}
table(phos1$Cv, phos1$Ptrt, phos1$Inoc) 
```
Looks like a well balanced design with 2 variety treatments and 3 fertilizer treatments.

- Check the extent of missing data

```{r}
colSums(is.na(phos1))
```
No missing values in data. 

- Inspect the dependent variable

Before fitting a model, let's check the distribution of the response variable.
```{r, echo=FALSE}
#| label: fig-split-split-plot_hist
#| fig-cap: "Histogram of the dependent variable."
#| column: margin
par(mar=c(5.1, 5, 2.1, 2.1))
hist(phos1$P_leaf, main = NA, xlab = "P leaf", cex.lab = cex_lab, cex.axis = cex_axis, col = base_plot_color)
```

```{r, echo=FALSE}
#| label: fig-split-split-plot_boxplot
#| fig-cap: "Boxplot of P concentration by stage"
#| column: margin
par(mar=c(5.1, 5, 2.1, 2.1))
```

```{r, eval=FALSE}
hist(phos1$P_leaf, main = NA, xlab = "P leaf")
boxplot(P_leaf ~ time, data = phos1, main = NA)
```
::: callout-warning
## distribution of dependent variables

Here note that we observed uneven distribution of response variable with a bimodal distribution and a noticeable gap in the 500 to 1500 range. Given this odd distribution, it may be tempting to consider a transformation in order to attempt to impose normality. It's important to remember that the assumption of normality applies to the residuals, not the raw data. Plotting the data is for checking the data looks *as expected*, a judgement that requires some knowledge of the experiment (this was Julia Piaskowski's PhD research). In this case, the time points, PT1 and PT2, reflect early wheat growth stages (tillering and jointing, respectively), and the final time point is senescent leaf tissue at grain maturity. At that physiological stage, it is normal for phosphorus leaf concentration to be much lower. Since the data look as expected, we will proceed with a general linear model and evaluate the residuals from the model fitting process when deciding if a non-normal distribution is appropriate for the data.  
:::

### Model fit

```{r}
#| warning: false
corr_str1 = corCompSymm(form = ~ time_num|rep/Ptrt/Inoc/plot, value = 0.2, fixed = FALSE)

pfit1 <- lme(P_leaf ~ time*Ptrt*Inoc*Cv,
                random = ~ 1|rep/Ptrt/Inoc/plot,
                corr = corr_str1,
                data = phos1, na.action= na.exclude)

```

### Check model assumptions 

```{r, fig.height=3}
#| warning: false
#| message: false
check_model(pfit1, check = c('linearity', 'qq'), detrend=FALSE, alpha =0)
```

This model fit a first glance is not ideal, but that LOESS line is trying to model a space where there are no data (between 500 and 1500 ppm P leaf concentration), so that can introduce artifacts. **Performance** does have an option for testing for heteroscedascity:

```{r}
check_heteroscedasticity(pfit1)
```
These results do confirm our suspicions that the residuals were not as heteroscedastic as they first appeared. However, the boxplot indicated a a difference in variance for each time time point. This addressed in the [chapter on variance components](variance-components.qmd). 

```{r, echo=FALSE, eval=FALSE}
save(pfit1, corr_str1, file = here::here("data/phos_fit1.RData"))
```


### Inference

```{r}
#| warning: false
#| message: false
anova(pfit1, type = "marginal")
```


```{r}
emmeans(pfit1, ~ Inoc|Cv)
emmeans(pfit1, ~ time|Cv)
```


